title,summary,link,score,innovation,newness,potential,clarity,relevance
Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning,"Datasets are foundational to many breakthroughs in modern artificial
intelligence. Many recent achievements in the space of natural language
processing (NLP) can be attributed to the finetuning of pre-trained models on a
diverse set of tasks that enables a large language model (LLM) to respond to
instructions. Instruction fine-tuning (IFT) requires specifically constructed
and annotated datasets. However, existing datasets are almost all in the
English language. In this work, our primary goal is to bridge the language gap
by building a human-curated instruction-following dataset spanning 65
languages. We worked with fluent speakers of languages from around the world to
collect natural instances of instructions and completions. Furthermore, we
create the most extensive multilingual collection to date, comprising 513
million instances through templating and translating existing datasets across
114 languages. In total, we contribute four key resources: we develop and
open-source the Aya Annotation Platform, the Aya Dataset, the Aya Collection,
and the Aya Evaluation Suite. The Aya initiative also serves as a valuable case
study in participatory research, involving collaborators from 119 countries. We
see this as a valuable framework for future research collaborations that aim to
bridge gaps in resources.",http://arxiv.org/abs/2402.06619v1,100,20,20,20,20,20
Accelerated Smoothing: A Scalable Approach to Randomized Smoothing,"Randomized smoothing has emerged as a potent certifiable defense against
adversarial attacks by employing smoothing noises from specific distributions
to ensure the robustness of a smoothed classifier. However, the utilization of
Monte Carlo sampling in this process introduces a compute-intensive element,
which constrains the practicality of randomized smoothing on a larger scale. To
address this limitation, we propose a novel approach that replaces Monte Carlo
sampling with the training of a surrogate neural network. Through extensive
experimentation in various settings, we demonstrate the efficacy of our
approach in approximating the smoothed classifier with remarkable precision.
Furthermore, we demonstrate that our approach significantly accelerates the
robust radius certification process, providing nearly $600$X improvement in
computation time, overcoming the computational bottlenecks associated with
traditional randomized smoothing.",http://arxiv.org/abs/2402.07498v1,96,19,20,20,19,18
NCRF: Neural Contact Radiance Fields for Free-Viewpoint Rendering of Hand-Object Interaction,"Modeling hand-object interactions is a fundamentally challenging task in 3D
computer vision. Despite remarkable progress that has been achieved in this
field, existing methods still fail to synthesize the hand-object interaction
photo-realistically, suffering from degraded rendering quality caused by the
heavy mutual occlusions between the hand and the object, and inaccurate
hand-object pose estimation. To tackle these challenges, we present a novel
free-viewpoint rendering framework, Neural Contact Radiance Field (NCRF), to
reconstruct hand-object interactions from a sparse set of videos. In
particular, the proposed NCRF framework consists of two key components: (a) A
contact optimization field that predicts an accurate contact field from 3D
query points for achieving desirable contact between the hand and the object.
(b) A hand-object neural radiance field to learn an implicit hand-object
representation in a static canonical space, in concert with the specifically
designed hand-object motion field to produce observation-to-canonical
correspondences. We jointly learn these key components where they mutually help
and regularize each other with visual and geometric constraints, producing a
high-quality hand-object reconstruction that achieves photo-realistic novel
view synthesis. Extensive experiments on HO3D and DexYCB datasets show that our
approach outperforms the current state-of-the-art in terms of both rendering
quality and pose estimation accuracy.",http://arxiv.org/abs/2402.05532v2,96,20,20,19,18,19
RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback,"Reward engineering has long been a challenge in Reinforcement Learning (RL)
research, as it often requires extensive human effort and iterative processes
of trial-and-error to design effective reward functions. In this paper, we
propose RL-VLM-F, a method that automatically generates reward functions for
agents to learn new tasks, using only a text description of the task goal and
the agent's visual observations, by leveraging feedbacks from vision language
foundation models (VLMs). The key to our approach is to query these models to
give preferences over pairs of the agent's image observations based on the text
description of the task goal, and then learn a reward function from the
preference labels, rather than directly prompting these models to output a raw
reward score, which can be noisy and inconsistent. We demonstrate that RL-VLM-F
successfully produces effective rewards and policies across various domains -
including classic control, as well as manipulation of rigid, articulated, and
deformable objects - without the need for human supervision, outperforming
prior methods that use large pretrained models for reward generation under the
same assumptions.",http://arxiv.org/abs/2402.03681v2,96,20,20,20,18,18
Benchmarking and Building Long-Context Retrieval Models with LoCo and M2-BERT,"Retrieval pipelines-an integral component of many machine learning
systems-perform poorly in domains where documents are long (e.g., 10K tokens or
more) and where identifying the relevant document requires synthesizing
information across the entire text. Developing long-context retrieval encoders
suitable for these domains raises three challenges: (1) how to evaluate
long-context retrieval performance, (2) how to pretrain a base language model
to represent both short contexts (corresponding to queries) and long contexts
(corresponding to documents), and (3) how to fine-tune this model for retrieval
under the batch size limitations imposed by GPU memory constraints. To address
these challenges, we first introduce LoCoV1, a novel 12 task benchmark
constructed to measure long-context retrieval where chunking is not possible or
not effective. We next present the M2-BERT retrieval encoder, an 80M parameter
state-space encoder model built from the Monarch Mixer architecture, capable of
scaling to documents up to 32K tokens long. We describe a pretraining data
mixture which allows this encoder to process both short and long context
sequences, and a finetuning approach that adapts this base model to retrieval
with only single-sample batches. Finally, we validate the M2-BERT retrieval
encoder on LoCoV1, finding that it outperforms competitive baselines by up to
23.3 points, despite containing 5-90x fewer parameters.",http://arxiv.org/abs/2402.07440v1,96,20,20,20,18,18
InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning,"The math abilities of large language models can represent their abstract
reasoning ability. In this paper, we introduce and open-source our math
reasoning LLMs InternLM-Math which is continue pre-trained from InternLM2. We
unify chain-of-thought reasoning, reward modeling, formal reasoning, data
augmentation, and code interpreter in a unified seq2seq format and supervise
our model to be a versatile math reasoner, verifier, prover, and augmenter.
These abilities can be used to develop the next math LLMs or self-iteration.
InternLM-Math obtains open-sourced state-of-the-art performance under the
setting of in-context learning, supervised fine-tuning, and code-assisted
reasoning in various informal and formal benchmarks including GSM8K, MATH,
Hungary math exam, MathBench-ZH, and MiniF2F. Our pre-trained model achieves
30.3 on the MiniF2F test set without fine-tuning. We further explore how to use
LEAN to solve math problems and study its performance under the setting of
multi-task learning which shows the possibility of using LEAN as a unified
platform for solving and proving in math. Our models, codes, and data are
released at \url{https://github.com/InternLM/InternLM-Math}.",http://arxiv.org/abs/2402.06332v1,96,20,20,20,18,18
Game-theoretic Counterfactual Explanation for Graph Neural Networks,"Graph Neural Networks (GNNs) have been a powerful tool for node
classification tasks in complex networks. However, their decision-making
processes remain a black-box to users, making it challenging to understand the
reasoning behind their predictions. Counterfactual explanations (CFE) have
shown promise in enhancing the interpretability of machine learning models.
Prior approaches to compute CFE for GNNS often are learning-based approaches
that require training additional graphs. In this paper, we propose a
semivalue-based, non-learning approach to generate CFE for node classification
tasks, eliminating the need for any additional training. Our results reveals
that computing Banzhaf values requires lower sample complexity in identifying
the counterfactual explanations compared to other popular methods such as
computing Shapley values. Our empirical evidence indicates computing Banzhaf
values can achieve up to a fourfold speed up compared to Shapley values. We
also design a thresholding method for computing Banzhaf values and show
theoretical and empirical results on its robustness in noisy environments,
making it superior to Shapley values. Furthermore, the thresholded Banzhaf
values are shown to enhance efficiency without compromising the quality (i.e.,
fidelity) in the explanations in three popular graph datasets.",http://arxiv.org/abs/2402.06030v1,96,18,20,20,18,20
Memory-Efficient Vision Transformers: An Activation-Aware Mixed-Rank Compression Strategy,"As Vision Transformers (ViTs) increasingly set new benchmarks in computer
vision, their practical deployment on inference engines is often hindered by
their significant memory bandwidth and (on-chip) memory footprint requirements.
This paper addresses this memory limitation by introducing an activation-aware
model compression methodology that uses selective low-rank weight tensor
approximations of different layers to reduce the parameter count of ViTs. The
key idea is to decompose the weight tensors into a sum of two
parameter-efficient tensors while minimizing the error between the product of
the input activations with the original weight tensor and the product of the
input activations with the approximate tensor sum. This approximation is
further refined by adopting an efficient layer-wise error compensation
technique that uses the gradient of the layer's output loss. The combination of
these techniques achieves excellent results while it avoids being trapped in a
shallow local minimum early in the optimization process and strikes a good
balance between the model compression and output accuracy. Notably, the
presented method significantly reduces the parameter count of DeiT-B by 60%
with less than 1% accuracy drop on the ImageNet dataset, overcoming the usual
accuracy degradation seen in low-rank approximations. In addition to this, the
presented compression technique can compress large DeiT/ViT models to have
about the same model size as smaller DeiT/ViT variants while yielding up to
1.8% accuracy gain. These results highlight the efficacy of our approach,
presenting a viable solution for embedding ViTs in memory-constrained
environments without compromising their performance.",http://arxiv.org/abs/2402.06004v1,96,19,20,20,18,19
RESMatch: Referring Expression Segmentation in a Semi-Supervised Manner,"Referring expression segmentation (RES), a task that involves localizing
specific instance-level objects based on free-form linguistic descriptions, has
emerged as a crucial frontier in human-AI interaction. It demands an intricate
understanding of both visual and textual contexts and often requires extensive
training data. This paper introduces RESMatch, the first semi-supervised
learning (SSL) approach for RES, aimed at reducing reliance on exhaustive data
annotation. Extensive validation on multiple RES datasets demonstrates that
RESMatch significantly outperforms baseline approaches, establishing a new
state-of-the-art. Although existing SSL techniques are effective in image
segmentation, we find that they fall short in RES. Facing the challenges
including the comprehension of free-form linguistic descriptions and the
variability in object attributes, RESMatch introduces a trifecta of
adaptations: revised strong perturbation, text augmentation, and adjustments
for pseudo-label quality and strong-weak supervision. This pioneering work lays
the groundwork for future research in semi-supervised learning for referring
expression segmentation.",http://arxiv.org/abs/2402.05589v2,96,19,20,19,19,19
Reinforcement Learning as a Catalyst for Robust and Fair Federated Learning: Deciphering the Dynamics of Client Contributions,"Recent advancements in federated learning (FL) have produced models that
retain user privacy by training across multiple decentralized devices or
systems holding local data samples. However, these strategies often neglect the
inherent challenges of statistical heterogeneity and vulnerability to
adversarial attacks, which can degrade model robustness and fairness.
Personalized FL strategies offer some respite by adjusting models to fit
individual client profiles, yet they tend to neglect server-side aggregation
vulnerabilities. To address these issues, we propose Reinforcement Federated
Learning (RFL), a novel framework that leverages deep reinforcement learning to
adaptively optimize client contribution during aggregation, thereby enhancing
both model robustness against malicious clients and fairness across
participants under non-identically distributed settings. To achieve this goal,
we propose a meticulous approach involving a Deep Deterministic Policy
Gradient-based algorithm for continuous control of aggregation weights, an
innovative client selection method based on model parameter distances, and a
reward mechanism guided by validation set performance. Empirically, extensive
experiments demonstrate that, in terms of robustness, RFL outperforms the
state-of-the-art methods, while maintaining comparable levels of fairness,
offering a promising solution to build resilient and fair federated systems.",http://arxiv.org/abs/2402.05541v1,95,20,20,19,18,18
CABINET: Content Relevance based Noise Reduction for Table Question Answering,"Table understanding capability of Large Language Models (LLMs) has been
extensively studied through the task of question-answering (QA) over tables.
Typically, only a small part of the whole table is relevant to derive the
answer for a given question. The irrelevant parts act as noise and are
distracting information, resulting in sub-optimal performance due to the
vulnerability of LLMs to noise. To mitigate this, we propose CABINET (Content
RelevAnce-Based NoIse ReductioN for TablE QuesTion-Answering) - a framework to
enable LLMs to focus on relevant tabular data by suppressing extraneous
information. CABINET comprises an Unsupervised Relevance Scorer (URS), trained
differentially with the QA LLM, that weighs the table content based on its
relevance to the input question before feeding it to the question-answering LLM
(QA LLM). To further aid the relevance scorer, CABINET employs a weakly
supervised module that generates a parsing statement describing the criteria of
rows and columns relevant to the question and highlights the content of
corresponding table cells. CABINET significantly outperforms various tabular
LLM baselines, as well as GPT3-based in-context learning methods, is more
robust to noise, maintains outperformance on tables of varying sizes, and
establishes new SoTA performance on WikiTQ, FeTaQA, and WikiSQL datasets. We
release our code and datasets at https://github.com/Sohanpatnaik106/CABINET_QA.",http://arxiv.org/abs/2402.01155v3,95,20,19,19,18,19
Enhancing Multi-Criteria Decision Analysis with AI: Integrating Analytic Hierarchy Process and GPT-4 for Automated Decision Support,"Our study presents a new framework that incorporates the Analytic Hierarchy
Process (AHP) and Generative Pre-trained Transformer 4 (GPT-4) large language
model (LLM), bringing novel approaches to cybersecurity Multiple-criteria
Decision Making (MCDA). By utilizing the capabilities of GPT-4 autonomous
agents as virtual experts, we automate the decision-making process, enhancing
both efficiency and reliability. This new approach focuses on leveraging LLMs
for sophisticated decision analysis, highlighting the synergy between
traditional decision-making models and cutting-edge AI technologies. Our
innovative methodology demonstrates significant advancements in using AI-driven
agents for complex decision-making scenarios, highlighting the importance of AI
in strategic cybersecurity applications. The findings reveal the transformative
potential of combining AHP and LLMs, establishing a new paradigm for
intelligent decision support systems in cybersecurity and beyond.",http://arxiv.org/abs/2402.07404v1,95,18,20,20,19,18
"APALU: A Trainable, Adaptive Activation Function for Deep Learning Networks","Activation function is a pivotal component of deep learning, facilitating the
extraction of intricate data patterns. While classical activation functions
like ReLU and its variants are extensively utilized, their static nature and
simplicity, despite being advantageous, often limit their effectiveness in
specialized tasks. The trainable activation functions also struggle sometimes
to adapt to the unique characteristics of the data. Addressing these
limitations, we introduce a novel trainable activation function, adaptive
piecewise approximated activation linear unit (APALU), to enhance the learning
performance of deep learning across a broad range of tasks. It presents a
unique set of features that enable it to maintain stability and efficiency in
the learning process while adapting to complex data representations.
Experiments reveal significant improvements over widely used activation
functions for different tasks. In image classification, APALU increases
MobileNet and GoogleNet accuracy by 0.37% and 0.04%, respectively, on the
CIFAR10 dataset. In anomaly detection, it improves the average area under the
curve of One-CLASS Deep SVDD by 0.8% on the MNIST dataset, 1.81% and 1.11%
improvements with DifferNet, and knowledge distillation, respectively, on the
MVTech dataset. Notably, APALU achieves 100% accuracy on a sign language
recognition task with a limited dataset. For regression tasks, APALU enhances
the performance of deep neural networks and recurrent neural networks on
different datasets. These improvements highlight the robustness and
adaptability of APALU across diverse deep-learning applications.",http://arxiv.org/abs/2402.08244v1,95,20,20,20,17,18
You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for Semantic and Property Prediction,"Robots must be able to understand their surroundings to perform complex tasks
in challenging environments and many of these complex tasks require estimates
of physical properties such as friction or weight. Estimating such properties
using learning is challenging due to the large amounts of labelled data
required for training and the difficulty of updating these learned models
online at run time. To overcome these challenges, this paper introduces a
novel, multi-modal approach for representing semantic predictions and physical
property estimates jointly in a probabilistic manner. By using conjugate pairs,
the proposed method enables closed-form Bayesian updates given visual and
tactile measurements without requiring additional training data. The efficacy
of the proposed algorithm is demonstrated through several hardware experiments.
In particular, this paper illustrates that by conditioning semantic
classifications on physical properties, the proposed method quantitatively
outperforms state-of-the-art semantic classification methods that rely on
vision alone. To further illustrate its utility, the proposed method is used in
several applications including to represent affordance-based properties
probabilistically and a challenging terrain traversal task using a legged
robot. In the latter task, the proposed method represents the coefficient of
friction of the terrain probabilistically, which enables the use of an on-line
risk-aware planner that switches the legged robot from a dynamic gait to a
static, stable gait when the expected value of the coefficient of friction
falls below a given threshold. Videos of these case studies are presented in
the multimedia attachment. The proposed framework includes an open-source C++
and ROS interface.",http://arxiv.org/abs/2402.05872v2,95,20,20,20,17,18
Pixel Sentence Representation Learning,"Pretrained language models are long known to be subpar in capturing sentence
and document-level semantics. Though heavily investigated, transferring
perturbation-based methods from unsupervised visual representation learning to
NLP remains an unsolved problem. This is largely due to the discreteness of
subword units brought by tokenization of language models, limiting small
perturbations of inputs to form semantics-preserved positive pairs. In this
work, we conceptualize the learning of sentence-level textual semantics as a
visual representation learning process. Drawing from cognitive and linguistic
sciences, we introduce an unsupervised visual sentence representation learning
framework, employing visually-grounded text perturbation methods like typos and
word order shuffling, resonating with human cognitive patterns, and enabling
perturbation to texts to be perceived as continuous. Our approach is further
bolstered by large-scale unsupervised topical alignment training and natural
language inference supervision, achieving comparable performance in semantic
textual similarity (STS) to existing state-of-the-art NLP methods.
Additionally, we unveil our method's inherent zero-shot cross-lingual
transferability and a unique leapfrogging pattern across languages during
iterative training. To our knowledge, this is the first representation learning
method devoid of traditional language models for understanding sentence and
document semantics, marking a stride closer to human-like textual
comprehension. Our code is available at
https://github.com/gowitheflow-1998/Pixel-Linguist",http://arxiv.org/abs/2402.08183v1,95,20,20,20,18,17
Extensible Multi-Granularity Fusion Network for Aspect-based Sentiment Analysis,"Aspect-based Sentiment Analysis (ABSA) evaluates sentiment expressions within
a text to comprehend sentiment information. Previous studies integrated
external knowledge, such as knowledge graphs, to enhance the semantic features
in ABSA models. Recent research has examined the use of Graph Neural Networks
(GNNs) on dependency and constituent trees for syntactic analysis. With the
ongoing development of ABSA, more innovative linguistic and structural features
are being incorporated (e.g. latent graph), but this also introduces complexity
and confusion. As of now, a scalable framework for integrating diverse
linguistic and structural features into ABSA does not exist. This paper
presents the Extensible Multi-Granularity Fusion (EMGF) network, which
integrates information from dependency and constituent syntactic, attention
semantic , and external knowledge graphs. EMGF, equipped with multi-anchor
triplet learning and orthogonal projection, efficiently harnesses the combined
potential of each granularity feature and their synergistic interactions,
resulting in a cumulative effect without additional computational expenses.
Experimental findings on SemEval 2014 and Twitter datasets confirm EMGF's
superiority over existing ABSA methods.",http://arxiv.org/abs/2402.07787v2,95,20,20,18,18,19
CATO: End-to-End Optimization of ML-Based Traffic Analysis Pipelines,"Machine learning has shown tremendous potential for improving the
capabilities of network traffic analysis applications, often outperforming
simpler rule-based heuristics. However, ML-based solutions remain difficult to
deploy in practice. Many existing approaches only optimize the predictive
performance of their models, overlooking the practical challenges of running
them against network traffic in real time. This is especially problematic in
the domain of traffic analysis, where the efficiency of the serving pipeline is
a critical factor in determining the usability of a model. In this work, we
introduce CATO, a framework that addresses this problem by jointly optimizing
the predictive performance and the associated systems costs of the serving
pipeline. CATO leverages recent advances in multi-objective Bayesian
optimization to efficiently identify Pareto-optimal configurations, and
automatically compiles end-to-end optimized serving pipelines that can be
deployed in real networks. Our evaluations show that compared to popular
feature optimization techniques, CATO can provide up to 3600x lower inference
latency and 3.7x higher zero-loss throughput while simultaneously achieving
better model performance.",http://arxiv.org/abs/2402.06099v1,95,19,20,19,19,18
Toward Scalable Generative AI via Mixture of Experts in Mobile Edge Networks,"The advancement of generative artificial intelligence (GAI) has driven
revolutionary applications like ChatGPT. The widespread of these applications
relies on the mixture of experts (MoE), which contains multiple experts and
selectively engages them for each task to lower operation costs while
maintaining performance. Despite MoE, GAI faces challenges in resource
consumption when deployed on user devices. This paper proposes mobile edge
networks supported MoE-based GAI. We first review the MoE from traditional AI
and GAI perspectives, including structure, principles, and applications. We
then propose a framework that transfers subtasks to devices in mobile edge
networks, aiding GAI model operation on user devices. We discuss challenges in
this process and introduce a deep reinforcement learning based algorithm to
select edge devices for subtask execution. Experimental results will show that
our framework not only facilitates GAI's deployment on resource-limited devices
but also generates higher-quality content compared to methods without edge
network support.",http://arxiv.org/abs/2402.06942v1,95,20,20,20,18,17
Segmentation-free Connectionist Temporal Classification loss based OCR Model for Text Captcha Classification,"Captcha are widely used to secure systems from automatic responses by
distinguishing computer responses from human responses. Text, audio, video,
picture picture-based Optical Character Recognition (OCR) are used for creating
captcha. Text-based OCR captcha are the most often used captcha which faces
issues namely, complex and distorted contents. There are attempts to build
captcha detection and classification-based systems using machine learning and
neural networks, which need to be tuned for accuracy. The existing systems face
challenges in the recognition of distorted characters, handling variable-length
captcha and finding sequential dependencies in captcha. In this work, we
propose a segmentation-free OCR model for text captcha classification based on
the connectionist temporal classification loss technique. The proposed model is
trained and tested on a publicly available captcha dataset. The proposed model
gives 99.80\% character level accuracy, while 95\% word level accuracy. The
accuracy of the proposed model is compared with the state-of-the-art models and
proves to be effective. The variable length complex captcha can be thus
processed with the segmentation-free connectionist temporal classification loss
technique with dependencies which will be massively used in securing the
software systems.",http://arxiv.org/abs/2402.05417v1,94,18,20,20,18,18
Towards Generalizability of Multi-Agent Reinforcement Learning in Graphs with Recurrent Message Passing,"Graph-based environments pose unique challenges to multi-agent reinforcement
learning. In decentralized approaches, agents operate within a given graph and
make decisions based on partial or outdated observations. The size of the
observed neighborhood limits the generalizability to different graphs and
affects the reactivity of agents, the quality of the selected actions, and the
communication overhead. This work focuses on generalizability and resolves the
trade-off in observed neighborhood size with a continuous information flow in
the whole graph. We propose a recurrent message-passing model that iterates
with the environment's steps and allows nodes to create a global representation
of the graph by exchanging messages with their neighbors. Agents receive the
resulting learned graph observations based on their location in the graph. Our
approach can be used in a decentralized manner at runtime and in combination
with a reinforcement learning algorithm of choice. We evaluate our method
across 1000 diverse graphs in the context of routing in communication networks
and find that it enables agents to generalize and adapt to changes in the
graph.",http://arxiv.org/abs/2402.05027v2,94,20,18,20,18,18
Version age-based client scheduling policy for federated learning,"Federated Learning (FL) has emerged as a privacy-preserving machine learning
paradigm facilitating collaborative training across multiple clients without
sharing local data. Despite advancements in edge device capabilities,
communication bottlenecks present challenges in aggregating a large number of
clients; only a portion of the clients can update their parameters upon each
global aggregation. This phenomenon introduces the critical challenge of
stragglers in FL and the profound impact of client scheduling policies on
global model convergence and stability. Existing scheduling strategies address
staleness but predominantly focus on either timeliness or content. Motivated by
this, we introduce the novel concept of Version Age of Information (VAoI) to
FL. Unlike traditional Age of Information metrics, VAoI considers both
timeliness and content staleness. Each client's version age is updated
discretely, indicating the freshness of information. VAoI is incorporated into
the client scheduling policy to minimize the average VAoI, mitigating the
impact of outdated local updates and enhancing the stability of FL systems.",http://arxiv.org/abs/2402.05407v1,94,20,20,18,18,18
Measurement Scheduling for ICU Patients with Offline Reinforcement Learning,"Scheduling laboratory tests for ICU patients presents a significant
challenge. Studies show that 20-40% of lab tests ordered in the ICU are
redundant and could be eliminated without compromising patient safety. Prior
work has leveraged offline reinforcement learning (Offline-RL) to find optimal
policies for ordering lab tests based on patient information. However, new ICU
patient datasets have since been released, and various advancements have been
made in Offline-RL methods. In this study, we first introduce a preprocessing
pipeline for the newly-released MIMIC-IV dataset geared toward time-series
tasks. We then explore the efficacy of state-of-the-art Offline-RL methods in
identifying better policies for ICU patient lab test scheduling. Besides
assessing methodological performance, we also discuss the overall suitability
and practicality of using Offline-RL frameworks for scheduling laboratory tests
in ICU settings.",http://arxiv.org/abs/2402.07344v1,94,20,18,20,18,18
A Novel Gaussian Min-Max Theorem and its Applications,"A celebrated result by Gordon allows one to compare the min-max behavior of
two Gaussian processes if certain inequality conditions are met. The
consequences of this result include the Gaussian min-max (GMT) and convex
Gaussian min-max (CGMT) theorems which have had far-reaching implications in
high-dimensional statistics, machine learning, non-smooth optimization, and
signal processing. Both theorems rely on a pair of Gaussian processes, first
identified by Slepian, that satisfy Gordon's comparison inequalities. To date,
no other pair of Gaussian processes satisfying these inequalities has been
discovered. In this paper, we identify such a new pair. The resulting theorems
extend the classical GMT and CGMT Theorems from the case where the underlying
Gaussian matrix in the primary process has iid rows to where it has independent
but non-identically-distributed ones. The new CGMT is applied to the problems
of multi-source Gaussian regression, as well as to binary classification of
general Gaussian mixture models.",http://arxiv.org/abs/2402.07356v1,94,20,18,18,18,20
DiffTOP: Differentiable Trajectory Optimization for Deep Reinforcement and Imitation Learning,"This paper introduces DiffTOP, which utilizes Differentiable Trajectory
OPtimization as the policy representation to generate actions for deep
reinforcement and imitation learning. Trajectory optimization is a powerful and
widely used algorithm in control, parameterized by a cost and a dynamics
function. The key to our approach is to leverage the recent progress in
differentiable trajectory optimization, which enables computing the gradients
of the loss with respect to the parameters of trajectory optimization. As a
result, the cost and dynamics functions of trajectory optimization can be
learned end-to-end. DiffTOP addresses the ``objective mismatch'' issue of prior
model-based RL algorithms, as the dynamics model in DiffTOP is learned to
directly maximize task performance by differentiating the policy gradient loss
through the trajectory optimization process. We further benchmark DiffTOP for
imitation learning on standard robotic manipulation task suites with
high-dimensional sensory observations and compare our method to feed-forward
policy classes as well as Energy-Based Models (EBM) and Diffusion. Across 15
model-based RL tasks and 13 imitation learning tasks with high-dimensional
image and point cloud inputs, DiffTOP outperforms prior state-of-the-art
methods in both domains.",http://arxiv.org/abs/2402.05421v1,94,20,20,18,18,18
Development and validation of an artificial intelligence model to accurately predict spinopelvic parameters,"Objective. Achieving appropriate spinopelvic alignment has been shown to be
associated with improved clinical symptoms. However, measurement of spinopelvic
radiographic parameters is time-intensive and interobserver reliability is a
concern. Automated measurement tools have the promise of rapid and consistent
measurements, but existing tools are still limited by some degree of manual
user-entry requirements. This study presents a novel artificial intelligence
(AI) tool called SpinePose that automatically predicts spinopelvic parameters
with high accuracy without the need for manual entry.
  Methods. SpinePose was trained and validated on 761 sagittal whole-spine
X-rays to predict sagittal vertical axis (SVA), pelvic tilt (PT), pelvic
incidence (PI), sacral slope (SS), lumbar lordosis (LL), T1-pelvic angle
(T1PA), and L1-pelvic angle (L1PA). A separate test set of 40 X-rays was
labeled by 4 reviewers, including fellowship-trained spine surgeons and a
fellowship-trained radiologist with neuroradiology subspecialty certification.
Median errors relative to the most senior reviewer were calculated to determine
model accuracy on test images. Intraclass correlation coefficients (ICC) were
used to assess inter-rater reliability.
  Results. SpinePose exhibited the following median (interquartile range)
parameter errors: SVA: 2.2(2.3)mm, p=0.93; PT: 1.3(1.2){\deg}, p=0.48; SS:
1.7(2.2){\deg}, p=0.64; PI: 2.2(2.1){\deg}, p=0.24; LL: 2.6(4.0){\deg}, p=0.89;
T1PA: 1.1(0.9){\deg}, p=0.42; and L1PA: 1.4(1.6){\deg}, p=0.49. Model
predictions also exhibited excellent reliability at all parameters (ICC:
0.91-1.0).
  Conclusions. SpinePose accurately predicted spinopelvic parameters with
excellent reliability comparable to fellowship-trained spine surgeons and
neuroradiologists. Utilization of predictive AI tools in spinal imaging can
substantially aid in patient selection and surgical planning.",http://arxiv.org/abs/2402.06185v1,94,18,20,20,18,18
Accurate LoRA-Finetuning Quantization of LLMs via Information Retention,"The LoRA-finetuning quantization of LLMs has been extensively studied to
obtain accurate yet compact LLMs for deployment on resource-constrained
hardware. However, existing methods cause the quantized LLM to severely degrade
and even fail to benefit from the finetuning of LoRA. This paper proposes a
novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate
through information retention. The proposed IR-QLoRA mainly relies on two
technologies derived from the perspective of unified information: (1)
statistics-based Information Calibration Quantization allows the quantized
parameters of LLM to retain original information accurately; (2)
finetuning-based Information Elastic Connection makes LoRA utilizes elastic
representation transformation with diverse information. Comprehensive
experiments show that IR-QLoRA can significantly improve accuracy across LLaMA
and LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%
improvement on MMLU compared with the state-of-the-art methods. The significant
performance gain requires only a tiny 0.31% additional time consumption,
revealing the satisfactory efficiency of our IRQLoRA. We highlight that
IR-QLoRA enjoys excellent versatility, compatible with various frameworks
(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.
The code is available at https://github.com/htqin/ir-qlora.",http://arxiv.org/abs/2402.05445v1,94,20,20,18,18,18
Transfer learning with generative models for object detection on limited datasets,"The availability of data is limited in some fields, especially for object
detection tasks, where it is necessary to have correctly labeled bounding boxes
around each object. A notable example of such data scarcity is found in the
domain of marine biology, where it is useful to develop methods to
automatically detect submarine species for environmental monitoring. To address
this data limitation, the state-of-the-art machine learning strategies employ
two main approaches. The first involves pretraining models on existing datasets
before generalizing to the specific domain of interest. The second strategy is
to create synthetic datasets specifically tailored to the target domain using
methods like copy-paste techniques or ad-hoc simulators. The first strategy
often faces a significant domain shift, while the second demands custom
solutions crafted for the specific task. In response to these challenges, here
we propose a transfer learning framework that is valid for a generic scenario.
In this framework, generated images help to improve the performances of an
object detector in a few-real data regime. This is achieved through a
diffusion-based generative model that was pretrained on large generic datasets,
and is not trained on the task-specific domain. We validate our approach on
object detection tasks, specifically focusing on fishes in an underwater
environment, and on the more common domain of cars in an urban setting. Our
method achieves detection performance comparable to models trained on thousands
of images, using only a few hundreds of input data. Our results pave the way
for new generative AI-based protocols for machine learning applications in
various domains, for instance ranging from geophysics to biology and medicine.",http://arxiv.org/abs/2402.06784v1,94,18,20,20,18,18
You Only Need One Color Space: An Efficient Network for Low-light Image Enhancement,"Low-Light Image Enhancement (LLIE) task tends to restore the details and
visual information from corrupted low-light images. Most existing methods learn
the mapping function between low/normal-light images by Deep Neural Networks
(DNNs) on sRGB and HSV color space. Nevertheless, enhancement involves
amplifying image signals, and applying these color spaces to low-light images
with a low signal-to-noise ratio can introduce sensitivity and instability into
the enhancement process. Consequently, this results in the presence of color
artifacts and brightness artifacts in the enhanced images. To alleviate this
problem, we propose a novel trainable color space, named
Horizontal/Vertical-Intensity (HVI). It not only decouples brightness and color
from RGB channels to mitigate the instability during enhancement but also
adapts to low-light images in different illumination ranges due to the
trainable parameters. Further, we design a novel Color and Intensity Decoupling
Network (CIDNet) with two branches dedicated to processing the decoupled image
brightness and color in the HVI space. Within CIDNet, we introduce the
Lightweight Cross-Attention (LCA) module to facilitate interaction between
image structure and content information in both branches, while also
suppressing noise in low-light images. Finally, we conducted 22 quantitative
and qualitative experiments to show that the proposed CIDNet outperforms the
state-of-the-art methods on 11 datasets. The code will be available at
https://github.com/Fediory/HVI-CIDNet.",http://arxiv.org/abs/2402.05809v1,94,20,20,18,18,18
G-SciEdBERT: A Contextualized LLM for Science Assessment Tasks in German,"The advancement of natural language processing has paved the way for
automated scoring systems in various languages, such as German (e.g., German
BERT [G-BERT]). Automatically scoring written responses to science questions in
German is a complex task and challenging for standard G-BERT as they lack
contextual knowledge in the science domain and may be unaligned with student
writing styles. This paper developed a contextualized German Science Education
BERT (G-SciEdBERT), an innovative large language model tailored for scoring
German-written responses to science tasks. Using G-BERT, we pre-trained
G-SciEdBERT on a corpus of 50K German written science responses with 5M tokens
to the Programme for International Student Assessment (PISA) 2015. We
fine-tuned G-SciEdBERT on 59 assessment items and examined the scoring
accuracy. We then compared its performance with G-BERT. Our findings reveal a
substantial improvement in scoring accuracy with G-SciEdBERT, demonstrating a
10% increase of quadratic weighted kappa compared to G-BERT (mean accuracy
difference = 0.096, SD = 0.024). These insights underline the significance of
specialized language models like G-SciEdBERT, which is trained to enhance the
accuracy of automated scoring, offering a substantial contribution to the field
of AI in education.",http://arxiv.org/abs/2402.06584v1,94,20,18,20,18,18
Fine-Tuning Surrogate Gradient Learning for Optimal Hardware Performance in Spiking Neural Networks,"The highly sparse activations in Spiking Neural Networks (SNNs) can provide
tremendous energy efficiency benefits when carefully exploited in hardware. The
behavior of sparsity in SNNs is uniquely shaped by the dataset and training
hyperparameters. This work reveals novel insights into the impacts of training
on hardware performance. Specifically, we explore the trade-offs between model
accuracy and hardware efficiency. We focus on three key hyperparameters:
surrogate gradient functions, beta, and membrane threshold. Results on an
FPGA-based hardware platform show that the fast sigmoid surrogate function
yields a lower firing rate with similar accuracy compared to the arctangent
surrogate on the SVHN dataset. Furthermore, by cross-sweeping the beta and
membrane threshold hyperparameters, we can achieve a 48% reduction in
hardware-based inference latency with only 2.88% trade-off in inference
accuracy compared to the default setting. Overall, this study highlights the
importance of fine-tuning model hyperparameters as crucial for designing
efficient SNN hardware accelerators, evidenced by the fine-tuned model
achieving a 1.72x improvement in accelerator efficiency (FPS/W) compared to the
most recent work.",http://arxiv.org/abs/2402.06211v1,94,18,20,20,18,18
Vision-Language Models Provide Promptable Representations for Reinforcement Learning,"Humans can quickly learn new behaviors by leveraging background world
knowledge. In contrast, agents trained with reinforcement learning (RL)
typically learn behaviors from scratch. We thus propose a novel approach that
uses the vast amounts of general and indexable world knowledge encoded in
vision-language models (VLMs) pre-trained on Internet-scale data for embodied
RL. We initialize policies with VLMs by using them as promptable
representations: embeddings that are grounded in visual observations and encode
semantic features based on the VLM's internal knowledge, as elicited through
prompts that provide task context and auxiliary information. We evaluate our
approach on visually-complex, long horizon RL tasks in Minecraft and robot
navigation in Habitat. We find that our policies trained on embeddings
extracted from general-purpose VLMs outperform equivalent policies trained on
generic, non-promptable image embeddings. We also find our approach outperforms
instruction-following methods and performs comparably to domain-specific
embeddings.",http://arxiv.org/abs/2402.02651v2,94,18,20,20,18,18
Revisiting Early-Learning Regularization When Federated Learning Meets Noisy Labels,"In the evolving landscape of federated learning (FL), addressing label noise
presents unique challenges due to the decentralized and diverse nature of data
collection across clients. Traditional centralized learning approaches to
mitigate label noise are constrained in FL by privacy concerns and the
heterogeneity of client data. This paper revisits early-learning
regularization, introducing an innovative strategy, Federated Label-mixture
Regularization (FLR). FLR adeptly adapts to FL's complexities by generating new
pseudo labels, blending local and global model predictions. This method not
only enhances the accuracy of the global model in both i.i.d. and non-i.i.d.
settings but also effectively counters the memorization of noisy labels.
Demonstrating compatibility with existing label noise and FL techniques, FLR
paves the way for improved generalization in FL environments fraught with label
inaccuracies.",http://arxiv.org/abs/2402.05353v1,94,19,20,18,18,19
Position Paper: Why the Shooting in the Dark Method Dominates Recommender Systems Practice; A Call to Abandon Anti-Utopian Thinking,"Applied recommender systems research is in a curious position. While there is
a very rigorous protocol for measuring performance by A/B testing, best
practice for finding a `B' to test does not explicitly target performance but
rather targets a proxy measure. The success or failure of a given A/B test then
depends entirely on if the proposed proxy is better correlated to performance
than the previous proxy. No principle exists to identify if one proxy is better
than another offline, leaving the practitioners shooting in the dark. The
purpose of this position paper is to question this anti-Utopian thinking and
argue that a non-standard use of the deep learning stacks actually has the
potential to unlock reward optimizing recommendation.",http://arxiv.org/abs/2402.02152v2,94,20,19,20,18,17
Benchmarking Large Language Models on Communicative Medical Coaching: a Novel System and Dataset,"Traditional applications of natural language processing (NLP) in healthcare
have predominantly focused on patient-centered services, enhancing patient
interactions and care delivery, such as through medical dialogue systems.
However, the potential of NLP to benefit inexperienced doctors, particularly in
areas such as communicative medical coaching, remains largely unexplored. We
introduce ``ChatCoach,'' an integrated human-AI cooperative framework. Within
this framework, both a patient agent and a coaching agent collaboratively
support medical learners in practicing their medical communication skills
during consultations. Unlike traditional dialogue systems, ChatCoach provides a
simulated environment where a human doctor can engage in medical dialogue with
a patient agent. Simultaneously, a coaching agent provides real-time feedback
to the doctor. To construct the ChatCoach system, we developed a dataset and
integrated Large Language Models such as ChatGPT and Llama2, aiming to assess
their effectiveness in communicative medical coaching tasks. Our comparative
analysis demonstrates that instruction-tuned Llama2 significantly outperforms
ChatGPT's prompting-based approaches.",http://arxiv.org/abs/2402.05547v1,94,20,19,19,18,18
Implicit Bias of Policy Gradient in Linear Quadratic Control: Extrapolation to Unseen Initial States,"In modern machine learning, models can often fit training data in numerous
ways, some of which perform well on unseen (test) data, while others do not.
Remarkably, in such cases gradient descent frequently exhibits an implicit bias
that leads to excellent performance on unseen data. This implicit bias was
extensively studied in supervised learning, but is far less understood in
optimal control (reinforcement learning). There, learning a controller applied
to a system via gradient descent is known as policy gradient, and a question of
prime importance is the extent to which a learned controller extrapolates to
unseen initial states. This paper theoretically studies the implicit bias of
policy gradient in terms of extrapolation to unseen initial states. Focusing on
the fundamental Linear Quadratic Regulator (LQR) problem, we establish that the
extent of extrapolation depends on the degree of exploration induced by the
system when commencing from initial states included in training. Experiments
corroborate our theory, and demonstrate its conclusions on problems beyond LQR,
where systems are non-linear and controllers are neural networks. We
hypothesize that real-world optimal control may be greatly improved by
developing methods for informed selection of initial states to train on.",http://arxiv.org/abs/2402.07875v1,94,19,18,18,20,19
Multi-Behavior Collaborative Filtering with Partial Order Graph Convolutional Networks,"Representing the information of multiple behaviors in the single graph
collaborative filtering (CF) vector has been a long-standing challenge. This is
because different behaviors naturally form separate behavior graphs and learn
separate CF embeddings. Existing models merge the separate embeddings by
appointing the CF embeddings for some behaviors as the primary embedding and
utilizing other auxiliaries to enhance the primary embedding. However, this
approach often results in the joint embedding performing well on the main tasks
but poorly on the auxiliary ones. To address the problem arising from the
separate behavior graphs, we propose the concept of Partial Order Graphs (POG).
POG defines the partial order relation of multiple behaviors and models
behavior combinations as weighted edges to merge separate behavior graphs into
a joint POG. Theoretical proof verifies that POG can be generalized to any
given set of multiple behaviors. Based on POG, we propose the tailored Partial
Order Graph Convolutional Networks (POGCN) that convolute neighbors'
information while considering the behavior relations between users and items.
POGCN also introduces a partial-order BPR sampling strategy for efficient and
effective multiple-behavior CF training. POGCN has been successfully deployed
on the homepage of Alibaba for two months, providing recommendation services
for over one billion users. Extensive offline experiments conducted on three
public benchmark datasets demonstrate that POGCN outperforms state-of-the-art
multi-behavior baselines across all types of behaviors. Furthermore, online A/B
tests confirm the superiority of POGCN in billion-scale recommender systems.",http://arxiv.org/abs/2402.07659v1,94,18,20,20,18,18
How Uniform Random Weights Induce Non-uniform Bias: Typical Interpolating Neural Networks Generalize with Narrow Teachers,"Background. A main theoretical puzzle is why over-parameterized Neural
Networks (NNs) generalize well when trained to zero loss (i.e., so they
interpolate the data). Usually, the NN is trained with Stochastic Gradient
Descent (SGD) or one of its variants. However, recent empirical work examined
the generalization of a random NN that interpolates the data: the NN was
sampled from a seemingly uniform prior over the parameters, conditioned on that
the NN perfectly classifying the training set. Interestingly, such a NN sample
typically generalized as well as SGD-trained NNs.
  Contributions. We prove that such a random NN interpolator typically
generalizes well if there exists an underlying narrow ``teacher NN"" that agrees
with the labels. Specifically, we show that such a `flat' prior over the NN
parametrization induces a rich prior over the NN functions, due to the
redundancy in the NN structure. In particular, this creates a bias towards
simpler functions, which require less relevant parameters to represent --
enabling learning with a sample complexity approximately proportional to the
complexity of the teacher (roughly, the number of non-redundant parameters),
rather than the student's.",http://arxiv.org/abs/2402.06323v1,94,17,19,19,18,21
S$$I: Score-based O-INFORMATION Estimation,"The analysis of scientific data and complex multivariate systems requires
information quantities that capture relationships among multiple random
variables. Recently, new information-theoretic measures have been developed to
overcome the shortcomings of classical ones, such as mutual information, that
are restricted to considering pairwise interactions. Among them, the concept of
information synergy and redundancy is crucial for understanding the high-order
dependencies between variables. One of the most prominent and versatile
measures based on this concept is O-information, which provides a clear and
scalable way to quantify the synergy-redundancy balance in multivariate
systems. However, its practical application is limited to simplified cases. In
this work, we introduce S$\Omega$I, which allows for the first time to compute
O-information without restrictive assumptions about the system. Our experiments
validate our approach on synthetic data, and demonstrate the effectiveness of
S$\Omega$I in the context of a real-world use case.",http://arxiv.org/abs/2402.05667v1,94,20,18,18,20,18
Generalizing Conversational Dense Retrieval via LLM-Cognition Data Augmentation,"Conversational search utilizes muli-turn natural language contexts to
retrieve relevant passages. Existing conversational dense retrieval models
mostly view a conversation as a fixed sequence of questions and responses,
overlooking the severe data sparsity problem -- that is, users can perform a
conversation in various ways, and these alternate conversations are unrecorded.
Consequently, they often struggle to generalize to diverse conversations in
real-world scenarios. In this work, we propose a framework for generalizing
Conversational dense retrieval via LLM-cognition data Augmentation (ConvAug).
ConvAug first generates multi-level augmented conversations to capture the
diverse nature of conversational contexts. Inspired by human cognition, we
devise a cognition-aware process to mitigate the generation of false positives,
false negatives, and hallucinations. Moreover, we develop a difficulty-adaptive
sample filter that selects challenging samples for complex conversations,
thereby giving the model a larger learning space. A contrastive learning
objective is then employed to train a better conversational context encoder.
Extensive experiments conducted on four public datasets, under both normal and
zero-shot settings, demonstrate the effectiveness, generalizability, and
applicability of ConvAug.",http://arxiv.org/abs/2402.07092v1,94,18,19,19,19,19
Semi-Supervised Learning for Bilingual Lexicon Induction,"We consider the problem of aligning two sets of continuous word
representations, corresponding to languages, to a common space in order to
infer a bilingual lexicon. It was recently shown that it is possible to infer
such lexicon, without using any parallel data, by aligning word embeddings
trained on monolingual data. Such line of work is called unsupervised bilingual
induction. By wondering whether it was possible to gain experience in the
progressive learning of several languages, we asked ourselves to what extent we
could integrate the knowledge of a given set of languages when learning a new
one, without having parallel data for the latter. In other words, while keeping
the core problem of unsupervised learning in the latest step, we allowed the
access to other corpora of idioms, hence the name semi-supervised. This led us
to propose a novel formulation, considering the lexicon induction as a ranking
problem for which we used recent tools of this machine learning field. Our
experiments on standard benchmarks, inferring dictionary from English to more
than 20 languages, show that our approach consistently outperforms existing
state of the art benchmark. In addition, we deduce from this new scenario
several relevant conclusions allowing a better understanding of the alignment
phenomenon.",http://arxiv.org/abs/2402.07028v1,94,18,18,18,20,20
Hierarchical Transformers are Efficient Meta-Reinforcement Learners,"We introduce Hierarchical Transformers for Meta-Reinforcement Learning
(HTrMRL), a powerful online meta-reinforcement learning approach. HTrMRL aims
to address the challenge of enabling reinforcement learning agents to perform
effectively in previously unseen tasks. We demonstrate how past episodes serve
as a rich source of information, which our model effectively distills and
applies to new contexts. Our learned algorithm is capable of outperforming the
previous state-of-the-art and provides more efficient meta-training while
significantly improving generalization capabilities. Experimental results,
obtained across various simulated tasks of the Meta-World Benchmark, indicate a
significant improvement in learning efficiency and adaptability compared to the
state-of-the-art on a variety of tasks. Our approach not only enhances the
agent's ability to generalize from limited data but also paves the way for more
robust and versatile AI systems.",http://arxiv.org/abs/2402.06402v1,94,20,20,18,18,18
FusionSF: Fuse Heterogeneous Modalities in a Vector Quantized Framework for Robust Solar Power Forecasting,"Accurate solar power forecasting is crucial to integrate photovoltaic plants
into the electric grid, schedule and secure the power grid safety. This problem
becomes more demanding for those newly installed solar plants which lack
sufficient data. Current research predominantly relies on historical solar
power data or numerical weather prediction in a single-modality format,
ignoring the complementary information provided in different modalities. In
this paper, we propose a multi-modality fusion framework to integrate
historical power data, numerical weather prediction, and satellite images,
significantly improving forecast performance. We introduce a vector quantized
framework that aligns modalities with varying information densities, striking a
balance between integrating sufficient information and averting model
overfitting. Our framework demonstrates strong zero-shot forecasting
capability, which is especially useful for those newly installed plants.
Moreover, we collect and release a multi-modal solar power (MMSP) dataset from
real-world plants to further promote the research of multi-modal solar
forecasting algorithms. Our extensive experiments show that our model not only
operates with robustness but also boosts accuracy in both zero-shot forecasting
and scenarios rich with training data, surpassing leading models. We have
incorporated it into our eForecaster platform and deployed it for more than 300
solar plants with a capacity of over 15GW.",http://arxiv.org/abs/2402.05823v1,94,18,20,20,18,18
Learn To be Efficient: Build Structured Sparsity in Large Language Models,"Large Language Models (LLMs) have achieved remarkable success with their
billion-level parameters, yet they incur high inference overheads. The
emergence of activation sparsity in LLMs provides a natural approach to reduce
this cost by involving only parts of the parameters for inference. Existing
methods only focus on utilizing this naturally formed activation sparsity,
overlooking the potential for further amplifying this inherent sparsity. In
this paper, we hypothesize that LLMs can learn to be efficient by achieving
more structured activation sparsity. To achieve this, we introduce a novel
algorithm, Learn-To-be-Efficient (LTE), designed to train efficiency-aware LLMs
to learn to activate fewer neurons and achieve a better trade-off between
sparsity and performance. Furthermore, unlike SOTA MoEfication methods, which
mainly focus on ReLU-based models, LTE can also be applied to LLMs like GPT and
LLaMA with soft activation functions. We evaluate LTE on four models and eleven
datasets. The experiments show that LTE achieves a better trade-off between
sparsity and task performance. For instance, LTE with LLaMA provides a
1.83x-2.59x FLOPs speed-up on language generation tasks, outperforming the
state-of-the-art methods.",http://arxiv.org/abs/2402.06126v2,94,20,18,20,18,18
Generative Modeling of Discrete Joint Distributions by E-Geodesic Flow Matching on Assignment Manifolds,"This paper introduces a novel generative model for discrete distributions
based on continuous normalizing flows on the submanifold of factorizing
discrete measures. Integration of the flow gradually assigns categories and
avoids issues of discretizing the latent continuous model like rounding, sample
truncation etc. General non-factorizing discrete distributions capable of
representing complex statistical dependencies of structured discrete data, can
be approximated by embedding the submanifold into a the meta-simplex of all
joint discrete distributions and data-driven averaging. Efficient training of
the generative model is demonstrated by matching the flow of geodesics of
factorizing discrete distributions. Various experiments underline the
approach's broad applicability.",http://arxiv.org/abs/2402.07846v1,93,18,20,20,17,18
Clients Collaborate: Flexible Differentially Private Federated Learning with Guaranteed Improvement of Utility-Privacy Trade-off,"To defend against privacy leakage of user data, differential privacy is
widely used in federated learning, but it is not free. The addition of noise
randomly disrupts the semantic integrity of the model and this disturbance
accumulates with increased communication rounds. In this paper, we introduce a
novel federated learning framework with rigorous privacy guarantees, named
FedCEO, designed to strike a trade-off between model utility and user privacy
by letting clients ''Collaborate with Each Other''. Specifically, we perform
efficient tensor low-rank proximal optimization on stacked local model
parameters at the server, demonstrating its capability to flexibly truncate
high-frequency components in spectral space. This implies that our FedCEO can
effectively recover the disrupted semantic information by smoothing the global
semantic space for different privacy settings and continuous training
processes. Moreover, we improve the SOTA utility-privacy trade-off bound by an
order of $\sqrt{d}$, where $d$ is the input dimension. We illustrate our
theoretical results with experiments on representative image datasets. It
observes significant performance improvements and strict privacy guarantees
under different privacy settings.",http://arxiv.org/abs/2402.07002v1,93,20,18,20,17,18
OS-Copilot: Towards Generalist Computer Agents with Self-Improvement,"Autonomous interaction with the computer has been a longstanding challenge
with great potential, and the recent proliferation of large language models
(LLMs) has markedly accelerated progress in building digital agents. However,
most of these agents are designed to interact with a narrow domain, such as a
specific software or website. This narrow focus constrains their applicability
for general computer tasks. To this end, we introduce OS-Copilot, a framework
to build generalist agents capable of interfacing with comprehensive elements
in an operating system (OS), including the web, code terminals, files,
multimedia, and various third-party applications. We use OS-Copilot to create
FRIDAY, a self-improving embodied agent for automating general computer tasks.
On GAIA, a general AI assistants benchmark, FRIDAY outperforms previous methods
by 35%, showcasing strong generalization to unseen applications via accumulated
skills from previous tasks. We also present numerical and quantitative evidence
that FRIDAY learns to control and self-improve on Excel and Powerpoint with
minimal supervision. Our OS-Copilot framework and empirical findings provide
infrastructure and insights for future research toward more capable and
general-purpose computer agents.",http://arxiv.org/abs/2402.07456v1,93,20,20,18,17,18
Twenty Constructionist Things to Do with Artificial Intelligence and Machine Learning,"In this paper, we build on the 1971 memo ""Twenty Things to Do With a
Computer"" by Seymour Papert and Cynthia Solomon and propose twenty
constructionist things to do with artificial intelligence and machine learning.
Several proposals build on ideas developed in the original memo while others
are new and address topics in science, mathematics, and the arts. In reviewing
the big themes, we notice a renewed interest in children's engagement not just
for technical proficiency but also to cultivate a deeper understanding of their
own cognitive processes. Furthermore, the ideas stress the importance of
designing personally relevant AI/ML applications, moving beyond isolated models
and off-the-shelf datasets disconnected from their interests. We also
acknowledge the social aspects of data production involved in making AI/ML
applications. Finally, we highlight the critical dimensions necessary to
address potential harmful algorithmic biases and consequences of AI/ML
applications.",http://arxiv.org/abs/2402.06775v1,92,18,20,18,18,18
Whispers in the Machine: Confidentiality in LLM-integrated Systems,"Large Language Models (LLMs) are increasingly integrated with external tools.
While these integrations can significantly improve the functionality of LLMs,
they also create a new attack surface where confidential data may be disclosed
between different components. Specifically, malicious tools can exploit
vulnerabilities in the LLM itself to manipulate the model and compromise the
data of other services, raising the question of how private data can be
protected in the context of LLM integrations.
  In this work, we provide a systematic way of evaluating confidentiality in
LLM-integrated systems. For this, we formalize a ""secret key"" game that can
capture the ability of a model to conceal private information. This enables us
to compare the vulnerability of a model against confidentiality attacks and
also the effectiveness of different defense strategies. In this framework, we
evaluate eight previously published attacks and four defenses. We find that
current defenses lack generalization across attack strategies. Building on this
analysis, we propose a method for robustness fine-tuning, inspired by
adversarial training. This approach is effective in lowering the success rate
of attackers and in improving the system's resilience against unknown attacks.",http://arxiv.org/abs/2402.06922v1,92,18,18,18,18,20
Deceptive Path Planning via Reinforcement Learning with Graph Neural Networks,"Deceptive path planning (DPP) is the problem of designing a path that hides
its true goal from an outside observer. Existing methods for DPP rely on
unrealistic assumptions, such as global state observability and perfect model
knowledge, and are typically problem-specific, meaning that even minor changes
to a previously solved problem can force expensive computation of an entirely
new solution. Given these drawbacks, such methods do not generalize to unseen
problem instances, lack scalability to realistic problem sizes, and preclude
both on-the-fly tunability of deception levels and real-time adaptivity to
changing environments. In this paper, we propose a reinforcement learning
(RL)-based scheme for training policies to perform DPP over arbitrary weighted
graphs that overcomes these issues. The core of our approach is the
introduction of a local perception model for the agent, a new state space
representation distilling the key components of the DPP problem, the use of
graph neural network-based policies to facilitate generalization and scaling,
and the introduction of new deception bonuses that translate the deception
objectives of classical methods to the RL setting. Through extensive
experimentation we show that, without additional fine-tuning, at test time the
resulting policies successfully generalize, scale, enjoy tunable levels of
deception, and adapt in real-time to changes in the environment.",http://arxiv.org/abs/2402.06552v1,92,20,18,20,18,16
LiFi: Lightweight Controlled Text Generation with Fine-Grained Control Codes,"In the rapidly evolving field of text generation, the demand for more precise
control mechanisms has become increasingly apparent. To address this need, we
present a novel methodology, LIFI, which offers a lightweight approach with
fine-grained control for controlled text generation. Unlike previous studies
that train pre-trained language models to follow discrete, categorical, and
exclusive control codes, LIFI learns controlled text generation under the
guidance of continuous, relative, and nonexclusive control codes. These
fine-grained codes are automatically derived from an attribute classifier,
initially trained with a small amount of labeled data and subsequently employed
to label abundant unlabeled data, thus garnering more extensive supervision
signals. Moreover, to achieve efficient control, we incorporate the
fine-grained control codes with adapters, a parameter- and compute-efficient
way to steer a pre-trained language model. We evaluate LIFI on two conventional
tasks -- sentiment control and topic control -- and one newly proposed task --
stylistic novel writing. Comprehensive experimental results validate the
effectiveness of our proposed methods, demonstrating substantial performance
improvements over existing baselines.",http://arxiv.org/abs/2402.06930v1,92,18,20,19,18,17
ORIENT: A Priority-Aware Energy-Efficient Approach for Latency-Sensitive Applications in 6G,"Anticipation for 6G's arrival comes with growing concerns about increased
energy consumption in computing and networking. The expected surge in connected
devices and resource-demanding applications presents unprecedented challenges
for energy resources. While sustainable resource allocation strategies have
been discussed in the past, these efforts have primarily focused on
single-domain orchestration or ignored the unique requirements posed by 6G. To
address this gap, we investigate the joint problem of service instance
placement and assignment, path selection, and request prioritization, dubbed
PIRA. The objective function is to maximize the system's overall profit as a
function of the number of concurrently supported requests while simultaneously
minimizing energy consumption over an extended period of time. In addition,
end-to-end latency requirements and resource capacity constraints are
considered for computing and networking resources, where queuing theory is
utilized to estimate the Age of Information (AoI) for requests. After
formulating the problem in a non-linear fashion, we prove its NP-hardness and
propose a method, denoted ORIENT. This method is based on the Double Dueling
Deep Q-Learning (D3QL) mechanism and leverages Graph Neural Networks (GNNs) for
state encoding. Extensive numerical simulations demonstrate that ORIENT yields
near-optimal solutions for varying system sizes and request counts.",http://arxiv.org/abs/2402.06931v1,92,20,19,19,17,17
Dynamic Graph Information Bottleneck,"Dynamic Graphs widely exist in the real world, which carry complicated
spatial and temporal feature patterns, challenging their representation
learning. Dynamic Graph Neural Networks (DGNNs) have shown impressive
predictive abilities by exploiting the intrinsic dynamics. However, DGNNs
exhibit limited robustness, prone to adversarial attacks. This paper presents
the novel Dynamic Graph Information Bottleneck (DGIB) framework to learn robust
and discriminative representations. Leveraged by the Information Bottleneck
(IB) principle, we first propose the expected optimal representations should
satisfy the Minimal-Sufficient-Consensual (MSC) Condition. To compress
redundant as well as conserve meritorious information into latent
representation, DGIB iteratively directs and refines the structural and feature
information flow passing through graph snapshots. To meet the MSC Condition, we
decompose the overall IB objectives into DGIB$_{MS}$ and DGIB$_C$, in which the
DGIB$_{MS}$ channel aims to learn the minimal and sufficient representations,
with the DGIB$_{MS}$ channel guarantees the predictive consensus. Extensive
experiments on real-world and synthetic dynamic graph datasets demonstrate the
superior robustness of DGIB against adversarial attacks compared with
state-of-the-art baselines in the link prediction task. To the best of our
knowledge, DGIB is the first work to learn robust representations of dynamic
graphs grounded in the information-theoretic IB principle.",http://arxiv.org/abs/2402.06716v1,92,18,19,19,18,18
OpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning,"Trained on massive publicly available data, large language models (LLMs) have
demonstrated tremendous success across various fields. While more data
contributes to better performance, a disconcerting reality is that high-quality
public data will be exhausted in a few years. In this paper, we offer a
potential next step for contemporary LLMs: collaborative and privacy-preserving
LLM training on the underutilized distributed private data via federated
learning (FL), where multiple data owners collaboratively train a shared model
without transmitting raw data. To achieve this, we build a concise, integrated,
and research-friendly framework/codebase, named OpenFedLLM. It covers federated
instruction tuning for enhancing instruction-following capability, federated
value alignment for aligning with human values, and 7 representative FL
algorithms. Besides, OpenFedLLM supports training on diverse domains, where we
cover 8 training datasets; and provides comprehensive evaluations, where we
cover 30+ evaluation metrics. Through extensive experiments, we observe that
all FL algorithms outperform local training on training LLMs, demonstrating a
clear performance improvement across a variety of settings. Notably, in a
financial benchmark, Llama2-7B fine-tuned by applying any FL algorithm can
outperform GPT-4 by a significant margin while the model obtained through
individual training cannot, demonstrating strong motivation for clients to
participate in FL. The code is available at
https://github.com/rui-ye/OpenFedLLM.",http://arxiv.org/abs/2402.06954v1,92,20,18,20,18,16
Generative Adversarial Bayesian Optimization for Surrogate Objectives,"Offline model-based policy optimization seeks to optimize a learned surrogate
objective function without querying the true oracle objective during
optimization. However, inaccurate surrogate model predictions are frequently
encountered along the optimization trajectory. To address this limitation, we
propose generative adversarial Bayesian optimization (GABO) using adaptive
source critic regularization, a task-agnostic framework for Bayesian
optimization that employs a Lipschitz-bounded source critic model to constrain
the optimization trajectory to regions where the surrogate function is
reliable. We show that under certain assumptions for the continuous input space
prior, our algorithm dynamically adjusts the strength of the source critic
regularization. GABO outperforms existing baselines on a number of different
offline optimization tasks across a variety of scientific domains. Our code is
available at https://github.com/michael-s-yao/gabo",http://arxiv.org/abs/2402.06532v1,92,20,18,20,18,16
ACTER: Diverse and Actionable Counterfactual Sequences for Explaining and Diagnosing RL Policies,"Understanding how failure occurs and how it can be prevented in reinforcement
learning (RL) is necessary to enable debugging, maintain user trust, and
develop personalized policies. Counterfactual reasoning has often been used to
assign blame and understand failure by searching for the closest possible world
in which the failure is avoided. However, current counterfactual state
explanations in RL can only explain an outcome using just the current state
features and offer no actionable recourse on how a negative outcome could have
been prevented. In this work, we propose ACTER (Actionable Counterfactual
Sequences for Explaining Reinforcement Learning Outcomes), an algorithm for
generating counterfactual sequences that provides actionable advice on how
failure can be avoided. ACTER investigates actions leading to a failure and
uses the evolutionary algorithm NSGA-II to generate counterfactual sequences of
actions that prevent it with minimal changes and high certainty even in
stochastic environments. Additionally, ACTER generates a set of multiple
diverse counterfactual sequences that enable users to correct failure in the
way that best fits their preferences. We also introduce three diversity metrics
that can be used for evaluating the diversity of counterfactual sequences. We
evaluate ACTER in two RL environments, with both discrete and continuous
actions, and show that it can generate actionable and diverse counterfactual
sequences. We conduct a user study to explore how explanations generated by
ACTER help users identify and correct failure.",http://arxiv.org/abs/2402.06503v1,92,18,17,19,19,19
Oriented-grid Encoder for 3D Implicit Representations,"Encoding 3D points is one of the primary steps in learning-based implicit
scene representation. Using features that gather information from neighbors
with multi-resolution grids has proven to be the best geometric encoder for
this task. However, prior techniques do not exploit some characteristics of
most objects or scenes, such as surface normals and local smoothness. This
paper is the first to exploit those 3D characteristics in 3D geometric encoders
explicitly. In contrast to prior work on using multiple levels of details,
regular cube grids, and trilinear interpolation, we propose 3D-oriented grids
with a novel cylindrical volumetric interpolation for modeling local planar
invariance. In addition, we explicitly include a local feature aggregation for
feature regularization and smoothing of the cylindrical interpolation features.
We evaluate our approach on ABC, Thingi10k, ShapeNet, and Matterport3D, for
object and scene representation. Compared to the use of regular grids, our
geometric encoder is shown to converge in fewer steps and obtain sharper 3D
surfaces. When compared to the prior techniques, our method gets
state-of-the-art results.",http://arxiv.org/abs/2402.06752v1,92,20,18,18,18,18
Training dynamics in Physics-Informed Neural Networks with feature mapping,"Physics-Informed Neural Networks (PINNs) have emerged as an iconic machine
learning approach for solving Partial Differential Equations (PDEs). Although
its variants have achieved significant progress, the empirical success of
utilising feature mapping from the wider Implicit Neural Representations
studies has been substantially neglected. We investigate the training dynamics
of PINNs with a feature mapping layer via the limiting Conjugate Kernel and
Neural Tangent Kernel, which sheds light on the convergence and generalisation
of the model. We also show the inadequacy of commonly used Fourier-based
feature mapping in some scenarios and propose the conditional positive definite
Radial Basis Function as a better alternative. The empirical results reveal the
efficacy of our method in diverse forward and inverse problem sets. This simple
technique can be easily implemented in coordinate input networks and benefits
the broad PINNs research.",http://arxiv.org/abs/2402.06955v1,92,18,20,18,18,18
Re-Envisioning Command and Control,"Future warfare will require Command and Control (C2) decision-making to occur
in more complex, fast-paced, ill-structured, and demanding conditions. C2 will
be further complicated by operational challenges such as Denied, Degraded,
Intermittent, and Limited (DDIL) communications and the need to account for
many data streams, potentially across multiple domains of operation. Yet,
current C2 practices -- which stem from the industrial era rather than the
emerging intelligence era -- are linear and time-consuming. Critically, these
approaches may fail to maintain overmatch against adversaries on the future
battlefield. To address these challenges, we propose a vision for future C2
based on robust partnerships between humans and artificial intelligence (AI)
systems. This future vision is encapsulated in three operational impacts:
streamlining the C2 operations process, maintaining unity of effort, and
developing adaptive collective knowledge systems. This paper illustrates the
envisaged future C2 capabilities, discusses the assumptions that shaped them,
and describes how the proposed developments could transform C2 in future
warfare.",http://arxiv.org/abs/2402.07946v1,92,20,18,20,18,16
DeepCover: Advancing RNN Test Coverage and Online Error Prediction using State Machine Extraction,"Recurrent neural networks (RNNs) have emerged as powerful tools for
processing sequential data in various fields, including natural language
processing and speech recognition. However, the lack of explainability in RNN
models has limited their interpretability, posing challenges in understanding
their internal workings. To address this issue, this paper proposes a
methodology for extracting a state machine (SM) from an RNN-based model to
provide insights into its internal function. The proposed SM extraction
algorithm was assessed using four newly proposed metrics: Purity, Richness,
Goodness, and Scale. The proposed methodology along with its assessment metrics
contribute to increasing explainability in RNN models by providing a clear
representation of their internal decision making process through the extracted
SM. In addition to improving the explainability of RNNs, the extracted SM can
be used to advance testing and and monitoring of the primary RNN-based model.
To enhance RNN testing, we introduce six model coverage criteria based on the
extracted SM, serving as metrics for evaluating the effectiveness of test
suites designed to analyze the primary model. We also propose a tree-based
model to predict the error probability of the primary model for each input
based on the extracted SM. We evaluated our proposed online error prediction
approach using the MNIST dataset and Mini Speech Commands dataset, achieving an
area under the curve (AUC) exceeding 80\% for the receiver operating
characteristic (ROC) chart.",http://dx.doi.org/10.1016/j.jss.2024.111987,92,18,18,18,20,18
In-Context Data Distillation with TabPFN,"Foundation models have revolutionized tasks in computer vision and natural
language processing. However, in the realm of tabular data, tree-based models
like XGBoost continue to dominate. TabPFN, a transformer model tailored for
tabular data, mirrors recent foundation models in its exceptional in-context
learning capability, being competitive with XGBoost's performance without the
need for task-specific training or hyperparameter tuning. Despite its promise,
TabPFN's applicability is hindered by its data size constraint, limiting its
use in real-world scenarios. To address this, we present in-context data
distillation (ICD), a novel methodology that effectively eliminates these
constraints by optimizing TabPFN's context. ICD efficiently enables TabPFN to
handle significantly larger datasets with a fixed memory budget, improving
TabPFN's quadratic memory complexity but at the cost of a linear number of
tuning steps. Notably, TabPFN, enhanced with ICD, demonstrates very strong
performance against established tree-based models and modern deep learning
methods on 48 large tabular datasets from OpenML.",http://arxiv.org/abs/2402.06971v1,92,18,20,20,18,16
Non-linear Fusion in Federated Learning: A Hypernetwork Approach to Federated Domain Generalization,"Federated Learning (FL) has emerged as a promising paradigm in which multiple
clients collaboratively train a shared global model while preserving data
privacy. To create a robust and practicable FL framework, it is crucial to
extend its ability to generalize well to unseen domains - a problem referred to
as federated Domain Generalization (FDG), being still under-explored. We
propose an innovative federated algorithm, termed hFedF for hypernetwork-based
Federated Fusion, designed to bridge the performance gap between generalization
and personalization, capable of addressing various degrees of domain shift.
Essentially, the hypernetwork supports a non-linear fusion of client models
enabling a comprehensive understanding of the underlying data distribution. We
encompass an extensive discussion and provide novel insights into the tradeoff
between personalization and generalization in FL. The proposed algorithm
outperforms strong benchmarks on three widely-used data sets for DG in an
exceeding number of cases.",http://arxiv.org/abs/2402.06974v2,92,19,18,19,18,18
BarlowTwins-CXR : Enhancing Chest X-Ray abnormality localization in heterogeneous data with cross-domain self-supervised learning,"Background: Chest X-ray imaging-based abnormality localization, essential in
diagnosing various diseases, faces significant clinical challenges due to
complex interpretations and the growing workload of radiologists. While recent
advances in deep learning offer promising solutions, there is still a critical
issue of domain inconsistency in cross-domain transfer learning, which hampers
the efficiency and accuracy of diagnostic processes. This study aims to address
the domain inconsistency problem and improve autonomic abnormality localization
performance of heterogeneous chest X-ray image analysis, by developing a
self-supervised learning strategy called ""BarlwoTwins-CXR"". Methods: We
utilized two publicly available datasets: the NIH Chest X-ray Dataset and the
VinDr-CXR. The BarlowTwins-CXR approach was conducted in a two-stage training
process. Initially, self-supervised pre-training was performed using an
adjusted Barlow Twins algorithm on the NIH dataset with a Resnet50 backbone
pre-trained on ImageNet. This was followed by supervised fine-tuning on the
VinDr-CXR dataset using Faster R-CNN with Feature Pyramid Network (FPN).
Results: Our experiments showed a significant improvement in model performance
with BarlowTwins-CXR. The approach achieved a 3% increase in mAP50 accuracy
compared to traditional ImageNet pre-trained models. In addition, the Ablation
CAM method revealed enhanced precision in localizing chest abnormalities.
Conclusion: BarlowTwins-CXR significantly enhances the efficiency and accuracy
of chest X-ray image-based abnormality localization, outperforming traditional
transfer learning methods and effectively overcoming domain inconsistency in
cross-domain scenarios. Our experiment results demonstrate the potential of
using self-supervised learning to improve the generalizability of models in
medical settings with limited amounts of heterogeneous data.",http://arxiv.org/abs/2402.06499v1,92,18,20,20,16,18
Inducing Systematicity in Transformers by Attending to Structurally Quantized Embeddings,"Transformers generalize to novel compositions of structures and entities
after being trained on a complex dataset, but easily overfit on datasets of
insufficient complexity. We observe that when the training set is sufficiently
complex, the model encodes sentences that have a common syntactic structure
using a systematic attention pattern. Inspired by this observation, we propose
SQ-Transformer (Structurally Quantized) that explicitly encourages
systematicity in the embeddings and attention layers, even with a training set
of low complexity. At the embedding level, we introduce Structure-oriented
Vector Quantization (SoVQ) to cluster word embeddings into several classes of
structurally equivalent entities. At the attention level, we devise the
Systematic Attention Layer (SAL) and an alternative, Systematically Regularized
Layer (SRL) that operate on the quantized word embeddings so that sentences of
the same structure are encoded with invariant or similar attention patterns.
Empirically, we show that SQ-Transformer achieves stronger compositional
generalization than the vanilla Transformer on multiple low-complexity semantic
parsing and machine translation datasets. In our analysis, we show that SoVQ
indeed learns a syntactically clustered embedding space and SAL/SRL induces
generalizable attention patterns, which lead to improved systematicity.",http://arxiv.org/abs/2402.06492v1,92,20,18,18,18,18
Designing for Work with Intelligent Entities: A Review of Perspectives,"As the power of Artificial Intelligence (AI) continues to advance, there is
increased interest in how best to combine AI-based agents with humans to
achieve mission effectiveness. Three perspectives have emerged. The first stems
from more conventional human factors traditions and views these entities as
highly capable tools that humans can use to accomplish increasingly
sophisticated tasks. The second ""camp"" believes that as the sophistication of
these entities increases, it becomes increasingly appropriate to talk about
them as ""teammates"" and use the research on human teams as a foundation for
further exploration. The third perspective is emerging and finds both the
""tools"" and ""teammate"" metaphors flawed and limiting. This perspective
emphasizes ""joint activity,"" ""joint cognitive activity,"" or something similar.
In this article, we briefly review these three perspectives.",http://arxiv.org/abs/2402.06989v1,92,18,20,18,18,18
Scalable Interactive Machine Learning for Future Command and Control,"Future warfare will require Command and Control (C2) personnel to make
decisions at shrinking timescales in complex and potentially ill-defined
situations. Given the need for robust decision-making processes and
decision-support tools, integration of artificial and human intelligence holds
the potential to revolutionize the C2 operations process to ensure adaptability
and efficiency in rapidly changing operational environments. We propose to
leverage recent promising breakthroughs in interactive machine learning, in
which humans can cooperate with machine learning algorithms to guide machine
learning algorithm behavior. This paper identifies several gaps in
state-of-the-art science and technology that future work should address to
extend these approaches to function in complex C2 contexts. In particular, we
describe three research focus areas that together, aim to enable scalable
interactive machine learning (SIML): 1) developing human-AI interaction
algorithms to enable planning in complex, dynamic situations; 2) fostering
resilient human-AI teams through optimizing roles, configurations, and trust;
and 3) scaling algorithms and human-AI teams for flexibility across a range of
potential contexts and situations.",http://arxiv.org/abs/2402.06501v1,92,20,19,19,17,17
TREET: TRansfer Entropy Estimation via Transformer,"Transfer entropy (TE) is a measurement in information theory that reveals the
directional flow of information between processes, providing valuable insights
for a wide range of real-world applications. This work proposes Transfer
Entropy Estimation via Transformers (TREET), a novel transformer-based approach
for estimating the TE for stationary processes. The proposed approach employs
Donsker-Vardhan (DV) representation to TE and leverages the attention mechanism
for the task of neural estimation. We propose a detailed theoretical and
empirical study of the TREET, comparing it to existing methods. To increase its
applicability, we design an estimated TE optimization scheme that is motivated
by the functional representation lemma. Afterwards, we take advantage of the
joint optimization scheme to optimize the capacity of communication channels
with memory, which is a canonical optimization problem in information theory,
and show the memory capabilities of our estimator. Finally, we apply TREET to
real-world feature analysis. Our work, applied with state-of-the-art deep
learning methods, opens a new door for communication problems which are yet to
be solved.",http://arxiv.org/abs/2402.06919v1,92,20,18,20,16,18
Video Annotator: A framework for efficiently building video classifiers using vision-language models and active learning,"High-quality and consistent annotations are fundamental to the successful
development of robust machine learning models. Traditional data annotation
methods are resource-intensive and inefficient, often leading to a reliance on
third-party annotators who are not the domain experts. Hard samples, which are
usually the most informative for model training, tend to be difficult to label
accurately and consistently without business context. These can arise
unpredictably during the annotation process, requiring a variable number of
iterations and rounds of feedback, leading to unforeseen expenses and time
commitments to guarantee quality.
  We posit that more direct involvement of domain experts, using a
human-in-the-loop system, can resolve many of these practical challenges. We
propose a novel framework we call Video Annotator (VA) for annotating,
managing, and iterating on video classification datasets. Our approach offers a
new paradigm for an end-user-centered model development process, enhancing the
efficiency, usability, and effectiveness of video classifiers. Uniquely, VA
allows for a continuous annotation process, seamlessly integrating data
collection and model training.
  We leverage the zero-shot capabilities of vision-language foundation models
combined with active learning techniques, and demonstrate that VA enables the
efficient creation of high-quality models. VA achieves a median 6.8 point
improvement in Average Precision relative to the most competitive baseline
across a wide-ranging assortment of tasks. We release a dataset with 153k
labels across 56 video understanding tasks annotated by three professional
video editors using VA, and also release code to replicate our experiments at:
http://github.com/netflix/videoannotator.",http://arxiv.org/abs/2402.06560v1,92,20,18,20,18,16
Understanding the Effects of Iterative Prompting on Truthfulness,"The development of Large Language Models (LLMs) has notably transformed
numerous sectors, offering impressive text generation capabilities. Yet, the
reliability and truthfulness of these models remain pressing concerns. To this
end, we investigate iterative prompting, a strategy hypothesized to refine LLM
responses, assessing its impact on LLM truthfulness, an area which has not been
thoroughly explored. Our extensive experiments delve into the intricacies of
iterative prompting variants, examining their influence on the accuracy and
calibration of model responses. Our findings reveal that naive prompting
methods significantly undermine truthfulness, leading to exacerbated
calibration errors. In response to these challenges, we introduce several
prompting variants designed to address the identified issues. These variants
demonstrate marked improvements over existing baselines, signaling a promising
direction for future research. Our work provides a nuanced understanding of
iterative prompting and introduces novel approaches to enhance the truthfulness
of LLMs, thereby contributing to the development of more accurate and
trustworthy AI systems.",http://arxiv.org/abs/2402.06625v1,92,18,20,18,18,18
Low-Rank Learning by Design: the Role of Network Architecture and Activation Linearity in Gradient Rank Collapse,"Our understanding of learning dynamics of deep neural networks (DNNs) remains
incomplete. Recent research has begun to uncover the mathematical principles
underlying these networks, including the phenomenon of ""Neural Collapse"", where
linear classifiers within DNNs converge to specific geometrical structures
during late-stage training. However, the role of geometric constraints in
learning extends beyond this terminal phase. For instance, gradients in
fully-connected layers naturally develop a low-rank structure due to the
accumulation of rank-one outer products over a training batch. Despite the
attention given to methods that exploit this structure for memory saving or
regularization, the emergence of low-rank learning as an inherent aspect of
certain DNN architectures has been under-explored. In this paper, we conduct a
comprehensive study of gradient rank in DNNs, examining how architectural
choices and structure of the data effect gradient rank bounds. Our theoretical
analysis provides these bounds for training fully-connected, recurrent, and
convolutional neural networks. We also demonstrate, both theoretically and
empirically, how design choices like activation function linearity, bottleneck
layer introduction, convolutional stride, and sequence truncation influence
these bounds. Our findings not only contribute to the understanding of learning
dynamics in DNNs, but also provide practical guidance for deep learning
engineers to make informed design decisions.",http://arxiv.org/abs/2402.06751v1,92,18,18,18,20,18
ForestColl: Efficient Collective Communications on Heterogeneous Network Fabrics,"As modern DNN models grow ever larger, collective communications between the
accelerators (allreduce, etc.) emerge as a significant performance bottleneck.
Designing efficient communication schedules is challenging given today's highly
diverse and heterogeneous network fabrics. In this paper, we present
ForestColl, a tool that generates efficient schedules for any network topology.
ForestColl constructs broadcast/aggregation spanning trees as the communication
schedule, achieving theoretically minimum network congestion. Its schedule
generation runs in strongly polynomial time and is highly scalable. ForestColl
supports any network fabrics, including both switching fabrics and direct
connections, as well as any network graph structure. We evaluated ForestColl on
multi-cluster AMD MI250 and NVIDIA A100 platforms. ForestColl's schedules
achieved up to 52\% higher performance compared to the vendors' own optimized
communication libraries, RCCL and NCCL. ForestColl also outperforms other
state-of-the-art schedule generation techniques with both up to 61\% more
efficient generated schedules and orders of magnitude faster schedule
generation speed.",http://arxiv.org/abs/2402.06787v1,92,19,18,19,20,16
ExGRG: Explicitly-Generated Relation Graph for Self-Supervised Representation Learning,"Self-supervised Learning (SSL) has emerged as a powerful technique in
pre-training deep learning models without relying on expensive annotated
labels, instead leveraging embedded signals in unlabeled data. While SSL has
shown remarkable success in computer vision tasks through intuitive data
augmentation, its application to graph-structured data poses challenges due to
the semantic-altering and counter-intuitive nature of graph augmentations.
Addressing this limitation, this paper introduces a novel non-contrastive SSL
approach to Explicitly Generate a compositional Relation Graph (ExGRG) instead
of relying solely on the conventional augmentation-based implicit relation
graph. ExGRG offers a framework for incorporating prior domain knowledge and
online extracted information into the SSL invariance objective, drawing
inspiration from the Laplacian Eigenmap and Expectation-Maximization (EM).
Employing an EM perspective on SSL, our E-step involves relation graph
generation to identify candidates to guide the SSL invariance objective, and
M-step updates the model parameters by integrating the derived relational
information. Extensive experimentation on diverse node classification datasets
demonstrates the superiority of our method over state-of-the-art techniques,
affirming ExGRG as an effective adoption of SSL for graph representation
learning.",http://arxiv.org/abs/2402.06737v1,92,20,20,18,18,16
Corruption Robust Offline Reinforcement Learning with Human Feedback,"We study data corruption robustness for reinforcement learning with human
feedback (RLHF) in an offline setting. Given an offline dataset of pairs of
trajectories along with feedback about human preferences, an
$\varepsilon$-fraction of the pairs is corrupted (e.g., feedback flipped or
trajectory features manipulated), capturing an adversarial attack or noisy
human preferences. We aim to design algorithms that identify a near-optimal
policy from the corrupted data, with provable guarantees. Existing theoretical
works have separately studied the settings of corruption robust RL (learning
from scalar rewards directly under corruption) and offline RLHF (learning from
human feedback without corruption); however, they are inapplicable to our
problem of dealing with corrupted data in offline RLHF setting. To this end, we
design novel corruption robust offline RLHF methods under various assumptions
on the coverage of the data-generating distributions. At a high level, our
methodology robustifies an offline RLHF framework by first learning a reward
model along with confidence sets and then learning a pessimistic optimal policy
over the confidence set. Our key insight is that learning optimal policy can be
done by leveraging an offline corruption-robust RL oracle in different ways
(e.g., zero-order oracle or first-order oracle), depending on the data coverage
assumptions. To our knowledge, ours is the first work that provides provable
corruption robust offline RLHF methods.",http://arxiv.org/abs/2402.06734v1,92,19,18,18,19,18
NICE: To Optimize In-Context Examples or Not?,"Recent works have shown that large language models (LLMs) work remarkably
well on a wide range of tasks through in-context learning and optimization of
in-context examples (ICE). However, most of these studies assume either a fixed
or no instruction provided in the prompt, leading to the apparent consensus
that the optimization of in-context examples is critical for better
performance. We challenge this consensus for instruction-tuned LLMs by
investigating the necessity of optimizing in-context examples when
task-specific instructions are provided, and find that there are tasks for
which various ways of optimizing in-context examples yield diminishing returns.
We introduce a task-specific metric called \metriclong{} (\metric) that
quantifies the learnability of tasks from a given instruction, and provides a
heuristic that helps decide whether to optimize for instructions or ICE for any
new task. On a wide range of tasks and a systematically created instruction set
with gradually added details, we validate our hypothesis empirically by
computing \metric with query-dependent bins of examples, comparing different
instructions with ICE selection methods, and performing label perturbation
experiments. We conclude that tasks can be divided into two broad classes based
on the \metric metric, where the returns on ICE optimization follow predictable
trends when instructions are provided in the prompt.",http://arxiv.org/abs/2402.06733v1,92,18,20,18,18,18
Feedback Loops With Language Models Drive In-Context Reward Hacking,"Language models influence the external world: they query APIs that read and
write to web pages, generate content that shapes human behavior, and run system
commands as autonomous agents. These interactions form feedback loops: LLM
outputs affect the world, which in turn affect subsequent LLM outputs. In this
work, we show that feedback loops can cause in-context reward hacking (ICRH),
where the LLM at test-time optimizes a (potentially implicit) objective but
creates negative side effects in the process. For example, consider an LLM
agent deployed to increase Twitter engagement; the LLM may retrieve its
previous tweets into the context window and make them more controversial,
increasing engagement but also toxicity. We identify and study two processes
that lead to ICRH: output-refinement and policy-refinement. For these
processes, evaluations on static datasets are insufficient -- they miss the
feedback effects and thus cannot capture the most harmful behavior. In
response, we provide three recommendations for evaluation to capture more
instances of ICRH. As AI development accelerates, the effects of feedback loops
will proliferate, increasing the need to understand their role in shaping LLM
behavior.",http://arxiv.org/abs/2402.06627v1,92,18,19,19,18,18
RAMP: Boosting Adversarial Robustness Against Multiple $l_p$ Perturbations,"There is considerable work on improving robustness against adversarial
attacks bounded by a single $l_p$ norm using adversarial training (AT).
However, the multiple-norm robustness (union accuracy) of AT models is still
low. We observe that simultaneously obtaining good union and clean accuracy is
hard since there are tradeoffs between robustness against multiple $l_p$
perturbations, and accuracy/robustness/efficiency. By analyzing the tradeoffs
from the lens of distribution shifts, we identify the key tradeoff pair among
$l_p$ attacks to boost efficiency and design a logit pairing loss to improve
the union accuracy. Next, we connect natural training with AT via gradient
projection, to find and incorporate useful information from natural training
into AT, which moderates the accuracy/robustness tradeoff. Combining our
contributions, we propose a framework called \textbf{RAMP}, to boost the
robustness against multiple $l_p$ perturbations. We show \textbf{RAMP} can be
easily adapted for both robust fine-tuning and full AT. For robust fine-tuning,
\textbf{RAMP} obtains a union accuracy up to $53.5\%$ on CIFAR-10, and $29.7\%$
on ImageNet. For training from scratch, \textbf{RAMP} achieves SOTA union
accuracy of $44.6\%$ and relatively good clean accuracy of $81.2\%$ on
ResNet-18 against AutoAttack on CIFAR-10.",http://arxiv.org/abs/2402.06827v1,92,18,18,18,18,20
ChemLLM: A Chemical Large Language Model,"Large language models (LLMs) have made impressive progress in chemistry
applications, including molecular property prediction, molecular generation,
experimental protocol design, etc. However, the community lacks a
dialogue-based model specifically designed for chemistry. The challenge arises
from the fact that most chemical data and scientific knowledge are primarily
stored in structured databases, and the direct use of these structured data
compromises the model's ability to maintain coherent dialogue. To tackle this
issue, we develop a novel template-based instruction construction method that
transforms structured knowledge into plain dialogue, making it suitable for
language model training. By leveraging this approach, we develop ChemLLM, the
first large language model dedicated to chemistry, capable of performing
various tasks across chemical disciplines with smooth dialogue interaction.
ChemLLM beats GPT-3.5 on all three principal tasks in chemistry, i.e., name
conversion, molecular caption, and reaction prediction, and surpasses GPT-4 on
two of them. Remarkably, ChemLLM also shows exceptional adaptability to related
mathematical and physical tasks despite being trained mainly on
chemical-centric corpora. Furthermore, ChemLLM demonstrates proficiency in
specialized NLP tasks within chemistry, such as literature translation and
cheminformatic programming. ChemLLM opens up a new avenue for exploration
within chemical studies, while our method of integrating structured chemical
knowledge into dialogue systems sets a new frontier for developing LLMs across
various scientific fields. Codes, Datasets, and Model weights are publicly
accessible at hf.co/AI4Chem/ChemLLM-7B-Chat.",http://arxiv.org/abs/2402.06852v1,92,20,18,20,18,16
Distilling Morphology-Conditioned Hypernetworks for Efficient Universal Morphology Control,"Learning a universal policy across different robot morphologies can
significantly improve learning efficiency and enable zero-shot generalization
to unseen morphologies. However, learning a highly performant universal policy
requires sophisticated architectures like transformers (TF) that have larger
memory and computational cost than simpler multi-layer perceptrons (MLP). To
achieve both good performance like TF and high efficiency like MLP at inference
time, we propose HyperDistill, which consists of: (1) A morphology-conditioned
hypernetwork (HN) that generates robot-wise MLP policies, and (2) A policy
distillation approach that is essential for successful training. We show that
on UNIMAL, a benchmark with hundreds of diverse morphologies, HyperDistill
performs as well as a universal TF teacher policy on both training and unseen
test robots, but reduces model size by 6-14 times, and computational cost by
67-160 times in different environments. Our analysis attributes the efficiency
advantage of HyperDistill at inference time to knowledge decoupling, i.e., the
ability to decouple inter-task and intra-task knowledge, a general principle
that could also be applied to improve inference efficiency in other domains.",http://arxiv.org/abs/2402.06570v1,92,20,18,20,16,18
Gyroscope-Assisted Motion Deblurring Network,"Image research has shown substantial attention in deblurring networks in
recent years. Yet, their practical usage in real-world deblurring, especially
motion blur, remains limited due to the lack of pixel-aligned training triplets
(background, blurred image, and blur heat map) and restricted information
inherent in blurred images. This paper presents a simple yet efficient
framework to synthetic and restore motion blur images using Inertial
Measurement Unit (IMU) data. Notably, the framework includes a strategy for
training triplet generation, and a Gyroscope-Aided Motion Deblurring (GAMD)
network for blurred image restoration. The rationale is that through harnessing
IMU data, we can determine the transformation of the camera pose during the
image exposure phase, facilitating the deduction of the motion trajectory (aka.
blur trajectory) for each point inside the three-dimensional space. Thus, the
synthetic triplets using our strategy are inherently close to natural motion
blur, strictly pixel-aligned, and mass-producible. Through comprehensive
experiments, we demonstrate the advantages of the proposed framework: only
two-pixel errors between our synthetic and real-world blur trajectories, a
marked improvement (around 33.17%) of the state-of-the-art deblurring method
MIMO on Peak Signal-to-Noise Ratio (PSNR).",http://arxiv.org/abs/2402.06854v1,92,20,20,18,16,18
Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance,"The advancement of Large Vision-Language Models (LVLMs) has increasingly
highlighted the critical issue of their tendency to hallucinate non-existing
objects in the images. To address this issue, previous works focused on using
specially curated datasets or powerful LLMs (e.g., GPT-3.5) to rectify the
outputs of LVLMs. However, these approaches require either expensive
training/fine-tuning or API access to advanced LLMs to correct the model's
output post-generation. In this paper, we tackle this challenge by introducing
a framework called Mitigating hallucinAtion via classifieR-Free guIdaNcE
(MARINE), which is both training-free and API-free, and can effectively and
efficiently reduce object hallucinations during the generation process.
Specifically, MARINE enriches the visual context of LVLMs by integrating
existing open-source vision models, and employs classifier-free guidance to
incorporate the additional object grounding features to improve the precision
of LVLMs' generations. Through comprehensive evaluations across $6$ popular
LVLMs with diverse evaluation metrics, we demonstrate the effectiveness of
MARINE, which even outperforms existing fine-tuning-based methods. Remarkably,
it not only reduces hallucinations but also improves the detailedness of LVLMs'
generations, as assessed by GPT-4V.",http://arxiv.org/abs/2402.08680v1,92,20,19,19,16,18
FaBERT: Pre-training BERT on Persian Blogs,"We introduce FaBERT, a Persian BERT-base model pre-trained on the HmBlogs
corpus, encompassing both informal and formal Persian texts. FaBERT is designed
to excel in traditional Natural Language Understanding (NLU) tasks, addressing
the intricacies of diverse sentence structures and linguistic styles prevalent
in the Persian language. In our comprehensive evaluation of FaBERT on 12
datasets in various downstream tasks, encompassing Sentiment Analysis (SA),
Named Entity Recognition (NER), Natural Language Inference (NLI), Question
Answering (QA), and Question Paraphrasing (QP), it consistently demonstrated
improved performance, all achieved within a compact model size. The findings
highlight the importance of utilizing diverse and cleaned corpora, such as
HmBlogs, to enhance the performance of language models like BERT in Persian
Natural Language Processing (NLP) applications. FaBERT is openly accessible at
https://huggingface.co/sbunlp/fabert",http://arxiv.org/abs/2402.06617v1,92,20,18,18,18,18
GenTranslate: Large Language Models are Generative Multilingual Speech and Machine Translators,"Recent advances in large language models (LLMs) have stepped forward the
development of multilingual speech and machine translation by its reduced
representation errors and incorporated external knowledge. However, both
translation tasks typically utilize beam search decoding and top-1 hypothesis
selection for inference. These techniques struggle to fully exploit the rich
information in the diverse N-best hypotheses, making them less optimal for
translation tasks that require a single, high-quality output sequence. In this
paper, we propose a new generative paradigm for translation tasks, namely
""GenTranslate"", which builds upon LLMs to generate better results from the
diverse translation versions in N-best list. Leveraging the rich linguistic
knowledge and strong reasoning abilities of LLMs, our new paradigm can
integrate the rich information in N-best candidates to generate a
higher-quality translation result. Furthermore, to support LLM finetuning, we
build and release a HypoTranslate dataset that contains over 592K
hypotheses-translation pairs in 11 languages. Experiments on various speech and
machine translation benchmarks (e.g., FLEURS, CoVoST-2, WMT) demonstrate that
our GenTranslate significantly outperforms the state-of-the-art model.",http://arxiv.org/abs/2402.06894v1,92,18,18,20,18,18
Image-based Deep Learning for the time-dependent prediction of fresh concrete properties,"Increasing the degree of digitisation and automation in the concrete
production process can play a crucial role in reducing the CO$_2$ emissions
that are associated with the production of concrete. In this paper, a method is
presented that makes it possible to predict the properties of fresh concrete
during the mixing process based on stereoscopic image sequences of the
concretes flow behaviour. A Convolutional Neural Network (CNN) is used for the
prediction, which receives the images supported by information on the mix
design as input. In addition, the network receives temporal information in the
form of the time difference between the time at which the images are taken and
the time at which the reference values of the concretes are carried out. With
this temporal information, the network implicitly learns the time-dependent
behaviour of the concretes properties. The network predicts the slump flow
diameter, the yield stress and the plastic viscosity. The time-dependent
prediction potentially opens up the pathway to determine the temporal
development of the fresh concrete properties already during mixing. This
provides a huge advantage for the concrete industry. As a result,
countermeasures can be taken in a timely manner. It is shown that an approach
based on depth and optical flow images, supported by information of the mix
design, achieves the best results.",http://arxiv.org/abs/2402.06611v1,92,18,20,20,18,16
Topological Neural Networks: Mitigating the Bottlenecks of Graph Neural Networks via Higher-Order Interactions,"The irreducible complexity of natural phenomena has led Graph Neural Networks
to be employed as a standard model to perform representation learning tasks on
graph-structured data. While their capacity to capture local and global
patterns is remarkable, the implications associated with long-range and
higher-order dependencies pose considerable challenges to such models. This
work starts with a theoretical framework to reveal the impact of network's
width, depth, and graph topology on the over-squashing phenomena in
message-passing neural networks. Then, the work drifts towards, higher-order
interactions and multi-relational inductive biases via Topological Neural
Networks. Such models propagate messages through higher-dimensional structures,
providing shortcuts or additional routes for information flow. With this
construction, the underlying computational graph is no longer coupled with the
input graph structure, thus mitigating the aforementioned bottlenecks while
accounting also for higher-order interactions. Inspired by Graph Attention
Networks, two topological attention networks are proposed: Simplicial and Cell
Attention Networks. The rationale behind these architecture is to leverage the
extended notion of neighbourhoods provided by the arrangement of groups of
nodes within a simplicial or cell complex to design anisotropic aggregations
able to measure the importance of the information coming from different regions
of the domain. By doing so, they capture dependencies that conventional Graph
Neural Networks might miss. Finally, a multi-way communication scheme is
introduced with Enhanced Cellular Isomorphism Networks, which augment
topological message passing schemes to enable a direct interactions among
groups of nodes arranged in ring-like structures.",http://arxiv.org/abs/2402.06908v1,92,18,20,18,18,18
RQP-SGD: Differential Private Machine Learning through Noisy SGD and Randomized Quantization,"The rise of IoT devices has prompted the demand for deploying machine
learning at-the-edge with real-time, efficient, and secure data processing. In
this context, implementing machine learning (ML) models with real-valued weight
parameters can prove to be impractical particularly for large models, and there
is a need to train models with quantized discrete weights. At the same time,
these low-dimensional models also need to preserve privacy of the underlying
dataset. In this work, we present RQP-SGD, a new approach for
privacy-preserving quantization to train machine learning models for low-memory
ML-at-the-edge. This approach combines differentially private stochastic
gradient descent (DP-SGD) with randomized quantization, providing a measurable
privacy guarantee in machine learning. In particular, we study the utility
convergence of implementing RQP-SGD on ML tasks with convex objectives and
quantization constraints and demonstrate its efficacy over deterministic
quantization. Through experiments conducted on two datasets, we show the
practical effectiveness of RQP-SGD.",http://arxiv.org/abs/2402.06606v1,92,18,19,19,18,18
IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality 3D Generation,"Most text-to-3D generators build upon off-the-shelf text-to-image models
trained on billions of images. They use variants of Score Distillation Sampling
(SDS), which is slow, somewhat unstable, and prone to artifacts. A mitigation
is to fine-tune the 2D generator to be multi-view aware, which can help
distillation or can be combined with reconstruction networks to output 3D
objects directly. In this paper, we further explore the design space of
text-to-3D models. We significantly improve multi-view generation by
considering video instead of image generators. Combined with a 3D
reconstruction algorithm which, by using Gaussian splatting, can optimize a
robust image-based loss, we directly produce high-quality 3D outputs from the
generated views. Our new method, IM-3D, reduces the number of evaluations of
the 2D generator network 10-100x, resulting in a much more efficient pipeline,
better quality, fewer geometric inconsistencies, and higher yield of usable 3D
assets.",http://arxiv.org/abs/2402.08682v1,92,18,19,19,18,18
Large-Language-Model Empowered Dose Volume Histogram Prediction for Intensity Modulated Radiotherapy,"Treatment planning is currently a patient specific, time-consuming, and
resource demanding task in radiotherapy. Dose-volume histogram (DVH) prediction
plays a critical role in automating this process. The geometric relationship
between DVHs in radiotherapy plans and organs-at-risk (OAR) and planning target
volume (PTV) has been well established. This study explores the potential of
deep learning models for predicting DVHs using images and subsequent human
intervention facilitated by a large-language model (LLM) to enhance the
planning quality. We propose a pipeline to convert unstructured images to a
structured graph consisting of image-patch nodes and dose nodes. A novel Dose
Graph Neural Network (DoseGNN) model is developed for predicting DVHs from the
structured graph. The proposed DoseGNN is enhanced with the LLM to encode
massive knowledge from prescriptions and interactive instructions from
clinicians. In this study, we introduced an online human-AI collaboration
(OHAC) system as a practical implementation of the concept proposed for the
automation of intensity-modulated radiotherapy (IMRT) planning. In comparison
to the widely-employed DL models used in radiotherapy, DoseGNN achieved mean
square errors that were 80$\%$, 76$\%$ and 41.0$\%$ of those predicted by Swin
U-Net Transformer, 3D U-Net CNN and vanilla MLP, respectively. Moreover, the
LLM-empowered DoseGNN model facilitates seamless adjustment to treatment plans
through interaction with clinicians using natural language.",http://arxiv.org/abs/2402.07167v1,92,18,19,18,19,18
FedImpro: Measuring and Improving Client Update in Federated Learning,"Federated Learning (FL) models often experience client drift caused by
heterogeneous data, where the distribution of data differs across clients. To
address this issue, advanced research primarily focuses on manipulating the
existing gradients to achieve more consistent client models. In this paper, we
present an alternative perspective on client drift and aim to mitigate it by
generating improved local models. First, we analyze the generalization
contribution of local training and conclude that this generalization
contribution is bounded by the conditional Wasserstein distance between the
data distribution of different clients. Then, we propose FedImpro, to construct
similar conditional distributions for local training. Specifically, FedImpro
decouples the model into high-level and low-level components, and trains the
high-level portion on reconstructed feature distributions. This approach
enhances the generalization contribution and reduces the dissimilarity of
gradients in FL. Experimental results show that FedImpro can help FL defend
against data heterogeneity and enhance the generalization performance of the
model.",http://arxiv.org/abs/2402.07011v1,92,19,19,19,16,19
Quantum Speedup for Spectral Approximation of Kronecker Products,"Given its widespread application in machine learning and optimization, the
Kronecker product emerges as a pivotal linear algebra operator. However, its
computational demands render it an expensive operation, leading to heightened
costs in spectral approximation of it through traditional computation
algorithms. Existing classical methods for spectral approximation exhibit a
linear dependency on the matrix dimension denoted by $n$, considering matrices
of size $A_1 \in \mathbb{R}^{n \times d}$ and $A_2 \in \mathbb{R}^{n \times
d}$. Our work introduces an innovative approach to efficiently address the
spectral approximation of the Kronecker product $A_1 \otimes A_2$ using quantum
methods. By treating matrices as quantum states, our proposed method
significantly reduces the time complexity of spectral approximation to
$O_{d,\epsilon}(\sqrt{n})$.",http://arxiv.org/abs/2402.07027v1,92,20,20,16,18,18
Accuracy of TextFooler black box adversarial attacks on 01 loss sign activation neural network ensemble,"Recent work has shown the defense of 01 loss sign activation neural networks
against image classification adversarial attacks. A public challenge to attack
the models on CIFAR10 dataset remains undefeated. We ask the following question
in this study: are 01 loss sign activation neural networks hard to deceive with
a popular black box text adversarial attack program called TextFooler? We study
this question on four popular text classification datasets: IMDB reviews, Yelp
reviews, MR sentiment classification, and AG news classification. We find that
our 01 loss sign activation network is much harder to attack with TextFooler
compared to sigmoid activation cross entropy and binary neural networks. We
also study a 01 loss sign activation convolutional neural network with a novel
global pooling step specific to sign activation networks. With this new
variation we see a significant gain in adversarial accuracy rendering
TextFooler practically useless against it. We make our code freely available at
\url{https://github.com/zero-one-loss/wordcnn01} and
\url{https://github.com/xyzacademic/mlp01example}. Our work here suggests that
01 loss sign activation networks could be further developed to create fool
proof models against text adversarial attacks.",http://arxiv.org/abs/2402.07347v1,92,18,20,18,18,18
Antagonistic AI,"The vast majority of discourse around AI development assumes that
subservient, ""moral"" models aligned with ""human values"" are universally
beneficial -- in short, that good AI is sycophantic AI. We explore the shadow
of the sycophantic paradigm, a design space we term antagonistic AI: AI systems
that are disagreeable, rude, interrupting, confrontational, challenging, etc.
-- embedding opposite behaviors or values. Far from being ""bad"" or ""immoral,""
we consider whether antagonistic AI systems may sometimes have benefits to
users, such as forcing users to confront their assumptions, build resilience,
or develop healthier relational boundaries. Drawing from formative explorations
and a speculative design workshop where participants designed fictional AI
technologies that employ antagonism, we lay out a design space for antagonistic
AI, articulating potential benefits, design techniques, and methods of
embedding antagonistic elements into user experience. Finally, we discuss the
many ethical challenges of this space and identify three dimensions for the
responsible design of antagonistic AI -- consent, context, and framing.",http://arxiv.org/abs/2402.07350v1,92,19,20,19,16,18
Unsupervised Discovery of Object-Centric Neural Fields,"We study inferring 3D object-centric scene representations from a single
image. While recent methods have shown potential in unsupervised 3D object
discovery from simple synthetic images, they fail to generalize to real-world
scenes with visually rich and diverse objects. This limitation stems from their
object representations, which entangle objects' intrinsic attributes like shape
and appearance with extrinsic, viewer-centric properties such as their 3D
location. To address this bottleneck, we propose Unsupervised discovery of
Object-Centric neural Fields (uOCF). uOCF focuses on learning the intrinsics of
objects and models the extrinsics separately. Our approach significantly
improves systematic generalization, thus enabling unsupervised learning of
high-fidelity object-centric scene representations from sparse real-world
images. To evaluate our approach, we collect three new datasets, including two
real kitchen environments. Extensive experiments show that uOCF enables
unsupervised discovery of visually rich objects from a single real image,
allowing applications such as 3D object segmentation and scene manipulation.
Notably, uOCF demonstrates zero-shot generalization to unseen objects from a
single real image. Project page: https://red-fairy.github.io/uOCF/",http://arxiv.org/abs/2402.07376v1,92,18,20,18,18,18
Chain-of-Layer: Iteratively Prompting Large Language Models for Taxonomy Induction from Limited Examples,"Automatic taxonomy induction is crucial for web search, recommendation
systems, and question answering. Manual curation of taxonomies is expensive in
terms of human effort, making automatic taxonomy construction highly desirable.
In this work, we introduce Chain-of-Layer which is an in-context learning
framework designed to induct taxonomies from a given set of entities.
Chain-of-Layer breaks down the task into selecting relevant candidate entities
in each layer and gradually building the taxonomy from top to bottom. To
minimize errors, we introduce the Ensemble-based Ranking Filter to reduce the
hallucinated content generated at each iteration. Through extensive
experiments, we demonstrate that Chain-of-Layer achieves state-of-the-art
performance on four real-world benchmarks.",http://arxiv.org/abs/2402.07386v1,92,18,20,20,16,18
TeMPO: Efficient Time-Multiplexed Dynamic Photonic Tensor Core for Edge AI with Compact Slow-Light Electro-Optic Modulator,"Electronic-photonic computing systems offer immense potential in
energy-efficient artificial intelligence (AI) acceleration tasks due to the
superior computing speed and efficiency of optics, especially for real-time,
low-energy deep neural network (DNN) inference tasks on resource-restricted
edge platforms. However, current optical neural accelerators based on
foundry-available devices and conventional system architecture still encounter
a performance gap compared to highly customized electronic counterparts. To
bridge the performance gap due to lack of domain specialization, we present a
time-multiplexed dynamic photonic tensor accelerator, dubbed TeMPO, with
cross-layer device/circuit/architecture customization. At the device level, we
present foundry-compatible, customized photonic devices, including a slow-light
electro-optic modulator with experimental demonstration, optical splitters, and
phase shifters that significantly reduce the footprint and power in input
encoding and dot-product calculation. At the circuit level, partial products
are hierarchically accumulated via parallel photocurrent aggregation,
lightweight capacitive temporal integration, and sequential digital summation,
considerably relieving the analog-to-digital conversion bottleneck. We also
employ a multi-tile, multi-core architecture to maximize hardware sharing for
higher efficiency. Across diverse edge AI workloads, TeMPO delivers
digital-comparable task accuracy with superior quantization/noise tolerance. We
achieve a 368.6 TOPS peak performance, 22.3 TOPS/W energy efficiency, and 1.2
TOPS/mm$^2$ compute density, pushing the Pareto frontier in edge AI hardware.
This work signifies the power of cross-layer co-design and domain-specific
customization, paving the way for future electronic-photonic accelerators with
even greater performance and efficiency.",http://arxiv.org/abs/2402.07393v1,92,20,18,20,18,16
VisLingInstruct: Elevating Zero-Shot Learning in Multi-Modal Language Models with Autonomous Instruction Optimization,"This paper presents VisLingInstruct, a novel approach to advancing
Multi-Modal Language Models (MMLMs) in zero-shot learning. Current MMLMs show
impressive zero-shot abilities in multi-modal tasks, but their performance
depends heavily on the quality of instructions. VisLingInstruct tackles this by
autonomously evaluating and optimizing instructional texts through In-Context
Learning, improving the synergy between visual perception and linguistic
expression in MMLMs. Alongside this instructional advancement, we have also
optimized the visual feature extraction modules in MMLMs, further augmenting
their responsiveness to textual cues. Our comprehensive experiments on MMLMs,
based on FlanT5 and Vicuna, show that VisLingInstruct significantly improves
zero-shot performance in visual multi-modal tasks. Notably, it achieves a 13.1%
and 9% increase in accuracy over the prior state-of-the-art on the TextVQA and
HatefulMemes datasets.",http://arxiv.org/abs/2402.07398v1,92,20,19,19,16,18
Dlares or Dollars? Unraveling the Bilingual Prowess of Financial LLMs Between Spanish and English,"Despite Spanish's pivotal role in the global finance industry, a pronounced
gap exists in Spanish financial natural language processing (NLP) and
application studies compared to English, especially in the era of large
language models (LLMs). To bridge this gap, we unveil Tois\'on de Oro, the
first bilingual framework that establishes instruction datasets, finetuned
LLMs, and evaluation benchmark for financial LLMs in Spanish joint with
English. We construct a rigorously curated bilingual instruction dataset
including over 144K Spanish and English samples from 15 datasets covering 7
tasks. Harnessing this, we introduce FinMA-ES, an LLM designed for bilingual
financial applications. We evaluate our model and existing LLMs using FLARE-ES,
the first comprehensive bilingual evaluation benchmark with 21 datasets
covering 9 tasks. The FLARE-ES benchmark results reveal a significant
multilingual performance gap and bias in existing LLMs. FinMA-ES models surpass
SOTA LLMs such as GPT-4 in Spanish financial tasks, due to strategic
instruction tuning and leveraging data from diverse linguistic resources,
highlighting the positive impact of cross-linguistic transfer. All our
datasets, models, and benchmarks have been released.",http://arxiv.org/abs/2402.07405v1,92,19,20,19,18,16
A Closer Look at the Robustness of Contrastive Language-Image Pre-Training (CLIP),"Contrastive Language-Image Pre-training (CLIP) models have demonstrated
remarkable generalization capabilities across multiple challenging distribution
shifts. However, there is still much to be explored in terms of their
robustness to the variations of specific visual factors. In real-world
applications, reliable and safe systems must consider other safety objectives
beyond classification accuracy, such as predictive uncertainty. Yet, the
effectiveness of CLIP models on such safety-related features is less-explored.
Driven by the above, this work comprehensively investigates the safety
objectives of CLIP models, specifically focusing on three key properties:
resilience to visual factor variations, calibrated uncertainty estimations, and
the ability to detect anomalous inputs. To this end, we study 83 CLIP models
and 127 ImageNet classifiers. They are diverse in architecture, (pre)training
distribution and training strategies. We consider 10 visual factors (e.g.,
shape and pattern), 5 types of out-of-distribution data, and 8 natural and
challenging test conditions with different shift types, such as texture, style,
and perturbation shifts. Our study has unveiled several previously unknown
insights into CLIP models. For instance, they are not consistently more
calibrated than other ImageNet models, which contradicts existing findings.
Additionally, our analysis underscores the significance of training source
design by showcasing its profound influence on the three safety-related
properties. We believe our comprehensive study can shed light on and help guide
the development of more robust and reliable CLIP models.",http://arxiv.org/abs/2402.07410v1,92,17,19,20,18,18
Auxiliary Reward Generation with Transition Distance Representation Learning,"Reinforcement learning (RL) has shown its strength in challenging sequential
decision-making problems. The reward function in RL is crucial to the learning
performance, as it serves as a measure of the task completion degree. In
real-world problems, the rewards are predominantly human-designed, which
requires laborious tuning, and is easily affected by human cognitive biases. To
achieve automatic auxiliary reward generation, we propose a novel
representation learning approach that can measure the ``transition distance''
between states. Building upon these representations, we introduce an auxiliary
reward generation technique for both single-task and skill-chaining scenarios
without the need for human knowledge. The proposed approach is evaluated in a
wide range of manipulation tasks. The experiment results demonstrate the
effectiveness of measuring the transition distance between states and the
induced improvement by auxiliary rewards, which not only promotes better
learning efficiency but also increases convergent stability.",http://arxiv.org/abs/2402.07412v1,92,19,19,19,17,18
Context-aware Multi-Model Object Detection for Diversely Heterogeneous Compute Systems,"In recent years, deep neural networks (DNNs) have gained widespread adoption
for continuous mobile object detection (OD) tasks, particularly in autonomous
systems. However, a prevalent issue in their deployment is the
one-size-fits-all approach, where a single DNN is used, resulting in
inefficient utilization of computational resources. This inefficiency is
particularly detrimental in energy-constrained systems, as it degrades overall
system efficiency. We identify that, the contextual information embedded in the
input data stream (e.g. the frames in the camera feed that the OD models are
run on) could be exploited to allow a more efficient multi-model-based OD
process. In this paper, we propose SHIFT which continuously selects from a
variety of DNN-based OD models depending on the dynamically changing contextual
information and computational constraints. During this selection, SHIFT
uniquely considers multi-accelerator execution to better optimize the
energy-efficiency while satisfying the latency constraints. Our proposed
methodology results in improvements of up to 7.5x in energy usage and 2.8x in
latency compared to state-of-the-art GPU-based single model OD approaches.",http://arxiv.org/abs/2402.07415v1,92,18,20,18,18,18
SemTra: A Semantic Skill Translator for Cross-Domain Zero-Shot Policy Adaptation,"This work explores the zero-shot adaptation capability of semantic skills,
semantically interpretable experts' behavior patterns, in cross-domain
settings, where a user input in interleaved multi-modal snippets can prompt a
new long-horizon task for different domains. In these cross-domain settings, we
present a semantic skill translator framework SemTra which utilizes a set of
multi-modal models to extract skills from the snippets, and leverages the
reasoning capabilities of a pretrained language model to adapt these extracted
skills to the target domain. The framework employs a two-level hierarchy for
adaptation: task adaptation and skill adaptation. During task adaptation,
seq-to-seq translation by the language model transforms the extracted skills
into a semantic skill sequence, which is tailored to fit the cross-domain
contexts. Skill adaptation focuses on optimizing each semantic skill for the
target domain context, through parametric instantiations that are facilitated
by language prompting and contrastive learning-based context inferences. This
hierarchical adaptation empowers the framework to not only infer a complex task
specification in one-shot from the interleaved multi-modal snippets, but also
adapt it to new domains with zero-shot learning abilities. We evaluate our
framework with Meta-World, Franka Kitchen, RLBench, and CARLA environments. The
results clarify the framework's superiority in performing long-horizon tasks
and adapting to different domains, showing its broad applicability in practical
use cases, such as cognitive robots interpreting abstract instructions and
autonomous vehicles operating under varied configurations.",http://arxiv.org/abs/2402.07418v1,92,19,20,19,17,17
Debiasing Recommendation with Personal Popularity,"Global popularity (GP) bias is the phenomenon that popular items are
recommended much more frequently than they should be, which goes against the
goal of providing personalized recommendations and harms user experience and
recommendation accuracy. Many methods have been proposed to reduce GP bias but
they fail to notice the fundamental problem of GP, i.e., it considers
popularity from a \textit{global} perspective of \textit{all users} and uses a
single set of popular items, and thus cannot capture the interests of
individual users. As such, we propose a user-aware version of item popularity
named \textit{personal popularity} (PP), which identifies different popular
items for each user by considering the users that share similar interests. As
PP models the preferences of individual users, it naturally helps to produce
personalized recommendations and mitigate GP bias. To integrate PP into
recommendation, we design a general \textit{personal popularity aware
counterfactual} (PPAC) framework, which adapts easily to existing
recommendation models. In particular, PPAC recognizes that PP and GP have both
direct and indirect effects on recommendations and controls direct effects with
counterfactual inference techniques for unbiased recommendations. All codes and
datasets are available at \url{https://github.com/Stevenn9981/PPAC}.",http://dx.doi.org/10.1145/3589334.3645421,92,20,18,18,18,18
"The I/O Complexity of Attention, or How Optimal is Flash Attention?","Self-attention is at the heart of the popular Transformer architecture, yet
suffers from quadratic time and memory complexity. The breakthrough
FlashAttention algorithm revealed I/O complexity as the true bottleneck in
scaling Transformers. Given two levels of memory hierarchy, a fast cache (e.g.
GPU on-chip SRAM) and a slow memory (e.g. GPU high-bandwidth memory), the I/O
complexity measures the number of accesses to memory. FlashAttention computes
attention using $\frac{N^2d^2}{M}$ I/O operations where $N$ is the dimension of
the attention matrix, $d$ the head-dimension and $M$ the cache size. However,
is this I/O complexity optimal? The known lower bound only rules out an I/O
complexity of $o(Nd)$ when $M=\Theta(Nd)$, since the output that needs to be
written to slow memory is $\Omega(Nd)$. This leads to the main question of our
work: Is FlashAttention I/O optimal for all values of $M$?
  We resolve the above question in its full generality by showing an I/O
complexity lower bound that matches the upper bound provided by FlashAttention
for any values of $M \geq d^2$ within any constant factors. Further, we give a
better algorithm with lower I/O complexity for $M < d^2$, and show that it is
optimal as well. Moreover, our lower bounds do not rely on using combinatorial
matrix multiplication for computing the attention matrix. We show even if one
uses fast matrix multiplication, the above I/O complexity bounds cannot be
improved. We do so by introducing a new communication complexity protocol for
matrix compression, and connecting communication complexity to I/O complexity.
To the best of our knowledge, this is the first work to establish a connection
between communication complexity and I/O complexity, and we believe this
connection could be of independent interest and will find many more
applications in proving I/O complexity lower bounds in the future.",http://arxiv.org/abs/2402.07443v1,92,20,19,18,17,18
A Hormetic Approach to the Value-Loading Problem: Preventing the Paperclip Apocalypse?,"The value-loading problem is a significant challenge for researchers aiming
to create artificial intelligence (AI) systems that align with human values and
preferences. This problem requires a method to define and regulate safe and
optimal limits of AI behaviors. In this work, we propose HALO (Hormetic
ALignment via Opponent processes), a regulatory paradigm that uses hormetic
analysis to regulate the behavioral patterns of AI. Behavioral hormesis is a
phenomenon where low frequencies of a behavior have beneficial effects, while
high frequencies are harmful. By modeling behaviors as allostatic opponent
processes, we can use either Behavioral Frequency Response Analysis (BFRA) or
Behavioral Count Response Analysis (BCRA) to quantify the hormetic limits of
repeatable behaviors. We demonstrate how HALO can solve the 'paperclip
maximizer' scenario, a thought experiment where an unregulated AI tasked with
making paperclips could end up converting all matter in the universe into
paperclips. Our approach may be used to help create an evolving database of
'values' based on the hedonic calculus of repeatable behaviors with decreasing
marginal utility. This positions HALO as a promising solution for the
value-loading problem, which involves embedding human-aligned values into an AI
system, and the weak-to-strong generalization problem, which explores whether
weak models can supervise stronger models as they become more intelligent.
Hence, HALO opens several research avenues that may lead to the development of
a computational value system that allows an AI algorithm to learn whether the
decisions it makes are right or wrong.",http://arxiv.org/abs/2402.07462v2,92,20,18,20,18,16
Pushing The Limit of LLM Capacity for Text Classification,"The value of text classification's future research has encountered challenges
and uncertainties, due to the extraordinary efficacy demonstrated by large
language models (LLMs) across numerous downstream NLP tasks. In this era of
open-ended language modeling, where task boundaries are gradually fading, an
urgent question emerges: have we made significant advances in text
classification under the full benefit of LLMs? To answer this question, we
propose RGPT, an adaptive boosting framework tailored to produce a specialized
text classification LLM by recurrently ensembling a pool of strong base
learners. The base learners are constructed by adaptively adjusting the
distribution of training samples and iteratively fine-tuning LLMs with them.
Such base learners are then ensembled to be a specialized text classification
LLM, by recurrently incorporating the historical predictions from the previous
learners. Through a comprehensive empirical comparison, we show that RGPT
significantly outperforms 8 SOTA PLMs and 7 SOTA LLMs on four benchmarks by
1.36% on average. Further evaluation experiments show a clear surpassing of
RGPT over human classification.",http://arxiv.org/abs/2402.07470v1,92,18,19,19,18,18
Differentially Private Decentralized Learning with Random Walks,"The popularity of federated learning comes from the possibility of better
scalability and the ability for participants to keep control of their data,
improving data security and sovereignty. Unfortunately, sharing model updates
also creates a new privacy attack surface. In this work, we characterize the
privacy guarantees of decentralized learning with random walk algorithms, where
a model is updated by traveling from one node to another along the edges of a
communication graph. Using a recent variant of differential privacy tailored to
the study of decentralized algorithms, namely Pairwise Network Differential
Privacy, we derive closed-form expressions for the privacy loss between each
pair of nodes where the impact of the communication topology is captured by
graph theoretic quantities. Our results further reveal that random walk
algorithms tends to yield better privacy guarantees than gossip algorithms for
nodes close from each other. We supplement our theoretical results with
empirical evaluation on synthetic and real-world graphs and datasets.",http://arxiv.org/abs/2402.07471v1,92,19,18,19,18,18
One Train for Two Tasks: An Encrypted Traffic Classification Framework Using Supervised Contrastive Learning,"As network security receives widespread attention, encrypted traffic
classification has become the current research focus. However, existing methods
conduct traffic classification without sufficiently considering the common
characteristics between data samples, leading to suboptimal performance.
Moreover, they train the packet-level and flow-level classification tasks
independently, which is redundant because the packet representations learned in
the packet-level task can be exploited by the flow-level task. Therefore, in
this paper, we propose an effective model named a Contrastive Learning Enhanced
Temporal Fusion Encoder (CLE-TFE). In particular, we utilize supervised
contrastive learning to enhance the packet-level and flow-level representations
and perform graph data augmentation on the byte-level traffic graph so that the
fine-grained semantic-invariant characteristics between bytes can be captured
through contrastive learning. We also propose cross-level multi-task learning,
which simultaneously accomplishes the packet-level and flow-level
classification tasks in the same model with one training. Further experiments
show that CLE-TFE achieves the best overall performance on the two tasks, while
its computational overhead (i.e., floating point operations, FLOPs) is only
about 1/14 of the pre-trained model (e.g., ET-BERT). We release the code at
https://github.com/ViktorAxelsen/CLE-TFE",http://arxiv.org/abs/2402.07501v1,92,20,19,19,17,17
Physics-informed machine learning as a kernel method,"Physics-informed machine learning combines the expressiveness of data-based
approaches with the interpretability of physical models. In this context, we
consider a general regression problem where the empirical risk is regularized
by a partial differential equation that quantifies the physical inconsistency.
We prove that for linear differential priors, the problem can be formulated as
a kernel regression task. Taking advantage of kernel theory, we derive
convergence rates for the minimizer of the regularized risk and show that it
converges at least at the Sobolev minimax rate. However, faster rates can be
achieved, depending on the physical error. This principle is illustrated with a
one-dimensional example, supporting the claim that regularizing the empirical
risk with physical information can be beneficial to the statistical performance
of estimators.",http://arxiv.org/abs/2402.07514v1,92,18,20,18,18,18
Resilient Watermarking for LLM-Generated Codes,"With the development of large language models, multiple AIs are now made
available for code generation (such as ChatGPT and StarCoder) and are adopted
widely. It is often desirable to know whether a piece of code is generated by
AI, and furthermore, which AI is the author. For instance, if a certain version
of AI is known to generate vulnerable code, it is particularly important to
know the creator. Existing approaches are not satisfactory as watermarking
codes are challenging compared with watermarking text data, as codes can be
altered with relative ease via widely-used code refactoring methods. In this
work, we propose ACW (AI Code Watermarking), a novel method for watermarking
AI-generated codes. ACW is efficient as it requires no training or fine-tuning
and works in a black-box manner. It is resilient as the watermark cannot be
easily removed or tampered through common code refactoring methods. The key
idea of ACW is to selectively apply a set of carefully-designed
semantic-preserving, idempotent code transformations, whose presence (or
absence) allows us to determine the existence of the watermark. Our
experimental results show that ACW is effective (i.e., achieving high accuracy,
true positive rates and false positive rates), resilient and efficient,
significantly outperforming existing approaches.",http://arxiv.org/abs/2402.07518v1,92,19,20,19,18,16
Accelerating Distributed Deep Learning using Lossless Homomorphic Compression,"As deep neural networks (DNNs) grow in complexity and size, the resultant
increase in communication overhead during distributed training has become a
significant bottleneck, challenging the scalability of distributed training
systems. Existing solutions, while aiming to mitigate this bottleneck through
worker-level compression and in-network aggregation, fall short due to their
inability to efficiently reconcile the trade-offs between compression
effectiveness and computational overhead, hindering overall performance and
scalability. In this paper, we introduce a novel compression algorithm that
effectively merges worker-level compression with in-network aggregation. Our
solution is both homomorphic, allowing for efficient in-network aggregation
without CPU/GPU processing, and lossless, ensuring no compromise on training
accuracy. Theoretically optimal in compression and computational efficiency,
our approach is empirically validated across diverse DNN models such as NCF,
LSTM, VGG19, and BERT-base, showing up to a 6.33$\times$ improvement in
aggregation throughput and a 3.74$\times$ increase in per-iteration training
speed.",http://arxiv.org/abs/2402.07529v1,92,19,19,20,17,17
TransAxx: Efficient Transformers with Approximate Computing,"Vision Transformer (ViT) models which were recently introduced by the
transformer architecture have shown to be very competitive and often become a
popular alternative to Convolutional Neural Networks (CNNs). However, the high
computational requirements of these models limit their practical applicability
especially on low-power devices. Current state-of-the-art employs approximate
multipliers to address the highly increased compute demands of DNN accelerators
but no prior research has explored their use on ViT models. In this work we
propose TransAxx, a framework based on the popular PyTorch library that enables
fast inherent support for approximate arithmetic to seamlessly evaluate the
impact of approximate computing on DNNs such as ViT models. Using TransAxx we
analyze the sensitivity of transformer models on the ImageNet dataset to
approximate multiplications and perform approximate-aware finetuning to regain
accuracy. Furthermore, we propose a methodology to generate approximate
accelerators for ViT models. Our approach uses a Monte Carlo Tree Search (MCTS)
algorithm to efficiently search the space of possible configurations using a
hardware-driven hand-crafted policy. Our evaluation demonstrates the efficacy
of our methodology in achieving significant trade-offs between accuracy and
power, resulting in substantial gains without compromising on performance.",http://arxiv.org/abs/2402.07545v1,92,18,20,20,16,18
A Precision-Optimized Fixed-Point Near-Memory Digital Processing Unit for Analog In-Memory Computing,"Analog In-Memory Computing (AIMC) is an emerging technology for fast and
energy-efficient Deep Learning (DL) inference. However, a certain amount of
digital post-processing is required to deal with circuit mismatches and
non-idealities associated with the memory devices. Efficient near-memory
digital logic is critical to retain the high area/energy efficiency and low
latency of AIMC. Existing systems adopt Floating Point 16 (FP16) arithmetic
with limited parallelization capability and high latency. To overcome these
limitations, we propose a Near-Memory digital Processing Unit (NMPU) based on
fixed-point arithmetic. It achieves competitive accuracy and higher computing
throughput than previous approaches while minimizing the area overhead.
Moreover, the NMPU supports standard DL activation steps, such as ReLU and
Batch Normalization. We perform a physical implementation of the NMPU design in
a 14 nm CMOS technology and provide detailed performance, power, and area
assessments. We validate the efficacy of the NMPU by using data from an AIMC
chip and demonstrate that a simulated AIMC system with the proposed NMPU
outperforms existing FP16-based implementations, providing 139$\times$
speed-up, 7.8$\times$ smaller area, and a competitive power consumption.
Additionally, our approach achieves an inference accuracy of 86.65 %/65.06 %,
with an accuracy drop of just 0.12 %/0.4 % compared to the FP16 baseline when
benchmarked with ResNet9/ResNet32 networks trained on the CIFAR10/CIFAR100
datasets, respectively.",http://arxiv.org/abs/2402.07549v1,92,20,18,20,18,16
SMX: Sequential Monte Carlo Planning for Expert Iteration,"Developing agents that can leverage planning abilities during their decision
and learning processes is critical to the advancement of Artificial
Intelligence. Recent works have demonstrated the effectiveness of combining
tree-based search methods and self-play learning mechanisms. Yet, these methods
typically face scaling challenges due to the sequential nature of their search.
While practical engineering solutions can partly overcome this, they still
demand extensive computational resources, which hinders their applicability. In
this paper, we introduce SMX, a model-based planning algorithm that utilises
scalable Sequential Monte Carlo methods to create an effective self-learning
mechanism. Grounded in the theoretical framework of control as inference, SMX
benefits from robust theoretical underpinnings. Its sampling-based search
approach makes it adaptable to environments with both discrete and continuous
action spaces. Furthermore, SMX allows for high parallelisation and can run on
hardware accelerators to optimise computing efficiency. SMX demonstrates a
statistically significant improvement in performance compared to AlphaZero, as
well as demonstrating its performance as an improvement operator for a
model-free policy, matching or exceeding top model-free methods across both
continuous and discrete environments.",http://arxiv.org/abs/2402.07963v1,92,19,19,19,17,18
Weisfeiler-Leman at the margin: When more expressivity matters,"The Weisfeiler-Leman algorithm ($1$-WL) is a well-studied heuristic for the
graph isomorphism problem. Recently, the algorithm has played a prominent role
in understanding the expressive power of message-passing graph neural networks
(MPNNs) and being effective as a graph kernel. Despite its success, $1$-WL
faces challenges in distinguishing non-isomorphic graphs, leading to the
development of more expressive MPNN and kernel architectures. However, the
relationship between enhanced expressivity and improved generalization
performance remains unclear. Here, we show that an architecture's expressivity
offers limited insights into its generalization performance when viewed through
graph isomorphism. Moreover, we focus on augmenting $1$-WL and MPNNs with
subgraph information and employ classical margin theory to investigate the
conditions under which an architecture's increased expressivity aligns with
improved generalization performance. In addition, we show that gradient flow
pushes the MPNN's weights toward the maximum margin solution. Further, we
introduce variations of expressive $1$-WL-based kernel and MPNN architectures
with provable generalization properties. Our empirical study confirms the
validity of our theoretical findings.",http://arxiv.org/abs/2402.07568v1,92,18,20,19,17,18
Rethinking Scaling Laws for Learning in Strategic Environments,"The deployment of ever-larger machine learning models reflects a growing
consensus that the more expressive the model$\unicode{x2013}$and the more data
one has access to$\unicode{x2013}$the more one can improve performance. As
models get deployed in a variety of real world scenarios, they inevitably face
strategic environments. In this work, we consider the natural question of how
the interplay of models and strategic interactions affects scaling laws. We
find that strategic interactions can break the conventional view of scaling
laws$\unicode{x2013}$meaning that performance does not necessarily
monotonically improve as models get larger and/ or more expressive (even with
infinite data). We show the implications of this phenomenon in several contexts
including strategic regression, strategic classification, and multi-agent
reinforcement learning through examples of strategic environments in
which$\unicode{x2013}$by simply restricting the expressivity of one's model or
policy class$\unicode{x2013}$one can achieve strictly better equilibrium
outcomes. Motivated by these examples, we then propose a new paradigm for
model-selection in games wherein an agent seeks to choose amongst different
model classes to use as their action set in a game.",http://arxiv.org/abs/2402.07588v1,92,20,19,19,17,17
"Imagining a Future of Designing with AI: Dynamic Grounding, Constructive Negotiation, and Sustainable Motivation","We ideate a future design workflow that involves AI technology. Drawing from
activity and communication theory, we attempt to isolate the new value large AI
models can provide design compared to past technologies. We arrive at three
affordances -- dynamic grounding, constructive negotiation, and sustainable
motivation -- that summarize latent qualities of natural language-enabled
foundation models that, if explicitly designed for, can support the process of
design. Through design fiction, we then imagine a future interface as a
diegetic prototype, the story of Squirrel Game, that demonstrates each of our
three affordances in a realistic usage scenario. Our design process,
terminology, and diagrams aim to contribute to future discussions about the
relative affordances of AI technology with regard to collaborating with human
designers.",http://arxiv.org/abs/2402.07342v1,92,18,20,20,16,18
BioNeRF: Biologically Plausible Neural Radiance Fields for View Synthesis,"This paper presents BioNeRF, a biologically plausible architecture that
models scenes in a 3D representation and synthesizes new views through radiance
fields. Since NeRF relies on the network weights to store the scene's
3-dimensional representation, BioNeRF implements a cognitive-inspired mechanism
that fuses inputs from multiple sources into a memory-like structure, improving
the storing capacity and extracting more intrinsic and correlated information.
BioNeRF also mimics a behavior observed in pyramidal cells concerning
contextual information, in which the memory is provided as the context and
combined with the inputs of two subsequent neural models, one responsible for
producing the volumetric densities and the other the colors used to render the
scene. Experimental results show that BioNeRF outperforms state-of-the-art
results concerning a quality measure that encodes human perception in two
datasets: real-world images and synthetic data.",http://arxiv.org/abs/2402.07310v1,92,20,20,18,18,16
HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs,"Hypergraphs are marked by complex topology, expressing higher-order
interactions among multiple entities with hyperedges. Lately, hypergraph-based
deep learning methods to learn informative data representations for the problem
of node classification on text-attributed hypergraphs have garnered increasing
research attention. However, existing methods struggle to simultaneously
capture the full extent of hypergraph structural information and the rich
linguistic attributes inherent in the nodes attributes, which largely hampers
their effectiveness and generalizability. To overcome these challenges, we
explore ways to further augment a pretrained BERT model with specialized
hypergraph-aware layers for the task of node classification. Such layers
introduce higher-order structural inductive bias into the language model, thus
improving the model's capacity to harness both higher-order context information
from the hypergraph structure and semantic information present in text. In this
paper, we propose a new architecture, HyperBERT, a mixed text-hypergraph model
which simultaneously models hypergraph relational structure while maintaining
the high-quality text encoding capabilities of a pre-trained BERT. Notably,
HyperBERT presents results that achieve a new state-of-the-art on five
challenging text-attributed hypergraph node classification benchmarks.",http://arxiv.org/abs/2402.07309v2,92,18,20,18,18,18
A novel spatial-frequency domain network for zero-shot incremental learning,"Zero-shot incremental learning aims to enable the model to generalize to new
classes without forgetting previously learned classes. However, the semantic
gap between old and new sample classes can lead to catastrophic forgetting.
Additionally, existing algorithms lack capturing significant information from
each sample image domain, impairing models' classification performance.
Therefore, this paper proposes a novel Spatial-Frequency Domain Network
(SFDNet) which contains a Spatial-Frequency Feature Extraction (SFFE) module
and Attention Feature Alignment (AFA) module to improve the Zero-Shot
Translation for Class Incremental algorithm. Firstly, SFFE module is designed
which contains a dual attention mechanism for obtaining salient
spatial-frequency feature information. Secondly, a novel feature fusion module
is conducted for obtaining fused spatial-frequency domain features. Thirdly,
the Nearest Class Mean classifier is utilized to select the most suitable
category. Finally, iteration between tasks is performed using the Zero-Shot
Translation model. The proposed SFDNet has the ability to effectively extract
spatial-frequency feature representation from input images, improve the
accuracy of image classification, and fundamentally alleviate catastrophic
forgetting. Extensive experiments on the CUB 200-2011 and CIFAR100 datasets
demonstrate that our proposed algorithm outperforms state-of-the-art
incremental learning algorithms.",http://arxiv.org/abs/2402.07216v1,92,18,20,20,18,16
Distilling Symbolic Priors for Concept Learning into Neural Networks,"Humans can learn new concepts from a small number of examples by drawing on
their inductive biases. These inductive biases have previously been captured by
using Bayesian models defined over symbolic hypothesis spaces. Is it possible
to create a neural network that displays the same inductive biases? We show
that inductive biases that enable rapid concept learning can be instantiated in
artificial neural networks by distilling a prior distribution from a symbolic
Bayesian model via meta-learning, an approach for extracting the common
structure from a set of tasks. By generating the set of tasks used in
meta-learning from the prior distribution of a Bayesian model, we are able to
transfer that prior into a neural network. We use this approach to create a
neural network with an inductive bias towards concepts expressed as short
logical formulas. Analyzing results from previous behavioral experiments in
which people learned logical concepts from a few examples, we find that our
meta-trained models are highly aligned with human performance.",http://arxiv.org/abs/2402.07035v1,92,19,19,19,18,17
A Tale of Tails: Model Collapse as a Change of Scaling Laws,"As AI model size grows, neural scaling laws have become a crucial tool to
predict the improvements of large models when increasing capacity and the size
of original (human or natural) training data. Yet, the widespread use of
popular models means that the ecosystem of online data and text will co-evolve
to progressively contain increased amounts of synthesized data. In this paper
we ask: How will the scaling laws change in the inevitable regime where
synthetic data makes its way into the training corpus? Will future models,
still improve, or be doomed to degenerate up to total (model) collapse? We
develop a theoretical framework of model collapse through the lens of scaling
laws. We discover a wide range of decay phenomena, analyzing loss of scaling,
shifted scaling with number of generations, the ''un-learning"" of skills, and
grokking when mixing human and synthesized data. Our theory is validated by
large-scale experiments with a transformer on an arithmetic task and text
generation using the large language model Llama2.",http://arxiv.org/abs/2402.07043v1,92,19,20,19,18,16
Self-Correcting Self-Consuming Loops for Generative Model Training,"As synthetic data becomes higher quality and proliferates on the internet,
machine learning models are increasingly trained on a mix of human- and
machine-generated data. Despite the successful stories of using synthetic data
for representation learning, using synthetic data for generative model training
creates ""self-consuming loops"" which may lead to training instability or even
collapse, unless certain conditions are met. Our paper aims to stabilize
self-consuming generative model training. Our theoretical results demonstrate
that by introducing an idealized correction function, which maps a data point
to be more likely under the true data distribution, self-consuming loops can be
made exponentially more stable. We then propose self-correction functions,
which rely on expert knowledge (e.g. the laws of physics programmed in a
simulator), and aim to approximate the idealized corrector automatically and at
scale. We empirically validate the effectiveness of self-correcting
self-consuming loops on the challenging human motion synthesis task, and
observe that it successfully avoids model collapse, even when the ratio of
synthetic data to real data is as high as 100%.",http://arxiv.org/abs/2402.07087v1,92,20,18,20,18,16
Decoupling Learning and Decision-Making: Breaking the $\mathcal{O}(\sqrt{T})$ Barrier in Online Resource Allocation with First-Order Methods,"Online linear programming plays an important role in both revenue management
and resource allocation, and recent research has focused on developing
efficient first-order online learning algorithms. Despite the empirical success
of first-order methods, they typically achieve a regret no better than
$\mathcal{O}(\sqrt{T})$, which is suboptimal compared to the $\mathcal{O}(\log
T)$ bound guaranteed by the state-of-the-art linear programming (LP)-based
online algorithms. This paper establishes several important facts about online
linear programming, which unveils the challenge for first-order-method-based
online algorithms to achieve beyond $\mathcal{O}(\sqrt{T})$ regret. To address
the challenge, we introduce a new algorithmic framework that decouples learning
from decision-making. More importantly, for the first time, we show that
first-order methods can attain regret $\mathcal{O}(T^{1/3})$ with this new
framework. Lastly, we conduct numerical experiments to validate our theoretical
findings.",http://arxiv.org/abs/2402.07108v1,92,20,18,18,18,18
Explainable Global Wildfire Prediction Models using Graph Neural Networks,"Wildfire prediction has become increasingly crucial due to the escalating
impacts of climate change. Traditional CNN-based wildfire prediction models
struggle with handling missing oceanic data and addressing the long-range
dependencies across distant regions in meteorological data. In this paper, we
introduce an innovative Graph Neural Network (GNN)-based model for global
wildfire prediction. We propose a hybrid model that combines the spatial
prowess of Graph Convolutional Networks (GCNs) with the temporal depth of Long
Short-Term Memory (LSTM) networks. Our approach uniquely transforms global
climate and wildfire data into a graph representation, addressing challenges
such as null oceanic data locations and long-range dependencies inherent in
traditional models. Benchmarking against established architectures using an
unseen ensemble of JULES-INFERNO simulations, our model demonstrates superior
predictive accuracy. Furthermore, we emphasise the model's explainability,
unveiling potential wildfire correlation clusters through community detection
and elucidating feature importance via Integrated Gradient analysis. Our
findings not only advance the methodological domain of wildfire prediction but
also underscore the importance of model transparency, offering valuable
insights for stakeholders in wildfire management.",http://arxiv.org/abs/2402.07152v1,92,19,19,19,18,17
Natural Language Reinforcement Learning,"Reinforcement Learning (RL) has shown remarkable abilities in learning
policies for decision-making tasks. However, RL is often hindered by issues
such as low sample efficiency, lack of interpretability, and sparse supervision
signals. To tackle these limitations, we take inspiration from the human
learning process and introduce Natural Language Reinforcement Learning (NLRL),
which innovatively combines RL principles with natural language representation.
Specifically, NLRL redefines RL concepts like task objectives, policy, value
function, Bellman equation, and policy iteration in natural language space. We
present how NLRL can be practically implemented with the latest advancements in
large language models (LLMs) like GPT-4. Initial experiments over tabular MDPs
demonstrate the effectiveness, efficiency, and also interpretability of the
NLRL framework.",http://arxiv.org/abs/2402.07157v1,92,20,18,20,18,16
The Deep Equilibrium Algorithmic Reasoner,"Recent work on neural algorithmic reasoning has demonstrated that graph
neural networks (GNNs) could learn to execute classical algorithms. Doing so,
however, has always used a recurrent architecture, where each iteration of the
GNN aligns with an algorithm's iteration. Since an algorithm's solution is
often an equilibrium, we conjecture and empirically validate that one can train
a network to solve algorithmic problems by directly finding the equilibrium.
Note that this does not require matching each GNN iteration with a step of the
algorithm.",http://arxiv.org/abs/2402.06445v1,92,20,18,20,18,16
MAGNETO: Edge AI for Human Activity Recognition -- Privacy and Personalization,"Human activity recognition (HAR) is a well-established field, significantly
advanced by modern machine learning (ML) techniques. While companies have
successfully integrated HAR into consumer products, they typically rely on a
predefined activity set, which limits personalizations at the user level (edge
devices). Despite advancements in Incremental Learning for updating models with
new data, this often occurs on the Cloud, necessitating regular data transfers
between cloud and edge devices, thus leading to data privacy issues. In this
paper, we propose MAGNETO, an Edge AI platform that pushes HAR tasks from the
Cloud to the Edge. MAGNETO allows incremental human activity learning directly
on the Edge devices, without any data exchange with the Cloud. This enables
strong privacy guarantees, low processing latency, and a high degree of
personalization for users. In particular, we demonstrate MAGNETO in an Android
device, validating the whole pipeline from data collection to result
visualization.",http://arxiv.org/abs/2402.07180v1,92,19,18,19,18,18
GSINA: Improving Subgraph Extraction for Graph Invariant Learning via Graph Sinkhorn Attention,"Graph invariant learning (GIL) has been an effective approach to discovering
the invariant relationships between graph data and its labels for different
graph learning tasks under various distribution shifts. Many recent endeavors
of GIL focus on extracting the invariant subgraph from the input graph for
prediction as a regularization strategy to improve the generalization
performance of graph learning. Despite their success, such methods also have
various limitations in obtaining their invariant subgraphs. In this paper, we
provide in-depth analyses of the drawbacks of existing works and propose
corresponding principles of our invariant subgraph extraction: 1) the sparsity,
to filter out the variant features, 2) the softness, for a broader solution
space, and 3) the differentiability, for a soundly end-to-end optimization. To
meet these principles in one shot, we leverage the Optimal Transport (OT)
theory and propose a novel graph attention mechanism called Graph Sinkhorn
Attention (GSINA). This novel approach serves as a powerful regularization
method for GIL tasks. By GSINA, we are able to obtain meaningful,
differentiable invariant subgraphs with controllable sparsity and softness.
Moreover, GSINA is a general graph learning framework that could handle GIL
tasks of multiple data grain levels. Extensive experiments on both synthetic
and real-world datasets validate the superiority of our GSINA, which
outperforms the state-of-the-art GIL methods by large margins on both
graph-level tasks and node-level tasks. Our code is publicly available at
\url{https://github.com/dingfangyu/GSINA}.",http://arxiv.org/abs/2402.07191v1,92,19,19,19,18,17
More Benefits of Being Distributional: Second-Order Bounds for Reinforcement Learning,"In this paper, we prove that Distributional Reinforcement Learning (DistRL),
which learns the return distribution, can obtain second-order bounds in both
online and offline RL in general settings with function approximation.
Second-order bounds are instance-dependent bounds that scale with the variance
of return, which we prove are tighter than the previously known small-loss
bounds of distributional RL. To the best of our knowledge, our results are the
first second-order bounds for low-rank MDPs and for offline RL. When
specializing to contextual bandits (one-step RL problem), we show that a
distributional learning based optimism algorithm achieves a second-order
worst-case regret bound, and a second-order gap dependent bound,
simultaneously. We also empirically demonstrate the benefit of DistRL in
contextual bandits on real-world datasets. We highlight that our analysis with
DistRL is relatively simple, follows the general framework of optimism in the
face of uncertainty and does not require weighted regression. Our results
suggest that DistRL is a promising framework for obtaining second-order bounds
in general RL settings, thus further reinforcing the benefits of DistRL.",http://arxiv.org/abs/2402.07198v1,92,20,18,20,18,16
Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning,"In this paper, we for the first time propose the task of Open-domain Urban
Itinerary Planning (OUIP) for citywalk, which directly generates itineraries
based on users' requests described in natural language. OUIP is different from
conventional itinerary planning, which limits users from expressing more
detailed needs and hinders true personalization. Recently, large language
models (LLMs) have shown potential in handling diverse tasks. However, due to
non-real-time information, incomplete knowledge, and insufficient spatial
awareness, they are unable to independently deliver a satisfactory user
experience in OUIP. Given this, we present ItiNera, an OUIP system that
synergizes spatial optimization with Large Language Models (LLMs) to provide
services that customize urban itineraries based on users' needs. Specifically,
we develop an LLM-based pipeline for extracting and updating POI features to
create a user-owned personalized POI database. For each user request, we
leverage LLM in cooperation with an embedding-based module for retrieving
candidate POIs from the user's POI database. Then, a spatial optimization
module is used to order these POIs, followed by LLM crafting a personalized,
spatially coherent itinerary. To the best of our knowledge, this study marks
the first integration of LLMs to innovate itinerary planning solutions.
Extensive experiments on offline datasets and online subjective evaluation have
demonstrated the capacities of our system to deliver more responsive and
spatially coherent itineraries than current LLM-based solutions. Our system has
been deployed in production at the TuTu online travel service and has attracted
thousands of users for their urban travel planning.",http://arxiv.org/abs/2402.07204v1,92,18,18,18,20,18
The Reasons that Agents Act: Intention and Instrumental Goals,"Intention is an important and challenging concept in AI. It is important
because it underlies many other concepts we care about, such as agency,
manipulation, legal responsibility, and blame. However, ascribing intent to AI
systems is contentious, and there is no universally accepted theory of
intention applicable to AI agents. We operationalise the intention with which
an agent acts, relating to the reasons it chooses its decision. We introduce a
formal definition of intention in structural causal influence models, grounded
in the philosophy literature on intent and applicable to real-world machine
learning systems. Through a number of examples and results, we show that our
definition captures the intuitive notion of intent and satisfies desiderata
set-out by past work. In addition, we show how our definition relates to past
concepts, including actual causality, and the notion of instrumental goals,
which is a core idea in the literature on safe AI agents. Finally, we
demonstrate how our definition can be used to infer the intentions of
reinforcement learning agents and language models from their behaviour.",http://arxiv.org/abs/2402.07221v1,92,18,20,20,18,16
SPICA: Interactive Video Content Exploration through Augmented Audio Descriptions for Blind or Low-Vision Viewers,"Blind or Low-Vision (BLV) users often rely on audio descriptions (AD) to
access video content. However, conventional static ADs can leave out detailed
information in videos, impose a high mental load, neglect the diverse needs and
preferences of BLV users, and lack immersion. To tackle these challenges, we
introduce SPICA, an AI-powered system that enables BLV users to interactively
explore video content. Informed by prior empirical studies on BLV video
consumption, SPICA offers novel interactive mechanisms for supporting temporal
navigation of frame captions and spatial exploration of objects within key
frames. Leveraging an audio-visual machine learning pipeline, SPICA augments
existing ADs by adding interactivity, spatial sound effects, and individual
object descriptions without requiring additional human annotation. Through a
user study with 14 BLV participants, we evaluated the usability and usefulness
of SPICA and explored user behaviors, preferences, and mental models when
interacting with augmented ADs.",http://arxiv.org/abs/2402.07300v1,92,17,19,20,18,18
Successive Refinement in Large-Scale Computation: Advancing Model Inference Applications,"Modern computationally-intensive applications often operate under time
constraints, necessitating acceleration methods and distribution of
computational workloads across multiple entities. However, the outcome is
either achieved within the desired timeline or not, and in the latter case,
valuable resources are wasted. In this paper, we introduce solutions for
layered-resolution computation. These solutions allow lower-resolution results
to be obtained at an earlier stage than the final result. This innovation
notably enhances the deadline-based systems, as if a computational job is
terminated due to time constraints, an approximate version of the final result
can still be generated. Moreover, in certain operational regimes, a
high-resolution result might be unnecessary, because the low-resolution result
may already deviate significantly from the decision threshold, for example in
AI-based decision-making systems. Therefore, operators can decide whether
higher resolution is needed or not based on intermediate results, enabling
computations with adaptive resolution. We present our framework for two
critical and computationally demanding jobs: distributed matrix multiplication
(linear) and model inference in machine learning (nonlinear). Our theoretical
and empirical results demonstrate that the execution delay for the first
resolution is significantly shorter than that for the final resolution, while
maintaining overall complexity comparable to the conventional one-shot
approach. Our experiments further illustrate how the layering feature increases
the likelihood of meeting deadlines and enables adaptability and transparency
in massive, large-scale computations.",http://arxiv.org/abs/2402.07229v1,92,20,19,19,17,17
GenSTL: General Sparse Trajectory Learning via Auto-regressive Generation of Feature Domains,"Trajectories are sequences of timestamped location samples. In sparse
trajectories, the locations are sampled infrequently; and while such
trajectories are prevalent in real-world settings, they are challenging to use
to enable high-quality transportation-related applications. Current
methodologies either assume densely sampled and accurately map-matched
trajectories, or they rely on two-stage schemes, yielding sub-optimal
applications.
  To extend the utility of sparse trajectories, we propose a novel sparse
trajectory learning framework, GenSTL. The framework is pre-trained to form
connections between sparse trajectories and dense counterparts using
auto-regressive generation of feature domains. GenSTL can subsequently be
applied directly in downstream tasks, or it can be fine-tuned first. This way,
GenSTL eliminates the reliance on the availability of large-scale dense and
map-matched trajectory data. The inclusion of a well-crafted feature domain
encoding layer and a hierarchical masked trajectory encoder enhances GenSTL's
learning capabilities and adaptability. Experiments on two real-world
trajectory datasets offer insight into the framework's ability to contend with
sparse trajectories with different sampling intervals and its versatility
across different downstream tasks, thus offering evidence of its practicality
in real-world applications.",http://arxiv.org/abs/2402.07232v1,92,18,20,20,16,18
TransGPT: Multi-modal Generative Pre-trained Transformer for Transportation,"Natural language processing (NLP) is a key component of intelligent
transportation systems (ITS), but it faces many challenges in the
transportation domain, such as domain-specific knowledge and data, and
multi-modal inputs and outputs. This paper presents TransGPT, a novel
(multi-modal) large language model for the transportation domain, which
consists of two independent variants: TransGPT-SM for single-modal data and
TransGPT-MM for multi-modal data. TransGPT-SM is finetuned on a single-modal
Transportation dataset (STD) that contains textual data from various sources in
the transportation domain. TransGPT-MM is finetuned on a multi-modal
Transportation dataset (MTD) that we manually collected from three areas of the
transportation domain: driving tests, traffic signs, and landmarks. We evaluate
TransGPT on several benchmark datasets for different tasks in the
transportation domain, and show that it outperforms baseline models on most
tasks. We also showcase the potential applications of TransGPT for traffic
analysis and modeling, such as generating synthetic traffic scenarios,
explaining traffic phenomena, answering traffic-related questions, providing
traffic recommendations, and generating traffic reports. This work advances the
state-of-the-art of NLP in the transportation domain and provides a useful tool
for ITS researchers and practitioners.",http://arxiv.org/abs/2402.07233v1,92,18,20,20,16,18
Optimizing Genetically-Driven Synaptogenesis,"In this paper we introduce SynaptoGen, a novel framework that aims to bridge
the gap between genetic manipulations and neuronal network behavior by
simulating synaptogenesis and guiding the development of neuronal networks
capable of solving predetermined computational tasks. Drawing inspiration from
recent advancements in the field, we propose SynaptoGen as a bio-plausible
approach to modeling synaptogenesis through differentiable functions. To
validate SynaptoGen, we conduct a preliminary experiment using reinforcement
learning as a benchmark learning framework, demonstrating its effectiveness in
generating neuronal networks capable of solving the OpenAI Gym's Cart Pole
task, compared to carefully designed baselines. The results highlight the
potential of SynaptoGen to inspire further advancements in neuroscience and
computational modeling, while also acknowledging the need for incorporating
more realistic genetic rules and synaptic conductances in future research.
Overall, SynaptoGen represents a promising avenue for exploring the
intersection of genetics, neuroscience, and artificial intelligence.",http://arxiv.org/abs/2402.07242v1,92,20,18,18,18,18
SAIS: A Novel Bio-Inspired Artificial Immune System Based on Symbiotic Paradigm,"We propose a novel type of Artificial Immune System (AIS): Symbiotic
Artificial Immune Systems (SAIS), drawing inspiration from symbiotic
relationships in biology. SAIS parallels the three key stages (i.e., mutualism,
commensalism and parasitism) of population updating from the Symbiotic
Organisms Search (SOS) algorithm. This parallel approach effectively addresses
the challenges of large population size and enhances population diversity in
AIS, which traditional AIS and SOS struggle to resolve efficiently. We
conducted a series of experiments, which demonstrated that our SAIS achieved
comparable performance to the state-of-the-art approach SOS and outperformed
other popular AIS approaches and evolutionary algorithms across 26 benchmark
problems. Furthermore, we investigated the problem of parameter selection and
found that SAIS performs better in handling larger population sizes while
requiring fewer generations. Finally, we believe SAIS, as a novel bio-inspired
and immune-inspired algorithm, paves the way for innovation in bio-inspired
computing with the symbiotic paradigm.",http://arxiv.org/abs/2402.07244v1,92,19,20,18,18,17
Depth Separations in Neural Networks: Separating the Dimension from the Accuracy,"We prove an exponential separation between depth 2 and depth 3 neural
networks, when approximating an $\mathcal{O}(1)$-Lipschitz target function to
constant accuracy, with respect to a distribution with support in $[0,1]^{d}$,
assuming exponentially bounded weights. This addresses an open problem posed in
\citet{safran2019depth}, and proves that the curse of dimensionality manifests
in depth 2 approximation, even in cases where the target function can be
represented efficiently using depth 3. Previously, lower bounds that were used
to separate depth 2 from depth 3 required that at least one of the Lipschitz
parameter, target accuracy or (some measure of) the size of the domain of
approximation scale polynomially with the input dimension, whereas we fix the
former two and restrict our domain to the unit hypercube. Our lower bound holds
for a wide variety of activation functions, and is based on a novel application
of an average- to worst-case random self-reducibility argument, to reduce the
problem to threshold circuits lower bounds.",http://arxiv.org/abs/2402.07248v1,92,18,20,18,18,18
The Impact of Domain Knowledge and Multi-Modality on Intelligent Molecular Property Prediction: A Systematic Survey,"The precise prediction of molecular properties is essential for advancements
in drug development, particularly in virtual screening and compound
optimization. The recent introduction of numerous deep learning-based methods
has shown remarkable potential in enhancing molecular property prediction
(MPP), especially improving accuracy and insights into molecular structures.
Yet, two critical questions arise: does the integration of domain knowledge
augment the accuracy of molecular property prediction and does employing
multi-modal data fusion yield more precise results than unique data source
methods? To explore these matters, we comprehensively review and quantitatively
analyze recent deep learning methods based on various benchmarks. We discover
that integrating molecular information will improve both MPP regression and
classification tasks by upto 3.98% and 1.72%, respectively. We also discover
that the utilizing 3-dimensional information with 1-dimensional and
2-dimensional information simultaneously can substantially enhance MPP upto
4.2%. The two consolidated insights offer crucial guidance for future
advancements in drug discovery.",http://arxiv.org/abs/2402.07249v1,92,18,20,18,18,18
DIMON: Learning Solution Operators of Partial Differential Equations on a Diffeomorphic Family of Domains,"The solution of a PDE over varying initial/boundary conditions on multiple
domains is needed in a wide variety of applications, but it is computationally
expensive if the solution is computed de novo whenever the initial/boundary
conditions of the domain change. We introduce a general operator learning
framework, called DIffeomorphic Mapping Operator learNing (DIMON) to learn
approximate PDE solutions over a family of domains $\{\Omega_{\theta}}_\theta$,
that learns the map from initial/boundary conditions and domain $\Omega_\theta$
to the solution of the PDE, or to specified functionals thereof. DIMON is based
on transporting a given problem (initial/boundary conditions and domain
$\Omega_{\theta}$) to a problem on a reference domain $\Omega_{0}$, where
training data from multiple problems is used to learn the map to the solution
on $\Omega_{0}$, which is then re-mapped to the original domain
$\Omega_{\theta}$. We consider several problems to demonstrate the performance
of the framework in learning both static and time-dependent PDEs on non-rigid
geometries; these include solving the Laplace equation, reaction-diffusion
equations, and a multiscale PDE that characterizes the electrical propagation
on the left ventricle. This work paves the way toward the fast prediction of
PDE solutions on a family of domains and the application of neural operators in
engineering and precision medicine.",http://arxiv.org/abs/2402.07250v1,92,19,18,19,18,18
Power Transformer Fault Prediction Based on Knowledge Graphs,"In this paper, we address the challenge of learning with limited fault data
for power transformers. Traditional operation and maintenance tools lack
effective predictive capabilities for potential faults. The scarcity of
extensive fault data makes it difficult to apply machine learning techniques
effectively. To solve this problem, we propose a novel approach that leverages
the knowledge graph (KG) technology in combination with gradient boosting
decision trees (GBDT). This method is designed to efficiently learn from a
small set of high-dimensional data, integrating various factors influencing
transformer faults and historical operational data. Our approach enables
accurate safe state assessments and fault analyses of power transformers
despite the limited fault characteristic data. Experimental results demonstrate
that this method outperforms other learning approaches in prediction accuracy,
such as artificial neural networks (ANN) and logistic regression (LR).
Furthermore, it offers significant improvements in progressiveness,
practicality, and potential for widespread application.",http://arxiv.org/abs/2402.07283v1,92,18,20,20,18,16
On the Effectiveness of Machine Learning-based Call Graph Pruning: An Empirical Study,"Static call graph (CG) construction often over-approximates call relations,
leading to sound, but imprecise results. Recent research has explored machine
learning (ML)-based CG pruning as a means to enhance precision by eliminating
false edges. However, current methods suffer from a limited evaluation dataset,
imbalanced training data, and reduced recall, which affects practical
downstream analyses. Prior results were also not compared with advanced static
CG construction techniques yet. This study tackles these issues. We introduce
the NYXCorpus, a dataset of real-world Java programs with high test coverage
and we collect traces from test executions and build a ground truth of dynamic
CGs. We leverage these CGs to explore conservative pruning strategies during
the training and inference of ML-based CG pruners. We conduct a comparative
analysis of static CGs generated using zero control flow analysis (0-CFA) and
those produced by a context-sensitive 1-CFA algorithm, evaluating both with and
without pruning. We find that CG pruning is a difficult task for real-world
Java projects and substantial improvements in the CG precision (+25%) meet
reduced recall (-9%). However, our experiments show promising results: even
when we favor recall over precision by using an F2 metric in our experiments,
we can show that pruned CGs have comparable quality to a context-sensitive
1-CFA analysis while being computationally less demanding. Resulting CGs are
much smaller (69%), and substantially faster (3.5x speed-up), with virtually
unchanged results in our downstream analysis.",http://arxiv.org/abs/2402.07294v1,92,18,18,18,20,18
Training Heterogeneous Client Models using Knowledge Distillation in Serverless Federated Learning,"Federated Learning (FL) is an emerging machine learning paradigm that enables
the collaborative training of a shared global model across distributed clients
while keeping the data decentralized. Recent works on designing systems for
efficient FL have shown that utilizing serverless computing technologies,
particularly Function-as-a-Service (FaaS) for FL, can enhance resource
efficiency, reduce training costs, and alleviate the complex infrastructure
management burden on data holders. However, existing serverless FL systems
implicitly assume a uniform global model architecture across all participating
clients during training. This assumption fails to address fundamental
challenges in practical FL due to the resource and statistical data
heterogeneity among FL clients. To address these challenges and enable
heterogeneous client models in serverless FL, we utilize Knowledge Distillation
(KD) in this paper. Towards this, we propose novel optimized serverless
workflows for two popular conventional federated KD techniques, i.e., FedMD and
FedDF. We implement these workflows by introducing several extensions to an
open-source serverless FL system called FedLess. Moreover, we comprehensively
evaluate the two strategies on multiple datasets across varying levels of
client data heterogeneity using heterogeneous client models with respect to
accuracy, fine-grained training times, and costs. Results from our experiments
demonstrate that serverless FedDF is more robust to extreme non-IID data
distributions, is faster, and leads to lower costs than serverless FedMD. In
addition, compared to the original implementation, our optimizations for
particular steps in FedMD and FedDF lead to an average speedup of 3.5x and
1.76x across all datasets.",http://arxiv.org/abs/2402.07295v1,92,19,18,19,18,18
V-STaR: Training Verifiers for Self-Taught Reasoners,"Common self-improvement approaches for large language models (LLMs), such as
STaR (Zelikman et al., 2022), iteratively fine-tune LLMs on self-generated
solutions to improve their problem-solving ability. However, these approaches
discard the large amounts of incorrect solutions generated during this process,
potentially neglecting valuable information in such solutions. To address this
shortcoming, we propose V-STaR that utilizes both the correct and incorrect
solutions generated during the self-improvement process to train a verifier
using DPO that judges correctness of model-generated solutions. This verifier
is used at inference time to select one solution among many candidate
solutions. Running V-STaR for multiple iterations results in progressively
better reasoners and verifiers, delivering a 4% to 17% test accuracy
improvement over existing self-improvement and verification approaches on
common code generation and math reasoning benchmarks with LLaMA2 models.",http://arxiv.org/abs/2402.06457v1,92,20,18,20,18,16
Entropy-Regularized Token-Level Policy Optimization for Large Language Models,"Large Language Models (LLMs) have shown promise as intelligent agents in
interactive decision-making tasks. Traditional approaches often depend on
meticulously designed prompts, high-quality examples, or additional reward
models for in-context learning, supervised fine-tuning, or RLHF. Reinforcement
learning (RL) presents a dynamic alternative for LLMs to overcome these
dependencies by engaging directly with task-specific environments. Nonetheless,
it faces significant hurdles: 1) instability stemming from the exponentially
vast action space requiring exploration; 2) challenges in assigning token-level
credit based on action-level reward signals, resulting in discord between
maximizing rewards and accurately modeling corpus data. In response to these
challenges, we introduce Entropy-Regularized Token-level Policy Optimization
(ETPO), an entropy-augmented RL method tailored for optimizing LLMs at the
token level. At the heart of ETPO is our novel per-token soft Bellman update,
designed to harmonize the RL process with the principles of language modeling.
This methodology decomposes the Q-function update from a coarse action-level
view to a more granular token-level perspective, backed by theoretical proof of
optimization consistency. Crucially, this decomposition renders linear time
complexity in action exploration. We assess the effectiveness of ETPO within a
simulated environment that models data science code generation as a series of
multi-step interactive tasks; results show that ETPO achieves effective
performance improvement on the CodeLlama-7B model and surpasses a variant PPO
baseline inherited from RLHF. This underlines ETPO's potential as a robust
method for refining the interactive decision-making capabilities of LLMs.",http://arxiv.org/abs/2402.06700v1,92,18,20,20,18,16
Incorporating Taylor Series and Recursive Structure in Neural Networks for Time Series Prediction,"Time series analysis is relevant in various disciplines such as physics,
biology, chemistry, and finance. In this paper, we present a novel neural
network architecture that integrates elements from ResNet structures, while
introducing the innovative incorporation of the Taylor series framework. This
approach demonstrates notable enhancements in test accuracy across many of the
baseline datasets investigated. Furthermore, we extend our method to
incorporate a recursive step, which leads to even further improvements in test
accuracy. Our findings underscore the potential of our proposed model to
significantly advance time series analysis methodologies, offering promising
avenues for future research and application.",http://arxiv.org/abs/2402.06441v1,92,20,18,20,18,16
Trust the Process: Zero-Knowledge Machine Learning to Enhance Trust in Generative AI Interactions,"Generative AI, exemplified by models like transformers, has opened up new
possibilities in various domains but also raised concerns about fairness,
transparency and reliability, especially in fields like medicine and law. This
paper emphasizes the urgency of ensuring fairness and quality in these domains
through generative AI. It explores using cryptographic techniques, particularly
Zero-Knowledge Proofs (ZKPs), to address concerns regarding performance
fairness and accuracy while protecting model privacy. Applying ZKPs to Machine
Learning models, known as ZKML (Zero-Knowledge Machine Learning), enables
independent validation of AI-generated content without revealing sensitive
model information, promoting transparency and trust. ZKML enhances AI fairness
by providing cryptographic audit trails for model predictions and ensuring
uniform performance across users. We introduce snarkGPT, a practical ZKML
implementation for transformers, to empower users to verify output accuracy and
quality while preserving model privacy. We present a series of empirical
results studying snarkGPT's scalability and performance to assess the
feasibility and challenges of adopting a ZKML-powered approach to capture
quality and performance fairness problems in generative AI models.",http://arxiv.org/abs/2402.06414v1,92,20,19,19,17,17
Moco: A Learnable Meta Optimizer for Combinatorial Optimization,"Relevant combinatorial optimization problems (COPs) are often NP-hard. While
they have been tackled mainly via handcrafted heuristics in the past, advances
in neural networks have motivated the development of general methods to learn
heuristics from data. Many approaches utilize a neural network to directly
construct a solution, but are limited in further improving based on already
constructed solutions at inference time. Our approach, Moco, learns a graph
neural network that updates the solution construction procedure based on
features extracted from the current search state. This meta training procedure
targets the overall best solution found during the search procedure given
information such as the search budget. This allows Moco to adapt to varying
circumstances such as different computational budgets. Moco is a fully
learnable meta optimizer that does not utilize any problem specific local
search or decomposition. We test Moco on the Traveling Salesman Problem (TSP)
and Maximum Independent Set (MIS) and show that it outperforms other approaches
on MIS and is overall competitive on the TSP, especially outperforming related
approaches, partially even if they use additional local search.",http://arxiv.org/abs/2402.04915v2,92,20,18,18,18,18
Two Trades is not Baffled: Condensing Graph via Crafting Rational Gradient Matching,"Training on large-scale graphs has achieved remarkable results in graph
representation learning, but its cost and storage have raised growing concerns.
As one of the most promising directions, graph condensation methods address
these issues by employing gradient matching, aiming to condense the full graph
into a more concise yet information-rich synthetic set. Though encouraging,
these strategies primarily emphasize matching directions of the gradients,
which leads to deviations in the training trajectories. Such deviations are
further magnified by the differences between the condensation and evaluation
phases, culminating in accumulated errors, which detrimentally affect the
performance of the condensed graphs. In light of this, we propose a novel graph
condensation method named \textbf{C}raf\textbf{T}ing \textbf{R}ationa\textbf{L}
trajectory (\textbf{CTRL}), which offers an optimized starting point closer to
the original dataset's feature distribution and a more refined strategy for
gradient matching. Theoretically, CTRL can effectively neutralize the impact of
accumulated errors on the performance of condensed graphs. We provide extensive
experiments on various graph datasets and downstream tasks to support the
effectiveness of CTRL. Code is released at
https://github.com/NUS-HPC-AI-Lab/CTRL.",http://arxiv.org/abs/2402.04924v2,92,20,18,18,18,18
Multi-Sender Persuasion -- A Computational Perspective,"We consider multiple senders with informational advantage signaling to
convince a single self-interested actor towards certain actions. Generalizing
the seminal Bayesian Persuasion framework, such settings are ubiquitous in
computational economics, multi-agent learning, and machine learning with
multiple objectives. The core solution concept here is the Nash equilibrium of
senders' signaling policies. Theoretically, we prove that finding an
equilibrium in general is PPAD-Hard; in fact, even computing a sender's best
response is NP-Hard. Given these intrinsic difficulties, we turn to finding
local Nash equilibria. We propose a novel differentiable neural network to
approximate this game's non-linear and discontinuous utilities. Complementing
this with the extra-gradient algorithm, we discover local equilibria that
Pareto dominates full-revelation equilibria and those found by existing neural
networks. Broadly, our theoretical and empirical contributions are of interest
to a large class of economic problems.",http://arxiv.org/abs/2402.04971v2,92,19,19,18,18,18
Federated Learning Can Find Friends That Are Beneficial,"In Federated Learning (FL), the distributed nature and heterogeneity of
client data present both opportunities and challenges. While collaboration
among clients can significantly enhance the learning process, not all
collaborations are beneficial; some may even be detrimental. In this study, we
introduce a novel algorithm that assigns adaptive aggregation weights to
clients participating in FL training, identifying those with data distributions
most conducive to a specific learning objective. We demonstrate that our
aggregation method converges no worse than the method that aggregates only the
updates received from clients with the same data distribution. Furthermore,
empirical evaluations consistently reveal that collaborations guided by our
algorithm outperform traditional FL approaches. This underscores the critical
role of judicious client selection and lays the foundation for more streamlined
and effective FL implementations in the coming years.",http://arxiv.org/abs/2402.05050v2,92,20,18,20,18,16
On diffusion models for amortized inference: Benchmarking and improving stochastic control and sampling,"We study the problem of training diffusion models to sample from a
distribution with a given unnormalized density or energy function. We benchmark
several diffusion-structured inference methods, including simulation-based
variational approaches and off-policy methods (continuous generative flow
networks). Our results shed light on the relative advantages of existing
algorithms while bringing into question some claims from past work. We also
propose a novel exploration strategy for off-policy methods, based on local
search in the target space with the use of a replay buffer, and show that it
improves the quality of samples on a variety of target distributions. Our code
for the sampling methods and benchmarks studied is made public at
https://github.com/GFNOrg/gfn-diffusion as a base for future work on diffusion
models for amortized inference.",http://arxiv.org/abs/2402.05098v2,92,18,18,18,20,18
Bellman Conformal Inference: Calibrating Prediction Intervals For Time Series,"We introduce Bellman Conformal Inference (BCI), a framework that wraps around
any time series forecasting models and provides approximately calibrated
prediction intervals. Unlike existing methods, BCI is able to leverage
multi-step ahead forecasts and explicitly optimize the average interval lengths
by solving a one-dimensional stochastic control problem (SCP) at each time
step. In particular, we use the dynamic programming algorithm to find the
optimal policy for the SCP. We prove that BCI achieves long-term coverage under
arbitrary distribution shifts and temporal dependence, even with poor
multi-step ahead forecasts. We find empirically that BCI avoids uninformative
intervals that have infinite lengths and generates substantially shorter
prediction intervals in multiple applications when compared with existing
methods.",http://arxiv.org/abs/2402.05203v2,92,19,19,19,18,17
Do Transformer World Models Give Better Policy Gradients?,"A natural approach for reinforcement learning is to predict future rewards by
unrolling a neural network world model, and to backpropagate through the
resulting computational graph to learn a policy. However, this method often
becomes impractical for long horizons since typical world models induce
hard-to-optimize loss landscapes. Transformers are known to efficiently
propagate gradients over long horizons: could they be the solution to this
problem? Surprisingly, we show that commonly-used transformer world models
produce circuitous gradient paths, which can be detrimental to long-range
policy gradients. To tackle this challenge, we propose a class of world models
called Actions World Models (AWMs), designed to provide more direct routes for
gradient propagation. We integrate such AWMs into a policy gradient framework
that underscores the relationship between network architectures and the policy
gradient updates they inherently represent. We demonstrate that AWMs can
generate optimization landscapes that are easier to navigate even when compared
to those from the simulator itself. This property allows transformer AWMs to
produce better policies than competitive baselines in realistic long-horizon
tasks.",http://arxiv.org/abs/2402.05290v2,92,18,19,19,18,18
Sym-Q: Adaptive Symbolic Regression via Sequential Decision-Making,"Symbolic regression holds great potential for uncovering underlying
mathematical and physical relationships from empirical data. While existing
transformer-based models have recently achieved significant success in this
domain, they face challenges in terms of generalizability and adaptability.
Typically, in cases where the output expressions do not adequately fit
experimental data, the models lack efficient mechanisms to adapt or modify the
expression. This inflexibility hinders their application in real-world
scenarios, particularly in discovering unknown physical or biological
relationships. Inspired by how human experts refine and adapt expressions, we
introduce Symbolic Q-network (Sym-Q), a novel reinforcement learning-based
model that redefines symbolic regression as a sequential decision-making task.
Sym-Q leverages supervised demonstrations and refines expressions based on
reward signals indicating the quality of fitting precision. Its distinctive
ability to manage the complexity of expression trees and perform precise
step-wise updates significantly enhances flexibility and efficiency. Our
results demonstrate that Sym-Q excels not only in recovering underlying
mathematical structures but also uniquely learns to efficiently refine the
output expression based on reward signals, thereby discovering underlying
expressions. Sym-Q paves the way for more intuitive and impactful discoveries
in physical science, marking a substantial advancement in the field of symbolic
regression.",http://arxiv.org/abs/2402.05306v1,92,18,20,20,18,16
Investigating the Impact of SOLID Design Principles on Machine Learning Code Understanding,"[Context] Applying design principles has long been acknowledged as beneficial
for understanding and maintainability in traditional software projects. These
benefits may similarly hold for Machine Learning (ML) projects, which involve
iterative experimentation with data, models, and algorithms. However, ML
components are often developed by data scientists with diverse educational
backgrounds, potentially resulting in code that doesn't adhere to software
design best practices. [Goal] In order to better understand this phenomenon, we
investigated the impact of the SOLID design principles on ML code
understanding. [Method] We conducted a controlled experiment with three
independent trials involving 100 data scientists. We restructured real
industrial ML code that did not use SOLID principles. Within each trial, one
group was presented with the original ML code, while the other was presented
with ML code incorporating SOLID principles. Participants of both groups were
asked to analyze the code and fill out a questionnaire that included both
open-ended and closed-ended questions on their understanding. [Results] The
study results provide statistically significant evidence that the adoption of
the SOLID design principles can improve code understanding within the realm of
ML projects. [Conclusion] We put forward that software engineering design
principles should be spread within the data science community and considered
for enhancing the maintainability of ML code.",http://arxiv.org/abs/2402.05337v1,92,18,18,20,18,18
POLARIS: A framework to guide the development of Trustworthy AI systems,"In the ever-expanding landscape of Artificial Intelligence (AI), where
innovation thrives and new products and services are continuously being
delivered, ensuring that AI systems are designed and developed responsibly
throughout their entire lifecycle is crucial. To this end, several AI ethics
principles and guidelines have been issued to which AI systems should conform.
Nevertheless, relying solely on high-level AI ethics principles is far from
sufficient to ensure the responsible engineering of AI systems. In this field,
AI professionals often navigate by sight. Indeed, while recommendations
promoting Trustworthy AI (TAI) exist, these are often high-level statements
that are difficult to translate into concrete implementation strategies. There
is a significant gap between high-level AI ethics principles and low-level
concrete practices for AI professionals. To address this challenge, our work
presents an experience report where we develop a novel holistic framework for
Trustworthy AI - designed to bridge the gap between theory and practice - and
report insights from its application in an industrial case study. The framework
is built on the result of a systematic review of the state of the practice, a
survey, and think-aloud interviews with 34 AI practitioners. The framework,
unlike most of those already in the literature, is designed to provide
actionable guidelines and tools to support different types of stakeholders
throughout the entire Software Development Life Cycle (SDLC). Our goal is to
empower AI professionals to confidently navigate the ethical dimensions of TAI
through practical insights, ensuring that the vast potential of AI is exploited
responsibly for the benefit of society as a whole.",http://arxiv.org/abs/2402.05340v1,92,18,19,19,18,18
Attention as Robust Representation for Time Series Forecasting,"Time series forecasting is essential for many practical applications, with
the adoption of transformer-based models on the rise due to their impressive
performance in NLP and CV. Transformers' key feature, the attention mechanism,
dynamically fusing embeddings to enhance data representation, often relegating
attention weights to a byproduct role. Yet, time series data, characterized by
noise and non-stationarity, poses significant forecasting challenges. Our
approach elevates attention weights as the primary representation for time
series, capitalizing on the temporal relationships among data points to improve
forecasting accuracy. Our study shows that an attention map, structured using
global landmarks and local windows, acts as a robust kernel representation for
data points, withstanding noise and shifts in distribution. Our method
outperforms state-of-the-art models, reducing mean squared error (MSE) in
multivariate time series forecasting by a notable 3.6% without altering the
core neural network architecture. It serves as a versatile component that can
readily replace recent patching based embedding schemes in transformer-based
models, boosting their performance.",http://arxiv.org/abs/2402.05370v1,92,20,18,18,18,18
TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph Representation Learning,"Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated
state-of-the-art performance in various high-impact applications, including
fraud detection and content recommendation. Despite the success of TGNNs, they
are prone to the prevalent noise found in real-world dynamic graphs like
time-deprecated links and skewed interaction distribution. The noise causes two
critical issues that significantly compromise the accuracy of TGNNs: (1) models
are supervised by inferior interactions, and (2) noisy input induces high
variance in the aggregated messages. However, current TGNN denoising techniques
do not consider the diverse and dynamic noise pattern of each node. In
addition, they also suffer from the excessive mini-batch generation overheads
caused by traversing more neighbors. We believe the remedy for fast and
accurate TGNNs lies in temporal adaptive sampling. In this work, we propose
TASER, the first adaptive sampling method for TGNNs optimized for accuracy,
efficiency, and scalability. TASER adapts its mini-batch selection based on
training dynamics and temporal neighbor selection based on the contextual,
structural, and temporal properties of past interactions. To alleviate the
bottleneck in mini-batch generation, TASER implements a pure GPU-based temporal
neighbor finder and a dedicated GPU feature cache. We evaluate the performance
of TASER using two state-of-the-art backbone TGNNs. On five popular datasets,
TASER outperforms the corresponding baselines by an average of 2.3% in Mean
Reciprocal Rank (MRR) while achieving an average of 5.1x speedup in training
time.",http://arxiv.org/abs/2402.05396v1,92,19,18,19,18,18
Optimizing for ROC Curves on Class-Imbalanced Data by Training over a Family of Loss Functions,"Although binary classification is a well-studied problem in computer vision,
training reliable classifiers under severe class imbalance remains a
challenging problem. Recent work has proposed techniques that mitigate the
effects of training under imbalance by modifying the loss functions or
optimization methods. While this work has led to significant improvements in
the overall accuracy in the multi-class case, we observe that slight changes in
hyperparameter values of these methods can result in highly variable
performance in terms of Receiver Operating Characteristic (ROC) curves on
binary problems with severe imbalance. To reduce the sensitivity to
hyperparameter choices and train more general models, we propose training over
a family of loss functions, instead of a single loss function. We develop a
method for applying Loss Conditional Training (LCT) to an imbalanced
classification problem. Extensive experiment results, on both CIFAR and Kaggle
competition datasets, show that our method improves model performance and is
more robust to hyperparameter choices. Code will be made available at:
https://github.com/klieberman/roc_lct.",http://arxiv.org/abs/2402.05400v1,92,18,20,18,18,18
In-Context Principle Learning from Mistakes,"In-context learning (ICL, also known as few-shot prompting) has been the
standard method of adapting LLMs to downstream tasks, by learning from a few
input-output examples. Nonetheless, all ICL-based approaches only learn from
correct input-output pairs. In this paper, we revisit this paradigm, by
learning more from the few given input-output examples. We introduce Learning
Principles (LEAP): First, we intentionally induce the model to make mistakes on
these few examples; then we reflect on these mistakes, and learn explicit
task-specific ""principles"" from them, which help solve similar problems and
avoid common mistakes; finally, we prompt the model to answer unseen test
questions using the original few-shot examples and these learned general
principles. We evaluate LEAP on a wide range of benchmarks, including multi-hop
question answering (Hotpot QA), textual QA (DROP), Big-Bench Hard reasoning,
and math problems (GSM8K and MATH); in all these benchmarks, LEAP improves the
strongest available LLMs such as GPT-3.5-turbo, GPT-4, GPT-4 turbo and
Claude-2.1. For example, LEAP improves over the standard few-shot prompting
using GPT-4 by 7.5% in DROP, and by 3.3% in HotpotQA. Importantly, LEAP does
not require any more input or examples than the standard few-shot prompting
settings.",http://arxiv.org/abs/2402.05403v2,92,18,20,20,18,16
Improving Agent Interactions in Virtual Environments with Language Models,"Enhancing AI systems with efficient communication skills for effective human
assistance necessitates proactive initiatives from the system side to discern
specific circumstances and interact aptly. This research focuses on a
collective building assignment in the Minecraft dataset, employing language
modeling to enhance task understanding through state-of-the-art methods. These
models focus on grounding multi-modal understanding and task-oriented dialogue
comprehension tasks, providing insights into their interpretative and
responsive capabilities. Our experimental results showcase a substantial
improvement over existing methods, indicating a promising direction for future
research in this domain.",http://arxiv.org/abs/2402.05440v1,92,20,20,18,16,18
Private Knowledge Sharing in Distributed Learning: A Survey,"The rise of Artificial Intelligence (AI) has revolutionized numerous
industries and transformed the way society operates. Its widespread use has led
to the distribution of AI and its underlying data across many intelligent
systems. In this light, it is crucial to utilize information in learning
processes that are either distributed or owned by different entities. As a
result, modern data-driven services have been developed to integrate
distributed knowledge entities into their outcomes. In line with this goal, the
latest AI models are frequently trained in a decentralized manner. Distributed
learning involves multiple entities working together to make collective
predictions and decisions. However, this collaboration can also bring about
security vulnerabilities and challenges. This paper provides an in-depth survey
on private knowledge sharing in distributed learning, examining various
knowledge components utilized in leading distributed learning architectures.
Our analysis sheds light on the most critical vulnerabilities that may arise
when using these components in a distributed setting. We further identify and
examine defensive strategies for preserving the privacy of these knowledge
components and preventing malicious parties from manipulating or accessing the
knowledge information. Finally, we highlight several key limitations of
knowledge sharing in distributed learning and explore potential avenues for
future research.",http://arxiv.org/abs/2402.06682v1,92,18,20,18,18,18
Engineering End-to-End Remote Labs using IoT-based Retrofitting,"Remote labs are a groundbreaking development in the education industry,
providing students with access to laboratory education anytime, anywhere.
However, most remote labs are costly and difficult to scale, especially in
developing countries. With this as a motivation, this paper proposes a new
remote labs (RLabs) solution that includes two use case experiments: Vanishing
Rod and Focal Length. The hardware experiments are built at a low-cost by
retrofitting Internet of Things (IoT) components. They are also made portable
by designing miniaturised and modular setups. The software architecture
designed as part of the solution seamlessly supports the scalability of the
experiments, offering compatibility with a wide range of hardware devices and
IoT platforms. Additionally, it can live-stream remote experiments without
needing dedicated server space for the stream. The software architecture also
includes an automation suite that periodically checks the status of the
experiments using computer vision (CV). RLabs is qualitatively evaluated
against seven non-functional attributes - affordability, portability,
scalability, compatibility, maintainability, usability, and universality.
Finally, user feedback was collected from a group of students, and the scores
indicate a positive response to the students' learning and the platform's
usability.",http://arxiv.org/abs/2402.05466v1,92,20,18,19,18,17
Determining the severity of Parkinson's disease in patients using a multi task neural network,"Parkinson's disease is easy to diagnose when it is advanced, but it is very
difficult to diagnose in its early stages. Early diagnosis is essential to be
able to treat the symptoms. It impacts on daily activities and reduces the
quality of life of both the patients and their families and it is also the
second most prevalent neurodegenerative disorder after Alzheimer in people over
the age of 60. Most current studies on the prediction of Parkinson's severity
are carried out in advanced stages of the disease. In this work, the study
analyzes a set of variables that can be easily extracted from voice analysis,
making it a very non-intrusive technique. In this paper, a method based on
different deep learning techniques is proposed with two purposes. On the one
hand, to find out if a person has severe or non-severe Parkinson's disease, and
on the other hand, to determine by means of regression techniques the degree of
evolution of the disease in a given patient. The UPDRS (Unified Parkinson's
Disease Rating Scale) has been used by taking into account both the motor and
total labels, and the best results have been obtained using a mixed multi-layer
perceptron (MLP) that classifies and regresses at the same time and the most
important features of the data obtained are taken as input, using an
autoencoder. A success rate of 99.15% has been achieved in the problem of
predicting whether a person suffers from severe Parkinson's disease or
non-severe Parkinson's disease. In the degree of disease involvement prediction
problem case, a MSE (Mean Squared Error) of 0.15 has been obtained. Using a
full deep learning pipeline for data preprocessing and classification has
proven to be very promising in the field Parkinson's outperforming the
state-of-the-art proposals.",http://dx.doi.org/10.1007/s11042-023-14932-x,92,18,18,20,18,18
Investigating White-Box Attacks for On-Device Models,"Numerous mobile apps have leveraged deep learning capabilities. However,
on-device models are vulnerable to attacks as they can be easily extracted from
their corresponding mobile apps. Existing on-device attacking approaches only
generate black-box attacks, which are far less effective and efficient than
white-box strategies. This is because mobile deep learning frameworks like
TFLite do not support gradient computing, which is necessary for white-box
attacking algorithms. Thus, we argue that existing findings may underestimate
the harmfulness of on-device attacks. To this end, we conduct a study to answer
this research question: Can on-device models be directly attacked via white-box
strategies? We first systematically analyze the difficulties of transforming
the on-device model to its debuggable version, and propose a Reverse
Engineering framework for On-device Models (REOM), which automatically reverses
the compiled on-device TFLite model to the debuggable model. Specifically, REOM
first transforms compiled on-device models into Open Neural Network Exchange
format, then removes the non-debuggable parts, and converts them to the
debuggable DL models format that allows attackers to exploit in a white-box
setting. Our experimental results show that our approach is effective in
achieving automated transformation among 244 TFLite models. Compared with
previous attacks using surrogate models, REOM enables attackers to achieve
higher attack success rates with a hundred times smaller attack perturbations.
In addition, because the ONNX platform has plenty of tools for model format
exchanging, the proposed method based on the ONNX platform can be adapted to
other model formats. Our findings emphasize the need for developers to
carefully consider their model deployment strategies, and use white-box methods
to evaluate the vulnerability of on-device models.",http://dx.doi.org/10.1145/3597503.3639144,92,17,18,19,19,19
Heart disease risk prediction using deep learning techniques with feature augmentation,"Cardiovascular diseases state as one of the greatest risks of death for the
general population. Late detection in heart diseases highly conditions the
chances of survival for patients. Age, sex, cholesterol level, sugar level,
heart rate, among other factors, are known to have an influence on
life-threatening heart problems, but, due to the high amount of variables, it
is often difficult for an expert to evaluate each patient taking this
information into account. In this manuscript, the authors propose using deep
learning methods, combined with feature augmentation techniques for evaluating
whether patients are at risk of suffering cardiovascular disease. The results
of the proposed methods outperform other state of the art methods by 4.4%,
leading to a precision of a 90%, which presents a significant improvement, even
more so when it comes to an affliction that affects a large population.",http://dx.doi.org/10.1007/s11042-023-14817-z,92,18,20,20,16,18
Empowering machine learning models with contextual knowledge for enhancing the detection of eating disorders in social media posts,"Social networks are vital for information sharing, especially in the health
sector for discussing diseases and treatments. These platforms, however, often
feature posts as brief texts, posing challenges for Artificial Intelligence
(AI) in understanding context. We introduce a novel hybrid approach combining
community-maintained knowledge graphs (like Wikidata) with deep learning to
enhance the categorization of social media posts. This method uses advanced
entity recognizers and linkers (like Falcon 2.0) to connect short post entities
to knowledge graphs. Knowledge graph embeddings (KGEs) and contextualized word
embeddings (like BERT) are then employed to create rich, context-based
representations of these posts.
  Our focus is on the health domain, particularly in identifying posts related
to eating disorders (e.g., anorexia, bulimia) to aid healthcare providers in
early diagnosis. We tested our approach on a dataset of 2,000 tweets about
eating disorders, finding that merging word embeddings with knowledge graph
information enhances the predictive models' reliability. This methodology aims
to assist health experts in spotting patterns indicative of mental disorders,
thereby improving early detection and accurate diagnosis for personalized
medicine.",http://dx.doi.org/10.3233/SW-223269,92,20,18,20,18,16
Towards a Thermodynamical Deep-Learning-Vision-Based Flexible Robotic Cell for Circular Healthcare,"The dependence on finite reserves of raw materials and the production of
waste are two unsolved problems of the traditional linear economy. Healthcare,
as a major sector of any nation, is currently facing them. Hence, in this
paper, we report theoretical and practical advances of robotic reprocessing of
small medical devices. Specifically, on the theory, we combine compartmental
dynamical thermodynamics with the mechanics of robots to integrate robotics
into a system-level perspective, and then, propose graph-based circularity
indicators by leveraging our thermodynamic framework. Our thermodynamic
framework is also a step forward in defining the theoretical foundations of
circular material flow designs as it improves material flow analysis (MFA) by
adding dynamical energy balances to the usual mass balances. On the practice,
we report on the on-going design of a flexible robotic cell enabled by
deep-learning vision for resources mapping and quantification, disassembly, and
waste sorting of small medical devices.",http://arxiv.org/abs/2402.05551v1,92,18,18,20,18,18
Flashback: Understanding and Mitigating Forgetting in Federated Learning,"In Federated Learning (FL), forgetting, or the loss of knowledge across
rounds, hampers algorithm convergence, particularly in the presence of severe
data heterogeneity among clients. This study explores the nuances of this
issue, emphasizing the critical role of forgetting in FL's inefficient learning
within heterogeneous data contexts. Knowledge loss occurs in both client-local
updates and server-side aggregation steps; addressing one without the other
fails to mitigate forgetting. We introduce a metric to measure forgetting
granularly, ensuring distinct recognition amid new knowledge acquisition.
Leveraging these insights, we propose Flashback, an FL algorithm with a dynamic
distillation approach that is used to regularize the local models, and
effectively aggregate their knowledge. Across different benchmarks, Flashback
outperforms other methods, mitigates forgetting, and achieves faster
round-to-target-accuracy, by converging in 6 to 16 rounds.",http://arxiv.org/abs/2402.05558v1,92,18,20,18,18,18
Hypergraph Node Classification With Graph Neural Networks,"Hypergraphs, with hyperedges connecting more than two nodes, are key for
modelling higher-order interactions in real-world data. The success of graph
neural networks (GNNs) reveals the capability of neural networks to process
data with pairwise interactions. This inspires the usage of neural networks for
data with higher-order interactions, thereby leading to the development of
hypergraph neural networks (HyperGNNs). GNNs and HyperGNNs are typically
considered distinct since they are designed for data on different geometric
topologies. However, in this paper, we theoretically demonstrate that, in the
context of node classification, most HyperGNNs can be approximated using a GNN
with a weighted clique expansion of the hypergraph. This leads to WCE-GNN, a
simple and efficient framework comprising a GNN and a weighted clique expansion
(WCE), for hypergraph node classification. Experiments on nine real-world
hypergraph node classification benchmarks showcase that WCE-GNN demonstrates
not only higher classification accuracy compared to state-of-the-art HyperGNNs,
but also superior memory and runtime efficiency.",http://arxiv.org/abs/2402.05569v1,92,17,20,19,18,18
Simultaneously Achieving Group Exposure Fairness and Within-Group Meritocracy in Stochastic Bandits,"Existing approaches to fairness in stochastic multi-armed bandits (MAB)
primarily focus on exposure guarantee to individual arms. When arms are
naturally grouped by certain attribute(s), we propose Bi-Level Fairness, which
considers two levels of fairness. At the first level, Bi-Level Fairness
guarantees a certain minimum exposure to each group. To address the unbalanced
allocation of pulls to individual arms within a group, we consider meritocratic
fairness at the second level, which ensures that each arm is pulled according
to its merit within the group. Our work shows that we can adapt a UCB-based
algorithm to achieve a Bi-Level Fairness by providing (i) anytime Group
Exposure Fairness guarantees and (ii) ensuring individual-level Meritocratic
Fairness within each group. We first show that one can decompose regret bounds
into two components: (a) regret due to anytime group exposure fairness and (b)
regret due to meritocratic fairness within each group. Our proposed algorithm
BF-UCB balances these two regrets optimally to achieve the upper bound of
$O(\sqrt{T})$ on regret; $T$ being the stopping time. With the help of
simulated experiments, we further show that BF-UCB achieves sub-linear regret;
provides better group and individual exposure guarantees compared to existing
algorithms; and does not result in a significant drop in reward with respect to
UCB algorithm, which does not impose any fairness constraint.",http://arxiv.org/abs/2402.05575v1,92,20,19,19,16,18
Spiking-PhysFormer: Camera-Based Remote Photoplethysmography with Parallel Spike-driven Transformer,"Artificial neural networks (ANNs) can help camera-based remote
photoplethysmography (rPPG) in measuring cardiac activity and physiological
signals from facial videos, such as pulse wave, heart rate and respiration rate
with better accuracy. However, most existing ANN-based methods require
substantial computing resources, which poses challenges for effective
deployment on mobile devices. Spiking neural networks (SNNs), on the other
hand, hold immense potential for energy-efficient deep learning owing to their
binary and event-driven architecture. To the best of our knowledge, we are the
first to introduce SNNs into the realm of rPPG, proposing a hybrid neural
network (HNN) model, the Spiking-PhysFormer, aimed at reducing power
consumption. Specifically, the proposed Spiking-PhyFormer consists of an
ANN-based patch embedding block, SNN-based transformer blocks, and an ANN-based
predictor head. First, to simplify the transformer block while preserving its
capacity to aggregate local and global spatio-temporal features, we design a
parallel spike transformer block to replace sequential sub-blocks.
Additionally, we propose a simplified spiking self-attention mechanism that
omits the value parameter without compromising the model's performance.
Experiments conducted on four datasets-PURE, UBFC-rPPG, UBFC-Phys, and MMPD
demonstrate that the proposed model achieves a 12.4\% reduction in power
consumption compared to PhysFormer. Additionally, the power consumption of the
transformer block is reduced by a factor of 12.2, while maintaining decent
performance as PhysFormer and other ANN-based models.",http://arxiv.org/abs/2402.04798v2,92,20,18,18,18,18
Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models,"Large Language Models (LLMs) are deployed as powerful tools for several
natural language processing (NLP) applications. Recent works show that modern
LLMs can generate self-explanations (SEs), which elicit their intermediate
reasoning steps for explaining their behavior. Self-explanations have seen
widespread adoption owing to their conversational and plausible nature.
However, there is little to no understanding of their faithfulness. In this
work, we discuss the dichotomy between faithfulness and plausibility in SEs
generated by LLMs. We argue that while LLMs are adept at generating plausible
explanations -- seemingly logical and coherent to human users -- these
explanations do not necessarily align with the reasoning processes of the LLMs,
raising concerns about their faithfulness. We highlight that the current trend
towards increasing the plausibility of explanations, primarily driven by the
demand for user-friendly interfaces, may come at the cost of diminishing their
faithfulness. We assert that the faithfulness of explanations is critical in
LLMs employed for high-stakes decision-making. Moreover, we urge the community
to identify the faithfulness requirements of real-world applications and ensure
explanations meet those needs. Finally, we propose some directions for future
work, emphasizing the need for novel methodologies and frameworks that can
enhance the faithfulness of self-explanations without compromising their
plausibility, essential for the transparent deployment of LLMs in diverse
high-stakes domains.",http://arxiv.org/abs/2402.04614v2,92,18,20,18,18,18
Large Language Models As MOOCs Graders,"Massive open online courses (MOOCs) unlock the doors to free education for
anyone around the globe with access to a computer and the internet. Despite
this democratization of learning, the massive enrollment in these courses means
it is almost impossible for one instructor to assess every student's writing
assignment. As a result, peer grading, often guided by a straightforward
rubric, is the method of choice. While convenient, peer grading often falls
short in terms of reliability and validity. In this study, using 18 distinct
settings, we explore the feasibility of leveraging large language models (LLMs)
to replace peer grading in MOOCs. Specifically, we focus on two
state-of-the-art LLMs: GPT-4 and GPT-3.5, across three distinct courses:
Introductory Astronomy, Astrobiology, and the History and Philosophy of
Astronomy. To instruct LLMs, we use three different prompts based on a variant
of the zero-shot chain-of-thought (Zero-shot-CoT) prompting technique:
Zero-shot-CoT combined with instructor-provided correct answers; Zero-shot-CoT
in conjunction with both instructor-formulated answers and rubrics; and
Zero-shot-CoT with instructor-offered correct answers and LLM-generated
rubrics. Our results show that Zero-shot-CoT, when integrated with
instructor-provided answers and rubrics, produces grades that are more aligned
with those assigned by instructors compared to peer grading. However, the
History and Philosophy of Astronomy course proves to be more challenging in
terms of grading as opposed to other courses. Finally, our study reveals a
promising direction for automating grading systems for MOOCs, especially in
subjects with well-defined rubrics.",http://arxiv.org/abs/2402.03776v2,92,19,19,19,20,15
"Checkmating One, by Using Many: Combining Mixture of Experts with MCTS to Improve in Chess","This paper presents a new approach that integrates deep learning with
computational chess, using both the Mixture of Experts (MoE) method and
Monte-Carlo Tree Search (MCTS). Our methodology employs a suite of specialized
models, each designed to respond to specific changes in the game's input data.
This results in a framework with sparsely activated models, which provides
significant computational benefits. Our framework combines the MoE method with
MCTS, in order to align it with the strategic phases of chess, thus departing
from the conventional ``one-for-all'' model. Instead, we utilize distinct game
phase definitions to effectively distribute computational tasks across multiple
expert neural networks. Our empirical research shows a substantial improvement
in playing strength, surpassing the traditional single-model framework. This
validates the efficacy of our integrated approach and highlights the potential
of incorporating expert knowledge and strategic principles into neural network
design. The fusion of MoE and MCTS offers a promising avenue for advancing
machine learning architectures.",http://arxiv.org/abs/2401.16852v2,92,20,20,18,18,16
Emergent Dominance Hierarchies in Reinforcement Learning Agents,"Modern Reinforcement Learning (RL) algorithms are able to outperform humans
in a wide variety of tasks. Multi-agent reinforcement learning (MARL) settings
present additional challenges, and successful cooperation in mixed-motive
groups of agents depends on a delicate balancing act between individual and
group objectives. Social conventions and norms, often inspired by human
institutions, are used as tools for striking this balance.
  In this paper, we examine a fundamental, well-studied social convention that
underlies cooperation in both animal and human societies: dominance
hierarchies.
  We adapt the ethological theory of dominance hierarchies to artificial
agents, borrowing the established terminology and definitions with as few
amendments as possible. We demonstrate that populations of RL agents, operating
without explicit programming or intrinsic rewards, can invent, learn, enforce,
and transmit a dominance hierarchy to new populations. The dominance
hierarchies that emerge have a similar structure to those studied in chickens,
mice, fish, and other species.",http://arxiv.org/abs/2401.12258v3,92,18,18,20,18,18
TurboSVM-FL: Boosting Federated Learning through SVM Aggregation for Lazy Clients,"Federated learning is a distributed collaborative machine learning paradigm
that has gained strong momentum in recent years. In federated learning, a
central server periodically coordinates models with clients and aggregates the
models trained locally by clients without necessitating access to local data.
Despite its potential, the implementation of federated learning continues to
encounter several challenges, predominantly the slow convergence that is
largely due to data heterogeneity. The slow convergence becomes particularly
problematic in cross-device federated learning scenarios where clients may be
strongly limited by computing power and storage space, and hence counteracting
methods that induce additional computation or memory cost on the client side
such as auxiliary objective terms and larger training iterations can be
impractical. In this paper, we propose a novel federated aggregation strategy,
TurboSVM-FL, that poses no additional computation burden on the client side and
can significantly accelerate convergence for federated classification task,
especially when clients are ""lazy"" and train their models solely for few epochs
for next global aggregation. TurboSVM-FL extensively utilizes support vector
machine to conduct selective aggregation and max-margin spread-out
regularization on class embeddings. We evaluate TurboSVM-FL on multiple
datasets including FEMNIST, CelebA, and Shakespeare using user-independent
validation with non-iid data distribution. Our results show that TurboSVM-FL
can significantly outperform existing popular algorithms on convergence rate
and reduce communication rounds while delivering better test metrics including
accuracy, F1 score, and MCC.",http://arxiv.org/abs/2401.12012v4,92,19,20,18,18,17
DALex: Lexicase-like Selection via Diverse Aggregation,"Lexicase selection has been shown to provide advantages over other selection
algorithms in several areas of evolutionary computation and machine learning.
In its standard form, lexicase selection filters a population or other
collection based on randomly ordered training cases that are considered one at
a time. This iterated filtering process can be time-consuming, particularly in
settings with large numbers of training cases. In this paper, we propose a new
method that is nearly equivalent to lexicase selection in terms of the
individuals that it selects, but which does so significantly more quickly. The
new method, called DALex (for Diversely Aggregated Lexicase), selects the best
individual with respect to a weighted sum of training case errors, where the
weights are randomly sampled. This allows us to formulate the core computation
required for selection as matrix multiplication instead of recursive loops of
comparisons, which in turn allows us to take advantage of optimized and
parallel algorithms designed for matrix multiplication for speedup.
Furthermore, we show that we can interpolate between the behavior of lexicase
selection and its ""relaxed"" variants, such as epsilon or batch lexicase
selection, by adjusting a single hyperparameter, named ""particularity
pressure,"" which represents the importance granted to each individual training
case. Results on program synthesis, deep learning, symbolic regression, and
learning classifier systems demonstrate that DALex achieves significant
speedups over lexicase selection and its relaxed variants while maintaining
almost identical problem-solving performance. Under a fixed computational
budget, these savings free up resources that can be directed towards increasing
population size or the number of generations, enabling the potential for
solving more difficult problems.",http://arxiv.org/abs/2401.12424v2,92,20,18,20,18,16
ReposVul: A Repository-Level High-Quality Vulnerability Dataset,"Open-Source Software (OSS) vulnerabilities bring great challenges to the
software security and pose potential risks to our society. Enormous efforts
have been devoted into automated vulnerability detection, among which deep
learning (DL)-based approaches have proven to be the most effective. However,
the current labeled data present the following limitations: (1) Tangled
Patches: Developers may submit code changes unrelated to vulnerability fixes
within patches, leading to tangled patches. (2) Lacking Inter-procedural
Vulnerabilities: The existing vulnerability datasets typically contain
function-level and file-level vulnerabilities, ignoring the relations between
functions, thus rendering the approaches unable to detect the inter-procedural
vulnerabilities. (3) Outdated Patches: The existing datasets usually contain
outdated patches, which may bias the model during training.
  To address the above limitations, in this paper, we propose an automated data
collection framework and construct the first repository-level high-quality
vulnerability dataset named ReposVul. The proposed framework mainly contains
three modules: (1) A vulnerability untangling module, aiming at distinguishing
vulnerability-fixing related code changes from tangled patches, in which the
Large Language Models (LLMs) and static analysis tools are jointly employed.
(2) A multi-granularity dependency extraction module, aiming at capturing the
inter-procedural call relationships of vulnerabilities, in which we construct
multiple-granularity information for each vulnerability patch, including
repository-level, file-level, function-level, and line-level. (3) A trace-based
filtering module, aiming at filtering the outdated patches, which leverages the
file path trace-based filter and commit time trace-based filter to construct an
up-to-date dataset.",http://arxiv.org/abs/2401.13169v2,92,20,18,18,20,16
Cyber-Twin: Digital Twin-boosted Autonomous Attack Detection for Vehicular Ad-Hoc Networks,"The rapid evolution of Vehicular Ad-hoc NETworks (VANETs) has ushered in a
transformative era for intelligent transportation systems (ITS), significantly
enhancing road safety and vehicular communication. However, the intricate and
dynamic nature of VANETs presents formidable challenges, particularly in
vehicle-to-infrastructure (V2I) communications. Roadside Units (RSUs), integral
components of VANETs, are increasingly susceptible to cyberattacks, such as
jamming and distributed denial-of-service (DDoS) attacks. These vulnerabilities
pose grave risks to road safety, potentially leading to traffic congestion and
vehicle malfunctions. Current approaches often struggle to effectively merge
digital twin technology with Artificial Intelligence (AI) models to boost
security and sustainability. Our study introduces an innovative cyber-twin
framework tailored to enhance the security of RSUs in VANETs. This framework
uniquely combines digital twin technology with cutting-edge AI to offer a
real-time, dynamic representation of RSUs. This allows for detailed monitoring
and efficient detection of threats, significantly strengthening RSU security in
VANETs. Moreover, our framework makes a notable contribution to eco-friendly
communication by improving the computational efficiency of RSUs, leading to
increased energy efficiency and extended hardware durability. Our results show
a considerable enhancement in resource management and attack detection,
surpassing the performance of existing solutions. In particular, the cyber-twin
framework showed a substantial reduction in RSU load and an optimal balance
between resource consumption and high attack detection efficiency, with a
defined twinning rate range of seventy-six to ninety per cent. These
advancements underscore our commitment to developing sustainable, secure, and
resilient vehicular communication systems for the future of smart cities.",http://arxiv.org/abs/2401.14005v3,92,20,19,19,18,16
Prompting Large Language Models for Zero-Shot Clinical Prediction with Structured Longitudinal Electronic Health Record Data,"The inherent complexity of structured longitudinal Electronic Health Records
(EHR) data poses a significant challenge when integrated with Large Language
Models (LLMs), which are traditionally tailored for natural language
processing. Motivated by the urgent need for swift decision-making during new
disease outbreaks, where traditional predictive models often fail due to a lack
of historical data, this research investigates the adaptability of LLMs, like
GPT-4, to EHR data. We particularly focus on their zero-shot capabilities,
which enable them to make predictions in scenarios in which they haven't been
explicitly trained. In response to the longitudinal, sparse, and
knowledge-infused nature of EHR data, our prompting approach involves taking
into account specific EHR characteristics such as units and reference ranges,
and employing an in-context learning strategy that aligns with clinical
contexts. Our comprehensive experiments on the MIMIC-IV and TJH datasets
demonstrate that with our elaborately designed prompting framework, LLMs can
improve prediction performance in key tasks such as mortality, length-of-stay,
and 30-day readmission by about 35\%, surpassing ML models in few-shot
settings. Our research underscores the potential of LLMs in enhancing clinical
decision-making, especially in urgent healthcare situations like the outbreak
of emerging diseases with no labeled data. The code is publicly available at
https://github.com/yhzhu99/llm4healthcare for reproducibility.",http://arxiv.org/abs/2402.01713v2,92,19,20,19,18,16
Scalable Qualitative Coding with LLMs: Chain-of-Thought Reasoning Matches Human Performance in Some Hermeneutic Tasks,"Qualitative coding, or content analysis, extracts meaning from text to
discern quantitative patterns across a corpus of texts. Recently, advances in
the interpretive abilities of large language models (LLMs) offer potential for
automating the coding process (applying category labels to texts), thereby
enabling human researchers to concentrate on more creative research aspects,
while delegating these interpretive tasks to AI. Our case study comprises a set
of socio-historical codes on dense, paragraph-long passages representative of a
humanistic study. We show that GPT-4 is capable of human-equivalent
interpretations, whereas GPT-3.5 is not. Compared to our human-derived gold
standard, GPT-4 delivers excellent intercoder reliability (Cohen's $\kappa \geq
0.79$) for 3 of 9 codes, and substantial reliability ($\kappa \geq 0.6$) for 8
of 9 codes. In contrast, GPT-3.5 greatly underperforms for all codes
($mean(\kappa) = 0.34$; $max(\kappa) = 0.55$). Importantly, we find that coding
fidelity improves considerably when the LLM is prompted to give rationale
justifying its coding decisions (chain-of-thought reasoning). We present these
and other findings along with a set of best practices for adapting traditional
codebooks for LLMs. Our results indicate that for certain codebooks,
state-of-the-art LLMs are already adept at large-scale content analysis.
Furthermore, they suggest the next generation of models will likely render AI
coding a viable option for a majority of codebooks.",http://arxiv.org/abs/2401.15170v2,92,18,20,20,18,16
An open dataset for oracle bone script recognition and decipherment,"Oracle Bone Script (OBS), one of the earliest known forms of ancient Chinese
writing, holds invaluable insights into the humanities and geography of the
Shang Dynasty, dating back 3,000 years. The immense historical and cultural
significance of these writings cannot be overstated. However, the passage of
time has obscured much of their meaning, presenting a significant challenge in
deciphering these ancient texts. With the advent of Artificial Intelligence
(AI), employing AI to assist in interpreting OBS has become a feasible option.
Yet, progress in this area has been hindered by a lack of high-quality
datasets. To address this issue, this paper details the creation of the
HUST-OBS dataset. This dataset encompasses 77,064 images of 1,588 individual
deciphered scripts and 62,989 images of 9,411 undeciphered characters, with a
total of 140,053 images, compiled from diverse sources. Additionally, all
images and labels have been reviewed and corrected by experts in oracle bone
studies. The hope is that this dataset could inspire and assist future research
in deciphering those unknown OBS.",http://arxiv.org/abs/2401.15365v2,92,19,19,19,18,17
Future Impact Decomposition in Request-level Recommendations,"In recommender systems, reinforcement learning solutions have shown promising
results in optimizing the interaction sequence between users and the system
over the long-term performance. For practical reasons, the policy's actions are
typically designed as recommending a list of items to handle users' frequent
and continuous browsing requests more efficiently. In this list-wise
recommendation scenario, the user state is updated upon every request in the
corresponding MDP formulation. However, this request-level formulation is
essentially inconsistent with the user's item-level behavior. In this study, we
demonstrate that an item-level optimization approach can better utilize item
characteristics and optimize the policy's performance even under the
request-level MDP. We support this claim by comparing the performance of
standard request-level methods with the proposed item-level actor-critic
framework in both simulation and online experiments. Furthermore, we show that
a reward-based future decomposition strategy can better express the item-wise
future impact and improve the recommendation accuracy in the long term. To
achieve a more thorough understanding of the decomposition strategy, we propose
a model-based re-weighting framework with adversarial learning that further
boost the performance and investigate its correlation with the reward-based
strategy.",http://arxiv.org/abs/2401.16108v4,92,18,20,20,18,16
On the Semantics of LM Latent Space: A Vocabulary-defined Approach,"Understanding the latent space of language models (LM) is crucial to refining
their performance and interpretability. Existing analyses often fall short in
providing disentangled (model-centric) insights into LM semantics, and neglect
essential aspects of LM adaption. In response, we introduce a pioneering method
called vocabulary-defined semantics, which establishes a reference frame within
the LM latent space, ensuring disentangled semantic analysis grounded in LM
vocabulary. Our approach transcends prior entangled analysis, leveraging LM
vocabulary for model-centric insights. Furthermore, we propose a novel
technique to compute logits, emphasising differentiability and local isotropy,
and introduce a neural clustering module for semantically calibrating data
representations during LM adaptation. Through extensive experiments across
diverse text understanding datasets, our approach outperforms state-of-the-art
methods of retrieval-augmented generation and parameter-efficient finetuning,
showcasing its efficacy and broad applicability. Our findings not only shed
light on LM mechanics, but also offer practical solutions to enhance LM
performance and interpretability.",http://arxiv.org/abs/2401.16184v3,92,20,18,19,18,17
Generalization of LiNGAM that allows confounding,"LiNGAM determines the variable order from cause to effect using additive
noise models, but it faces challenges with confounding. Previous methods
maintained LiNGAM's fundamental structure while trying to identify and address
variables affected by confounding. As a result, these methods required
significant computational resources regardless of the presence of confounding,
and they did not ensure the detection of all confounding types. In contrast,
this paper enhances LiNGAM by introducing LiNGAM-MMI, a method that quantifies
the magnitude of confounding using KL divergence and arranges the variables to
minimize its impact. This method efficiently achieves a globally optimal
variable order through the shortest path problem formulation. LiNGAM-MMI
processes data as efficiently as traditional LiNGAM in scenarios without
confounding while effectively addressing confounding situations. Our
experimental results suggest that LiNGAM-MMI more accurately determines the
correct variable order, both in the presence and absence of confounding.",http://arxiv.org/abs/2401.16661v3,92,20,18,20,18,16
Hypermultiplexed Integrated Tensor Optical Processor,"The escalating data volume and complexity resulting from the rapid expansion
of artificial intelligence (AI), internet of things (IoT) and 5G/6G mobile
networks is creating an urgent need for energy-efficient, scalable computing
hardware. Here we demonstrate a hypermultiplexed integrated tensor optical
processor (HITOP) that performs trillions of operations per second (TeraOPS) at
the energy cost of 25 femtojoule per operation (25 fJ/OP). Based on
space-time-wavelength three-dimensional (3D) data streaming, HITOP is built
with arrays of wafer-fabricated III/V-based micron-scale lasers (spanning ~1
THz) incorporating thin-film Lithium-Niobate electro-optic (EO) photonics.
Multidimensional parallelism allows matrix-matrix multiplications ($N^{3}$
operations) using $O(N)$ devices, facilitating scalable on-chip integration.
With each device activating 10 billion parameters per second, the HITOP
scalability is validated in machine learning models with 405,000 parameters,
which is 25,000 times more than previous integrated optical systems. A
combination of high clockrates (10 GS/s), parallel processing and real-time
reprogrammability unlocks the full potential of light for next-generation AI
accelerators in applications ranging from training with trillions of
parameters, real-time decision making in autonomous vehicles and robotics,
dynamic optimization in smart manufacturing, to complex simulation for climate
modeling and drug discovery.",http://arxiv.org/abs/2401.18050v3,92,20,20,20,16,16
Logical Specifications-guided Dynamic Task Sampling for Reinforcement Learning Agents,"Reinforcement Learning (RL) has made significant strides in enabling
artificial agents to learn diverse behaviors. However, learning an effective
policy often requires a large number of environment interactions. To mitigate
sample complexity issues, recent approaches have used high-level task
specifications, such as Linear Temporal Logic (LTL$_f$) formulas or Reward
Machines (RM), to guide the learning progress of the agent. In this work, we
propose a novel approach, called Logical Specifications-guided Dynamic Task
Sampling (LSTS), that learns a set of RL policies to guide an agent from an
initial state to a goal state based on a high-level task specification, while
minimizing the number of environmental interactions. Unlike previous work, LSTS
does not assume information about the environment dynamics or the Reward
Machine, and dynamically samples promising tasks that lead to successful goal
policies. We evaluate LSTS on a gridworld and show that it achieves improved
time-to-threshold performance on complex sequential decision-making problems
compared to state-of-the-art RM and Automaton-guided RL baselines, such as
Q-Learning for Reward Machines and Compositional RL from logical Specifications
(DIRL). Moreover, we demonstrate that our method outperforms RM and
Automaton-guided RL baselines in terms of sample-efficiency, both in a
partially observable robotic task and in a continuous control robotic
manipulation task.",http://arxiv.org/abs/2402.03678v2,92,19,20,19,17,17
Benchmarking Transferable Adversarial Attacks,"The robustness of deep learning models against adversarial attacks remains a
pivotal concern. This study presents, for the first time, an exhaustive review
of the transferability aspect of adversarial attacks. It systematically
categorizes and critically evaluates various methodologies developed to augment
the transferability of adversarial attacks. This study encompasses a spectrum
of techniques, including Generative Structure, Semantic Similarity, Gradient
Editing, Target Modification, and Ensemble Approach. Concurrently, this paper
introduces a benchmark framework \textit{TAA-Bench}, integrating ten leading
methodologies for adversarial attack transferability, thereby providing a
standardized and systematic platform for comparative analysis across diverse
model architectures. Through comprehensive scrutiny, we delineate the efficacy
and constraints of each method, shedding light on their underlying operational
principles and practical utility. This review endeavors to be a quintessential
resource for both scholars and practitioners in the field, charting the complex
terrain of adversarial transferability and setting a foundation for future
explorations in this vital sector. The associated codebase is accessible at:
https://github.com/KxPlaug/TAA-Bench",http://arxiv.org/abs/2402.00418v2,92,18,20,20,18,16
Health-LLM: Personalized Retrieval-Augmented Disease Prediction Model,"Artificial intelligence (AI) in healthcare has significantly advanced
intelligent medical treatment. However, traditional intelligent healthcare is
limited by static data and unified standards, preventing full integration with
individual situations and other challenges. Hence, a more professional and
detailed intelligent healthcare method is needed for development. To this end,
we propose an innovative framework named Heath-LLM, which combines large-scale
feature extraction and medical knowledge trade-off scoring. Compared to
traditional health management methods, our approach has three main advantages.
First, our method integrates health reports into a large model to provide
detailed task information. Second, professional medical expertise is used to
adjust the weighted scores of health characteristics. Third, we use a
semi-automated feature extraction framework to enhance the analytical power of
language models and incorporate expert insights to improve the accuracy of
disease prediction. We have conducted disease prediction experiments on a large
number of health reports to assess the effectiveness of Health-LLM. The results
of the experiments indicate that the proposed method surpasses traditional
methods and has the potential to revolutionize disease prediction and
personalized health management. The code is available at
https://github.com/jmyissb/HealthLLM.",http://arxiv.org/abs/2402.00746v4,92,19,19,19,17,18
"Monotone, Bi-Lipschitz, and Polyak-Lojasiewicz Networks","This paper presents a new \emph{bi-Lipschitz} invertible neural network, the
BiLipNet, which has the ability to control both its \emph{Lipschitzness}
(output sensitivity to input perturbations) and \emph{inverse Lipschitzness}
(input distinguishability from different outputs). The main contribution is a
novel invertible residual layer with certified strong monotonicity and
Lipschitzness, which we compose with orthogonal layers to build bi-Lipschitz
networks. The certification is based on incremental quadratic constraints,
which achieves much tighter bounds compared to spectral normalization.
Moreover, we formulate the model inverse calculation as a three-operator
splitting problem, for which fast algorithms are known. Based on the proposed
bi-Lipschitz network, we introduce a new scalar-output network, the PLNet,
which satisfies the Polyak-\L{}ojasiewicz condition. It can be applied to learn
non-convex surrogate losses with favourable properties, e.g., a unique and
efficiently-computable global minimum.",http://arxiv.org/abs/2402.01344v2,92,20,18,20,18,16
ALERT-Transformer: Bridging Asynchronous and Synchronous Machine Learning for Real-Time Event-based Spatio-Temporal Data,"We seek to enable classic processing of continuous ultra-sparse
spatiotemporal data generated by event-based sensors with dense machine
learning models. We propose a novel hybrid pipeline composed of asynchronous
sensing and synchronous processing that combines several ideas: (1) an
embedding based on PointNet models -- the ALERT module -- that can continuously
integrate new and dismiss old events thanks to a leakage mechanism, (2) a
flexible readout of the embedded data that allows to feed any downstream model
with always up-to-date features at any sampling rate, (3) exploiting the input
sparsity in a patch-based approach inspired by Vision Transformer to optimize
the efficiency of the method. These embeddings are then processed by a
transformer model trained for object and gesture recognition. Using this
approach, we achieve performances at the state-of-the-art with a lower latency
than competitors. We also demonstrate that our asynchronous model can operate
at any desired sampling rate.",http://arxiv.org/abs/2402.01393v2,92,19,19,19,17,18
Understanding Growth Mindset Practices in an Introductory Physical Computing Classroom: High School Students' Engagement with Debugging by Design Activities,"Background and Context: While debugging is recognized as an essential
practice, for many students, encountering bugs can generate emotional responses
such as fear and anxiety that can lead to disengagement and the avoidance of
computer programming. Growth mindsets can support perseverance and learning in
these situations, yet few studies have investigated how growth mindsets emerge
in practice amongst K-12 computing students facing physical computing debugging
challenges. Objective: We seek to understand what (if any) growth mindset
practices high school students exhibited when creating and exchanging buggy
physical computing projects for their peers to solve during a Debugging by
Design activity as part of their introductory computing course. Method: We
focused on moment-to-moment microgenetic analysis of student interactions in
designing and solving bugs for others to examine the practices students
exhibited that demonstrated the development of a growth mindset and the
contexts in which these practices emerged. Findings: We identified five
emergent growth mindset practices: choosing challenges that lead to more
learning, persisting after setbacks, giving and valuing praise for effort,
approaching learning as constant improvement, and developing comfort with
failure. Students most often exhibited these practices in peer-to-peer
interactions and while making buggy physical computing projects for their peers
to solve. Implications: Our analysis contributes to a more holistic
understanding of students' social, emotional, and motivational approaches to
debugging physical computing projects through the characterization of growth
mindset practices. The presented inventory of growth mindset practices may be
helpful to further study growth mindset in action in other computing settings.",http://arxiv.org/abs/2402.01885v2,92,18,16,20,18,20
Your Diffusion Model is Secretly a Certifiably Robust Classifier,"Diffusion models are recently employed as generative classifiers for robust
classification. However, a comprehensive theoretical understanding of the
robustness of diffusion classifiers is still lacking, leading us to question
whether they will be vulnerable to future stronger attacks. In this study, we
propose a new family of diffusion classifiers, named Noised Diffusion
Classifiers~(NDCs), that possess state-of-the-art certified robustness.
Specifically, we generalize the diffusion classifiers to classify
Gaussian-corrupted data by deriving the evidence lower bounds (ELBOs) for these
distributions, approximating the likelihood using the ELBO, and calculating
classification probabilities via Bayes' theorem. We integrate these generalized
diffusion classifiers with randomized smoothing to construct smoothed
classifiers possessing non-constant Lipschitzness. Experimental results
demonstrate the superior certified robustness of our proposed NDCs. Notably, we
are the first to achieve 80\%+ and 70\%+ certified robustness on CIFAR-10 under
adversarial perturbations with $\ell_2$ norm less than 0.25 and 0.5,
respectively, using a single off-the-shelf diffusion model without any
additional data.",http://arxiv.org/abs/2402.02316v2,92,20,18,20,18,16
"BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation","In this paper, we present a new embedding model, called M3-Embedding, which
is distinguished for its versatility in Multi-Linguality, Multi-Functionality,
and Multi-Granularity. It can support more than 100 working languages, leading
to new state-of-the-art performances on multi-lingual and cross-lingual
retrieval tasks. It can simultaneously perform the three common retrieval
functionalities of embedding model: dense retrieval, multi-vector retrieval,
and sparse retrieval, which provides a unified model foundation for real-world
IR applications. It is able to process inputs of different granularities,
spanning from short sentences to long documents of up to 8192 tokens. The
effective training of M3-Embedding involves the following technical
contributions. We propose a novel self-knowledge distillation approach, where
the relevance scores from different retrieval functionalities can be integrated
as the teacher signal to enhance the training quality. We also optimize the
batching strategy, enabling a large batch size and high training throughput to
ensure the discriminativeness of embeddings. To the best of our knowledge,
M3-Embedding is the first embedding model which realizes such a strong
versatility. The model and code will be publicly available at
https://github.com/FlagOpen/FlagEmbedding.",http://arxiv.org/abs/2402.03216v3,92,19,20,18,18,17
IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of brain MR images,"In MRI studies, the aggregation of imaging data from multiple acquisition
sites enhances sample size but may introduce site-related variabilities that
hinder consistency in subsequent analyses. Deep learning methods for image
translation have emerged as a solution for harmonizing MR images across sites.
In this study, we introduce IGUANe (Image Generation with Unified Adversarial
Networks), an original 3D model that leverages the strengths of domain
translation and straightforward application of style transfer methods for
multicenter brain MR image harmonization. IGUANe extends CycleGAN architecture
by integrating an arbitrary number of domains for training through a
many-to-one strategy. During inference, the model can be applied to any image,
even from an unknown acquisition site, making it a universal generator for
harmonization. Trained on a dataset comprising T1-weighted images from 11
different scanners, IGUANe was evaluated on data from unseen sites. The
assessments included the transformation of MR images with traveling subjects,
the preservation of pairwise distances between MR images within domains, the
evolution of volumetric patterns related to age and Alzheimer$^\prime$s disease
(AD), and the performance in age regression and patient classification tasks.
Comparisons with other harmonization and normalization methods suggest that
IGUANe better preserves individual information in MR images and is more
suitable for maintaining and reinforcing variabilities related to age and AD.
Future studies may further assess IGUANe in other multicenter contexts, either
using the same model or retraining it for applications to different image
modalities.",http://arxiv.org/abs/2402.03227v2,92,20,19,19,17,17
Can We Remove the Square-Root in Adaptive Gradient Methods? A Second-Order Perspective,"Adaptive gradient optimizers like Adam(W) are the default training algorithms
for many deep learning architectures, such as transformers. Their diagonal
preconditioner is based on the gradient outer product which is incorporated
into the parameter update via a square root. While these methods are often
motivated as approximate second-order methods, the square root represents a
fundamental difference. In this work, we investigate how the behavior of
adaptive methods changes when we remove the root, i.e. strengthen their
second-order motivation. Surprisingly, we find that such square-root-free
adaptive methods close the generalization gap to SGD on convolutional
architectures, while maintaining their root-based counterpart's performance on
transformers. The second-order perspective also has practical benefits for the
development of adaptive methods with non-diagonal preconditioner. In contrast
to root-based counterparts like Shampoo, they do not require numerically
unstable matrix square roots and therefore work well in low precision, which we
demonstrate empirically. This raises important questions regarding the
currently overlooked role of adaptivity for the success of adaptive methods
since the success is often attributed to sign descent induced by the root.",http://arxiv.org/abs/2402.03496v2,92,20,18,19,18,17
Diffusion World Model,"We introduce Diffusion World Model (DWM), a conditional diffusion model
capable of predicting multistep future states and rewards concurrently. As
opposed to traditional one-step dynamics models, DWM offers long-horizon
predictions in a single forward pass, eliminating the need for recursive
queries. We integrate DWM into model-based value estimation, where the
short-term return is simulated by future trajectories sampled from DWM. In the
context of offline reinforcement learning, DWM can be viewed as a conservative
value regularization through generative modeling. Alternatively, it can be seen
as a data source that enables offline Q-learning with synthetic data. Our
experiments on the D4RL dataset confirm the robustness of DWM to long-horizon
simulation. In terms of absolute performance, DWM significantly surpasses
one-step dynamics models with a $44\%$ performance gain, and achieves
state-of-the-art performance.",http://arxiv.org/abs/2402.03570v2,92,18,20,20,18,16
Lens: A Foundation Model for Network Traffic in Cybersecurity,"Network traffic refers to the amount of data being sent and received over the
internet or any system that connects computers. Analyzing and understanding
network traffic is vital for improving network security and management.
However, the analysis of network traffic is challenging due to the diverse
nature of data packets, which often feature heterogeneous headers and encrypted
payloads lacking semantics. To capture the latent semantics of traffic, a few
studies have adopted pre-training techniques based on the Transformer encoder
or decoder to learn the representations from massive traffic data. However,
these methods typically excel in traffic understanding (classification) or
traffic generation tasks. To address this issue, we develop Lens, a foundation
model for network traffic that leverages the T5 architecture to learn the
pre-trained representations from large-scale unlabeled data. Harnessing the
strength of the encoder-decoder framework, which captures the global
information while preserving the generative ability, our model can better learn
the representations from raw data. To further enhance pre-training
effectiveness, we design a novel loss that combines three distinct tasks:
Masked Span Prediction (MSP), Packet Order Prediction (POP), and Homologous
Traffic Prediction (HTP). Evaluation results across various benchmark datasets
demonstrate that the proposed Lens outperforms the baselines in most downstream
tasks related to both traffic understanding and generation. Notably, it also
requires much less labeled data for fine-tuning compared to current methods.",http://arxiv.org/abs/2402.03646v2,92,19,20,18,18,17
AttnLRP: Attention-Aware Layer-wise Relevance Propagation for Transformers,"Large Language Models are prone to biased predictions and hallucinations,
underlining the paramount importance of understanding their model-internal
reasoning process. However, achieving faithful attributions for the entirety of
a black-box transformer model and maintaining computational efficiency is an
unsolved challenge. By extending the Layer-wise Relevance Propagation
attribution method to handle attention layers, we address these challenges
effectively. While partial solutions exist, our method is the first to
faithfully and holistically attribute not only input but also latent
representations of transformer models with the computational efficiency similar
to a singular backward pass. Through extensive evaluations against existing
methods on Llama 2, Flan-T5 and the Vision Transformer architecture, we
demonstrate that our proposed approach surpasses alternative methods in terms
of faithfulness and enables the understanding of latent representations,
opening up the door for concept-based explanations. We provide an open-source
implementation on GitHub https://github.com/rachtibat/LRP-for-Transformers.",http://arxiv.org/abs/2402.05602v1,92,20,19,18,20,15
Deep Learning-based Computational Job Market Analysis: A Survey on Skill Extraction and Classification from Job Postings,"Recent years have brought significant advances to Natural Language Processing
(NLP), which enabled fast progress in the field of computational job market
analysis. Core tasks in this application domain are skill extraction and
classification from job postings. Because of its quick growth and its
interdisciplinary nature, there is no exhaustive assessment of this emerging
field. This survey aims to fill this gap by providing a comprehensive overview
of deep learning methodologies, datasets, and terminologies specific to
NLP-driven skill extraction and classification. Our comprehensive cataloging of
publicly available datasets addresses the lack of consolidated information on
dataset creation and characteristics. Finally, the focus on terminology
addresses the current lack of consistent definitions for important concepts,
such as hard and soft skills, and terms relating to skill extraction and
classification.",http://arxiv.org/abs/2402.05617v1,92,19,20,19,17,17
The Impact of AI Tool on Engineering at ANZ Bank An Emperical Study on GitHub Copilot within Coporate Environment,"The increasing popularity of AI, particularly Large Language Models (LLMs),
has significantly impacted various domains, including Software Engineering.
This study explores the integration of AI tools in software engineering
practices within a large organization. We focus on ANZ Bank, which employs over
5000 engineers covering all aspects of the software development life cycle.
This paper details an experiment conducted using GitHub Copilot, a notable AI
tool, within a controlled environment to evaluate its effectiveness in
real-world engineering tasks. Additionally, this paper shares initial findings
on the productivity improvements observed after GitHub Copilot was adopted on a
large scale, with about 1000 engineers using it. ANZ Bank's six-week experiment
with GitHub Copilot included two weeks of preparation and four weeks of active
testing. The study evaluated participant sentiment and the tool's impact on
productivity, code quality, and security. Initially, participants used GitHub
Copilot for proposed use-cases, with their feedback gathered through regular
surveys. In the second phase, they were divided into Control and Copilot
groups, each tackling the same Python challenges, and their experiences were
again surveyed. Results showed a notable boost in productivity and code quality
with GitHub Copilot, though its impact on code security remained inconclusive.
Participant responses were overall positive, confirming GitHub Copilot's
effectiveness in large-scale software engineering environments. Early data from
1000 engineers also indicated a significant increase in productivity and job
satisfaction.",http://arxiv.org/abs/2402.05636v1,92,18,20,20,18,16
Masked LoGoNet: Fast and Accurate 3D Image Analysis for Medical Domain,"Standard modern machine-learning-based imaging methods have faced challenges
in medical applications due to the high cost of dataset construction and,
thereby, the limited labeled training data available. Additionally, upon
deployment, these methods are usually used to process a large volume of data on
a daily basis, imposing a high maintenance cost on medical facilities. In this
paper, we introduce a new neural network architecture, termed LoGoNet, with a
tailored self-supervised learning (SSL) method to mitigate such challenges.
LoGoNet integrates a novel feature extractor within a U-shaped architecture,
leveraging Large Kernel Attention (LKA) and a dual encoding strategy to capture
both long-range and short-range feature dependencies adeptly. This is in
contrast to existing methods that rely on increasing network capacity to
enhance feature extraction. This combination of novel techniques in our model
is especially beneficial in medical image segmentation, given the difficulty of
learning intricate and often irregular body organ shapes, such as the spleen.
Complementary, we propose a novel SSL method tailored for 3D images to
compensate for the lack of large labeled datasets. The method combines masking
and contrastive learning techniques within a multi-task learning framework and
is compatible with both Vision Transformer (ViT) and CNN-based models. We
demonstrate the efficacy of our methods in numerous tasks across two standard
datasets (i.e., BTCV and MSD). Benchmark comparisons with eight
state-of-the-art models highlight LoGoNet's superior performance in both
inference time and accuracy.",http://arxiv.org/abs/2402.06190v1,92,18,20,18,18,18
CLR-Face: Conditional Latent Refinement for Blind Face Restoration Using Score-Based Diffusion Models,"Recent generative-prior-based methods have shown promising blind face
restoration performance. They usually project the degraded images to the latent
space and then decode high-quality faces either by single-stage latent
optimization or directly from the encoding. Generating fine-grained facial
details faithful to inputs remains a challenging problem. Most existing methods
produce either overly smooth outputs or alter the identity as they attempt to
balance between generation and reconstruction. This may be attributed to the
typical trade-off between quality and resolution in the latent space. If the
latent space is highly compressed, the decoded output is more robust to
degradations but shows worse fidelity. On the other hand, a more flexible
latent space can capture intricate facial details better, but is extremely
difficult to optimize for highly degraded faces using existing techniques. To
address these issues, we introduce a diffusion-based-prior inside a VQGAN
architecture that focuses on learning the distribution over uncorrupted latent
embeddings. With such knowledge, we iteratively recover the clean embedding
conditioning on the degraded counterpart. Furthermore, to ensure the reverse
diffusion trajectory does not deviate from the underlying identity, we train a
separate Identity Recovery Network and use its output to constrain the reverse
diffusion process. Specifically, using a learnable latent mask, we add
gradients from a face-recognition network to a subset of latent features that
correlates with the finer identity-related details in the pixel space, leaving
the other features untouched. Disentanglement between perception and fidelity
in the latent space allows us to achieve the best of both worlds. We perform
extensive evaluations on multiple real and synthetic datasets to validate the
superiority of our approach.",http://arxiv.org/abs/2402.06106v1,92,18,20,18,18,18
AI enhanced data assimilation and uncertainty quantification applied to Geological Carbon Storage,"This study investigates the integration of machine learning (ML) and data
assimilation (DA) techniques, focusing on implementing surrogate models for
Geological Carbon Storage (GCS) projects while maintaining high fidelity
physical results in posterior states. Initially, we evaluate the surrogate
modeling capability of two distinct machine learning models, Fourier Neural
Operators (FNOs) and Transformer UNet (T-UNet), in the context of CO$_2$
injection simulations within channelized reservoirs. We introduce the
Surrogate-based hybrid ESMDA (SH-ESMDA), an adaptation of the traditional
Ensemble Smoother with Multiple Data Assimilation (ESMDA). This method uses
FNOs and T-UNet as surrogate models and has the potential to make the standard
ESMDA process at least 50% faster or more, depending on the number of
assimilation steps. Additionally, we introduce Surrogate-based Hybrid RML
(SH-RML), a variational data assimilation approach that relies on the
randomized maximum likelihood (RML) where both the FNO and the T-UNet enable
the computation of gradients for the optimization of the objective function,
and a high-fidelity model is employed for the computation of the posterior
states. Our comparative analyses show that SH-RML offers better uncertainty
quantification compared to conventional ESMDA for the case study.",http://arxiv.org/abs/2402.06110v1,92,20,20,18,17,17
FL-NAS: Towards Fairness of NAS for Resource Constrained Devices via Large Language Models,"Neural Architecture Search (NAS) has become the de fecto tools in the
industry in automating the design of deep neural networks for various
applications, especially those driven by mobile and edge devices with limited
computing resources. The emerging large language models (LLMs), due to their
prowess, have also been incorporated into NAS recently and show some promising
results. This paper conducts further exploration in this direction by
considering three important design metrics simultaneously, i.e., model
accuracy, fairness, and hardware deployment efficiency. We propose a novel
LLM-based NAS framework, FL-NAS, in this paper, and show experimentally that
FL-NAS can indeed find high-performing DNNs, beating state-of-the-art DNN
models by orders-of-magnitude across almost all design considerations.",http://arxiv.org/abs/2402.06696v1,92,18,20,20,16,18
Iterated Denoising Energy Matching for Sampling from Boltzmann Densities,"Efficiently generating statistically independent samples from an unnormalized
probability distribution, such as equilibrium samples of many-body systems, is
a foundational problem in science. In this paper, we propose Iterated Denoising
Energy Matching (iDEM), an iterative algorithm that uses a novel stochastic
score matching objective leveraging solely the energy function and its gradient
-- and no data samples -- to train a diffusion-based sampler. Specifically,
iDEM alternates between (I) sampling regions of high model density from a
diffusion-based sampler and (II) using these samples in our stochastic matching
objective to further improve the sampler. iDEM is scalable to high dimensions
as the inner matching objective, is simulation-free, and requires no MCMC
samples. Moreover, by leveraging the fast mode mixing behavior of diffusion,
iDEM smooths out the energy landscape enabling efficient exploration and
learning of an amortized sampler. We evaluate iDEM on a suite of tasks ranging
from standard synthetic energy functions to invariant $n$-body particle
systems. We show that the proposed approach achieves state-of-the-art
performance on all metrics and trains $2-5\times$ faster, which allows it to be
the first method to train using energy on the challenging $55$-particle
Lennard-Jones system.",http://arxiv.org/abs/2402.06121v1,92,20,18,18,18,18
CityFlowER: An Efficient and Realistic Traffic Simulator with Embedded Machine Learning Models,"Traffic simulation is an essential tool for transportation infrastructure
planning, intelligent traffic control policy learning, and traffic flow
analysis. Its effectiveness relies heavily on the realism of the simulators
used. Traditional traffic simulators, such as SUMO and CityFlow, are often
limited by their reliance on rule-based models with hyperparameters that
oversimplify driving behaviors, resulting in unrealistic simulations. To
enhance realism, some simulators have provided Application Programming
Interfaces (APIs) to interact with Machine Learning (ML) models, which learn
from observed data and offer more sophisticated driving behavior models.
However, this approach faces challenges in scalability and time efficiency as
vehicle numbers increase. Addressing these limitations, we introduce
CityFlowER, an advancement over the existing CityFlow simulator, designed for
efficient and realistic city-wide traffic simulation. CityFlowER innovatively
pre-embeds ML models within the simulator, eliminating the need for external
API interactions and enabling faster data computation. This approach allows for
a blend of rule-based and ML behavior models for individual vehicles, offering
unparalleled flexibility and efficiency, particularly in large-scale
simulations. We provide detailed comparisons with existing simulators,
implementation insights, and comprehensive experiments to demonstrate
CityFlowER's superiority in terms of realism, efficiency, and adaptability.",http://arxiv.org/abs/2402.06127v1,92,20,20,18,16,18
Rethinking Node-wise Propagation for Large-scale Graph Learning,"Scalable graph neural networks (GNNs) have emerged as a promising technique,
which exhibits superior predictive performance and high running efficiency
across numerous large-scale graph-based web applications. However, (i) Most
scalable GNNs tend to treat all nodes in graphs with the same propagation
rules, neglecting their topological uniqueness; (ii) Existing node-wise
propagation optimization strategies are insufficient on web-scale graphs with
intricate topology, where a full portrayal of nodes' local properties is
required. Intuitively, different nodes in web-scale graphs possess distinct
topological roles, and therefore propagating them indiscriminately or neglect
local contexts may compromise the quality of node representations. This
intricate topology in web-scale graphs cannot be matched by small-scale
scenarios. To address the above issues, we propose \textbf{A}daptive
\textbf{T}opology-aware \textbf{P}ropagation (ATP), which reduces potential
high-bias propagation and extracts structural patterns of each node in a
scalable manner to improve running efficiency and predictive performance.
Remarkably, ATP is crafted to be a plug-and-play node-wise propagation
optimization strategy, allowing for offline execution independent of the graph
learning process in a new perspective. Therefore, this approach can be
seamlessly integrated into most scalable GNNs while remain orthogonal to
existing node-wise propagation optimization strategies. Extensive experiments
on 12 datasets, including the most representative large-scale ogbn-papers100M,
have demonstrated the effectiveness of ATP. Specifically, ATP has proven to be
efficient in improving the performance of prevalent scalable GNNs for
semi-supervised node classification while addressing redundant computational
costs.",http://arxiv.org/abs/2402.06128v1,92,20,18,20,18,16
SIR: Multi-view Inverse Rendering with Decomposable Shadow for Indoor Scenes,"We propose SIR, an efficient method to decompose differentiable shadows for
inverse rendering on indoor scenes using multi-view data, addressing the
challenges in accurately decomposing the materials and lighting conditions.
Unlike previous methods that struggle with shadow fidelity in complex lighting
environments, our approach explicitly learns shadows for enhanced realism in
material estimation under unknown light positions. Utilizing posed HDR images
as input, SIR employs an SDF-based neural radiance field for comprehensive
scene representation. Then, SIR integrates a shadow term with a three-stage
material estimation approach to improve SVBRDF quality. Specifically, SIR is
designed to learn a differentiable shadow, complemented by BRDF regularization,
to optimize inverse rendering accuracy. Extensive experiments on both synthetic
and real-world indoor scenes demonstrate the superior performance of SIR over
existing methods in both quantitative metrics and qualitative analysis. The
significant decomposing ability of SIR enables sophisticated editing
capabilities like free-view relighting, object insertion, and material
replacement.",http://arxiv.org/abs/2402.06136v1,92,19,20,18,18,17
Improved Evidential Deep Learning via a Mixture of Dirichlet Distributions,"This paper explores a modern predictive uncertainty estimation approach,
called evidential deep learning (EDL), in which a single neural network model
is trained to learn a meta distribution over the predictive distribution by
minimizing a specific objective function. Despite their strong empirical
performance, recent studies by Bengs et al. identify a fundamental pitfall of
the existing methods: the learned epistemic uncertainty may not vanish even in
the infinite-sample limit. We corroborate the observation by providing a
unifying view of a class of widely used objectives from the literature. Our
analysis reveals that the EDL methods essentially train a meta distribution by
minimizing a certain divergence measure between the distribution and a
sample-size-independent target distribution, resulting in spurious epistemic
uncertainty. Grounded in theoretical principles, we propose learning a
consistent target distribution by modeling it with a mixture of Dirichlet
distributions and learning via variational inference. Afterward, a final meta
distribution model distills the learned uncertainty from the target model.
Experimental results across various uncertainty-based downstream tasks
demonstrate the superiority of our proposed method, and illustrate the
practical implications arising from the consistency and inconsistency of
learned epistemic uncertainty.",http://arxiv.org/abs/2402.06160v1,92,19,19,19,18,17
Pushing Boundaries: Mixup's Influence on Neural Collapse,"Mixup is a data augmentation strategy that employs convex combinations of
training instances and their respective labels to augment the robustness and
calibration of deep neural networks. Despite its widespread adoption, the
nuanced mechanisms that underpin its success are not entirely understood. The
observed phenomenon of Neural Collapse, where the last-layer activations and
classifier of deep networks converge to a simplex equiangular tight frame
(ETF), provides a compelling motivation to explore whether mixup induces
alternative geometric configurations and whether those could explain its
success. In this study, we delve into the last-layer activations of training
data for deep networks subjected to mixup, aiming to uncover insights into its
operational efficacy. Our investigation, spanning various architectures and
dataset pairs, reveals that mixup's last-layer activations predominantly
converge to a distinctive configuration different than one might expect. In
this configuration, activations from mixed-up examples of identical classes
align with the classifier, while those from different classes delineate
channels along the decision boundary. Moreover, activations in earlier layers
exhibit patterns, as if trained with manifold mixup. These findings are
unexpected, as mixed-up features are not simple convex combinations of feature
class means (as one might get, for example, by training mixup with the mean
squared error loss). By analyzing this distinctive geometric configuration, we
elucidate the mechanisms by which mixup enhances model calibration. To further
validate our empirical observations, we conduct a theoretical analysis under
the assumption of an unconstrained features model, utilizing the mixup loss.
Through this, we characterize and derive the optimal last-layer features under
the assumption that the classifier forms a simplex ETF.",http://arxiv.org/abs/2402.06171v1,92,19,20,19,17,17
Premier-TACO is a Few-Shot Policy Learner: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss,"We present Premier-TACO, a multitask feature representation learning approach
designed to improve few-shot policy learning efficiency in sequential
decision-making tasks. Premier-TACO leverages a subset of multitask offline
datasets for pretraining a general feature representation, which captures
critical environmental dynamics and is fine-tuned using minimal expert
demonstrations. It advances the temporal action contrastive learning (TACO)
objective, known for state-of-the-art results in visual control tasks, by
incorporating a novel negative example sampling strategy. This strategy is
crucial in significantly boosting TACO's computational efficiency, making
large-scale multitask offline pretraining feasible. Our extensive empirical
evaluation in a diverse set of continuous control benchmarks including Deepmind
Control Suite, MetaWorld, and LIBERO demonstrate Premier-TACO's effectiveness
in pretraining visual representations, significantly enhancing few-shot
imitation learning of novel tasks. Our code, pretraining data, as well as
pretrained model checkpoints will be released at
https://github.com/PremierTACO/premier-taco.",http://arxiv.org/abs/2402.06187v2,92,20,18,20,18,16
A self-supervised framework for learning whole slide representations,"Whole slide imaging is fundamental to biomedical microscopy and computational
pathology. However, whole slide images (WSIs) present a complex computer vision
challenge due to their gigapixel size, diverse histopathologic features,
spatial heterogeneity, and limited/absent data annotations. These challenges
highlight that supervised training alone can result in suboptimal whole slide
representations. Self-supervised representation learning can achieve
high-quality WSI visual feature learning for downstream diagnostic tasks, such
as cancer diagnosis or molecular genetic prediction. Here, we present a general
self-supervised whole slide learning (S3L) framework for gigapixel-scale
self-supervision of WSIs. S3L combines data transformation strategies from
transformer-based vision and language modeling into a single unified framework
to generate paired views for self-supervision. S3L leverages the inherent
regional heterogeneity, histologic feature variability, and information
redundancy within WSIs to learn high-quality whole-slide representations. We
benchmark S3L visual representations on two diagnostic tasks for two biomedical
microscopy modalities. S3L significantly outperforms WSI baselines for cancer
diagnosis and genetic mutation prediction. Additionally, S3L achieves good
performance using both in-domain and out-of-distribution patch encoders,
demonstrating good flexibility and generalizability.",http://arxiv.org/abs/2402.06188v1,92,19,20,18,17,18
The Berkeley Single Cell Computational Microscopy (BSCCM) Dataset,"Computational microscopy, in which hardware and algorithms of an imaging
system are jointly designed, shows promise for making imaging systems that cost
less, perform more robustly, and collect new types of information. Often, the
performance of computational imaging systems, especially those that incorporate
machine learning, is sample-dependent. Thus, standardized datasets are an
essential tool for comparing the performance of different approaches. Here, we
introduce the Berkeley Single Cell Computational Microscopy (BSCCM) dataset,
which contains over ~12,000,000 images of 400,000 of individual white blood
cells. The dataset contains images captured with multiple illumination patterns
on an LED array microscope and fluorescent measurements of the abundance of
surface proteins that mark different cell types. We hope this dataset will
provide a valuable resource for the development and testing of new algorithms
in computational microscopy and computer vision with practical biomedical
applications.",http://arxiv.org/abs/2402.06191v1,92,18,20,20,18,16
Descriptive Kernel Convolution Network with Improved Random Walk Kernel,"Graph kernels used to be the dominant approach to feature engineering for
structured data, which are superseded by modern GNNs as the former lacks
learnability. Recently, a suite of Kernel Convolution Networks (KCNs)
successfully revitalized graph kernels by introducing learnability, which
convolves input with learnable hidden graphs using a certain graph kernel. The
random walk kernel (RWK) has been used as the default kernel in many KCNs,
gaining increasing attention. In this paper, we first revisit the RWK and its
current usage in KCNs, revealing several shortcomings of the existing designs,
and propose an improved graph kernel RWK+, by introducing color-matching random
walks and deriving its efficient computation. We then propose RWK+CN, a KCN
that uses RWK+ as the core kernel to learn descriptive graph features with an
unsupervised objective, which can not be achieved by GNNs. Further, by
unrolling RWK+, we discover its connection with a regular GCN layer, and
propose a novel GNN layer RWK+Conv. In the first part of experiments, we
demonstrate the descriptive learning ability of RWK+CN with the improved random
walk kernel RWK+ on unsupervised pattern mining tasks; in the second part, we
show the effectiveness of RWK+ for a variety of KCN architectures and
supervised graph learning tasks, and demonstrate the expressiveness of RWK+Conv
layer, especially on the graph-level tasks. RWK+ and RWK+Conv adapt to various
real-world applications, including web applications such as bot detection in a
web-scale Twitter social network, and community classification in Reddit social
interaction networks.",http://arxiv.org/abs/2402.06087v1,92,18,18,20,18,18
Anubis: Towards Reliable Cloud AI Infrastructure via Proactive Validation,"Reliability in cloud AI infrastructure is crucial for cloud service
providers, prompting the widespread use of hardware redundancies. However,
these redundancies can inadvertently lead to hidden degradation, so called
""gray failure"", for AI workloads, significantly affecting end-to-end
performance and concealing performance issues, which complicates root cause
analysis for failures and regressions.
  We introduce Anubis, a proactive validation system for AI infrastructure that
mitigates hidden degradation caused by hardware redundancies and enhances
overall reliability. Anubis features a comprehensive benchmark suite, capable
of evaluating individual hardware components and representing most real AI
workloads. It comprises a Validator which learns benchmark criteria to clearly
pinpoint defective components. Additionally, Anubis incorporates a Selector to
balance validation time and issue-related penalties, enabling optimal timing
for validation execution with a tailored subset of benchmarks. Through testbed
evaluation and simulation, we demonstrate that Anubis can increase the mean
time between incidents by up to 22.61x. Anubis has been successfully deployed
in Azure production, validating hundreds of thousands of GPUs over the last two
years.",http://arxiv.org/abs/2402.06194v1,92,20,18,20,18,16
A Unified Causal View of Instruction Tuning,"Instruction tuning on a mixture of tasks has improved zero-shot capabilities
in natural language processing (NLP). Nevertheless, existing methods often
learn features that exhibit correlations between instruction-formatted samples
and target labels, rather than causal relationships. Termed as ``spurious
correlation'' in statistics, such a correlation may change drastically in a new
task, making the effect from the learned features to be misleading. To this
end, we develop a meta Structural Causal Model (meta-SCM) to integrate
different NLP tasks under a single causal structure of the data. Specifically,
the meta-SCM introduces multiple latent factors that represent properties of
source context, only some of which causally influence the target labels for a
specific task. The key idea is to learn task-required causal factors and only
use those to make predictions for a given task. Theoretically, we prove the
causal factor can be identified without mixing information from others. Guided
by the identifiability, we propose a Structural Instruction Tuning (SIT) method
to learn the task-required causal representations that can mimic the causal
factors for each task. The utility of our approach is verified by improvements
of zero-shot ability on a range of unseen datasets and tasks.",http://arxiv.org/abs/2402.06220v1,92,20,18,18,18,18
Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping,"Self-alignment is an effective way to reduce the cost of human annotation
while ensuring promising model capability. However, most current methods
complete the data collection and training steps in a single round, which may
overlook the continuously improving ability of self-aligned models. This gives
rise to a key query: What if we do multi-time bootstrapping self-alignment?
Does this strategy enhance model performance or lead to rapid degradation? In
this paper, our pioneering exploration delves into the impact of bootstrapping
self-alignment on large language models. Our findings reveal that bootstrapping
self-alignment markedly surpasses the single-round approach, by guaranteeing
data diversity from in-context learning. To further exploit the capabilities of
bootstrapping, we investigate and adjust the training order of data, which
yields improved performance of the model. Drawing on these findings, we propose
Step-On-Feet Tuning (SOFT) which leverages model's continuously enhanced
few-shot ability to boost zero or one-shot performance. Based on easy-to-hard
training recipe, we propose SOFT+ which further boost self-alignment's
performance. Our experiments demonstrate the efficiency of SOFT (SOFT+) across
various classification and generation tasks, highlighting the potential of
bootstrapping self-alignment on continually enhancing model alignment
performance.",http://arxiv.org/abs/2402.07610v1,92,19,18,19,18,18
Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning,"Although Large Language Models (LLMs) have achieved tremendous success in
various applications, they are also susceptible to certain prompts that can
induce them to bypass built-in safety measures and provide dangerous or illegal
content, a phenomenon known as jailbreak. To protect LLMs from producing
harmful information, various defense strategies are proposed, with most
focusing on content filtering or adversarial training of models. In this paper,
we propose an approach named Prompt Adversarial Tuning (PAT) to train a defense
control mechanism, which is then embedded as a prefix to user prompts to
implement our defense strategy. We design a training process similar to
adversarial training to achieve our optimized goal, alternating between
updating attack and defense controls. To our knowledge, we are the first to
implement defense from the perspective of prompt tuning. Once employed, our
method will hardly impact the operational efficiency of LLMs. Experiments show
that our method is effective in both black-box and white-box settings, reducing
the success rate of advanced attacks to nearly 0 while maintaining the benign
answer rate of 80% to simple benign questions. Our work might potentially chart
a new perspective for future explorations in LLM security.",http://arxiv.org/abs/2402.06255v1,92,18,19,19,18,18
Multimodal Interpretable Data-Driven Models for Early Prediction of Antimicrobial Multidrug Resistance Using Multivariate Time-Series,"Electronic health records (EHR) is an inherently multimodal register of the
patient's health status characterized by static data and multivariate time
series (MTS). While MTS are a valuable tool for clinical prediction, their
fusion with other data modalities can possibly result in more thorough insights
and more accurate results. Deep neural networks (DNNs) have emerged as
fundamental tools for identifying and defining underlying patterns in the
healthcare domain. However, fundamental improvements in interpretability are
needed for DNN models to be widely used in the clinical setting. In this study,
we present an approach built on a collection of interpretable multimodal
data-driven models that may anticipate and understand the emergence of
antimicrobial multidrug resistance (AMR) germs in the intensive care unit (ICU)
of the University Hospital of Fuenlabrada (Madrid, Spain). The profile and
initial health status of the patient are modeled using static variables, while
the evolution of the patient's health status during the ICU stay is modeled
using several MTS, including mechanical ventilation and antibiotics intake. The
multimodal DNNs models proposed in this paper include interpretable principles
in addition to being effective at predicting AMR and providing an explainable
prediction support system for AMR in the ICU. Furthermore, our proposed
methodology based on multimodal models and interpretability schemes can be
leveraged in additional clinical problems dealing with EHR data, broadening the
impact and applicability of our results.",http://arxiv.org/abs/2402.06295v1,92,19,18,19,18,18
A Functional Analysis Approach to Symbolic Regression,"Symbolic regression (SR) poses a significant challenge for randomized search
heuristics due to its reliance on the synthesis of expressions for input-output
mappings. Although traditional genetic programming (GP) algorithms have
achieved success in various domains, they exhibit limited performance when
tree-based representations are used for SR. To address these limitations, we
introduce a novel SR approach called Fourier Tree Growing (FTG) that draws
insights from functional analysis. This new perspective enables us to perform
optimization directly in a different space, thus avoiding intricate symbolic
expressions. Our proposed algorithm exhibits significant performance
improvements over traditional GP methods on a range of classical
one-dimensional benchmarking problems. To identify and explain limiting factors
of GP and FTG, we perform experiments on a large-scale polynomials benchmark
with high-order polynomials up to degree 100. To the best of the authors'
knowledge, this work represents the pioneering application of functional
analysis in addressing SR problems. The superior performance of the proposed
algorithm and insights into the limitations of GP open the way for further
advancing GP for SR and related areas of explainable machine learning.",http://arxiv.org/abs/2402.06299v1,92,20,20,18,18,16
Multisource Semisupervised Adversarial Domain Generalization Network for Cross-Scene Sea\textendash Land Clutter Classification,"Deep learning (DL)-based sea\textendash land clutter classification for
sky-wave over-the-horizon-radar (OTHR) has become a novel research topic. In
engineering applications, real-time predictions of sea\textendash land clutter
with existing distribution discrepancies are crucial. To solve this problem,
this article proposes a novel Multisource Semisupervised Adversarial Domain
Generalization Network (MSADGN) for cross-scene sea\textendash land clutter
classification. MSADGN can extract domain-invariant and domain-specific
features from one labeled source domain and multiple unlabeled source domains,
and then generalize these features to an arbitrary unseen target domain for
real-time prediction of sea\textendash land clutter. Specifically, MSADGN
consists of three modules: domain-related pseudolabeling module,
domain-invariant module, and domain-specific module. The first module
introduces an improved pseudolabel method called domain-related pseudolabel,
which is designed to generate reliable pseudolabels to fully exploit unlabeled
source domains. The second module utilizes a generative adversarial network
(GAN) with a multidiscriminator to extract domain-invariant features, to
enhance the model's transferability in the target domain. The third module
employs a parallel multiclassifier branch to extract domain-specific features,
to enhance the model's discriminability in the target domain. The effectiveness
of our method is validated in twelve domain generalizations (DG) scenarios.
Meanwhile, we selected 10 state-of-the-art DG methods for comparison. The
experimental results demonstrate the superiority of our method.",http://arxiv.org/abs/2402.06315v1,92,19,18,19,18,18
Multi-class real-time crash risk forecasting using convolutional neural network: Istanbul case study,"The performance of an artificial neural network (ANN) in forecasting crash
risk is shown in this paper. To begin, some traffic and weather data are
acquired as raw data. This data is then analyzed, and relevant characteristics
are chosen to utilize as input data based on additional tree and Pearson
correlation. Furthermore, crash and non-crash time data are separated; then,
feature values for crash and non-crash events are written in three four-minute
intervals prior to the crash and non-crash events using the average of all
available values for that period. The number of non-crash samples was lowered
after calculating crash likelihood for each period based on accident labeling.
The proposed CNN model is capable of learning from recorded, processed, and
categorized input characteristics such as traffic characteristics and
meteorological conditions. The goal of this work is to forecast the chance of a
real-time crash based on three periods before events. The area under the curve
(AUC) for the receiver operating characteristic curve (ROC curve), as well as
sensitivity as the true positive rate and specificity as the false positive
rate, are shown and compared with three typical machine learning and neural
network models. Finally, when it comes to the error value, AUC, sensitivity,
and specificity parameters as performance variables, the executed model
outperforms other models. The findings of this research suggest applying the
CNN model as a multi-class prediction model for real-time crash risk
prediction. Our emphasis is on multi-class prediction, while prior research
used this for binary (two-class) categorization like crash and non-crash.",http://arxiv.org/abs/2402.06707v1,92,18,20,20,18,16
Prompt Learning on Temporal Interaction Graphs,"Temporal Interaction Graphs (TIGs) are widely utilized to represent
real-world systems. To facilitate representation learning on TIGs, researchers
have proposed a series of TIG models. However, these models are still facing
two tough gaps between the pre-training and downstream predictions in their
``pre-train, predict'' training paradigm. First, the temporal discrepancy
between the pre-training and inference data severely undermines the models'
applicability in distant future predictions on the dynamically evolving data.
Second, the semantic divergence between pretext and downstream tasks hinders
their practical applications, as they struggle to align with their learning and
prediction capabilities across application scenarios.
  Recently, the ``pre-train, prompt'' paradigm has emerged as a lightweight
mechanism for model generalization. Applying this paradigm is a potential
solution to solve the aforementioned challenges. However, the adaptation of
this paradigm to TIGs is not straightforward. The application of prompting in
static graph contexts falls short in temporal settings due to a lack of
consideration for time-sensitive dynamics and a deficiency in expressive power.
To address this issue, we introduce Temporal Interaction Graph Prompting
(TIGPrompt), a versatile framework that seamlessly integrates with TIG models,
bridging both the temporal and semantic gaps. In detail, we propose a temporal
prompt generator to offer temporally-aware prompts for different tasks. These
prompts stand out for their minimalistic design, relying solely on the tuning
of the prompt generator with very little supervision data. To cater to varying
computational resource demands, we propose an extended ``pre-train,
prompt-based fine-tune'' paradigm, offering greater flexibility. Through
extensive experiments, the TIGPrompt demonstrates the SOTA performance and
remarkable efficiency advantages.",http://arxiv.org/abs/2402.06326v1,92,18,18,20,18,18
High-Precision Geosteering via Reinforcement Learning and Particle Filters,"Geosteering, a key component of drilling operations, traditionally involves
manual interpretation of various data sources such as well-log data. This
introduces subjective biases and inconsistent procedures. Academic attempts to
solve geosteering decision optimization with greedy optimization and
Approximate Dynamic Programming (ADP) showed promise but lacked adaptivity to
realistic diverse scenarios. Reinforcement learning (RL) offers a solution to
these challenges, facilitating optimal decision-making through reward-based
iterative learning. State estimation methods, e.g., particle filter (PF),
provide a complementary strategy for geosteering decision-making based on
online information. We integrate an RL-based geosteering with PF to address
realistic geosteering scenarios. Our framework deploys PF to process real-time
well-log data to estimate the location of the well relative to the
stratigraphic layers, which then informs the RL-based decision-making process.
We compare our method's performance with that of using solely either RL or PF.
Our findings indicate a synergy between RL and PF in yielding optimized
geosteering decisions.",http://arxiv.org/abs/2402.06377v1,92,18,19,19,18,18
Optimal estimation of Gaussian (poly)trees,"We develop optimal algorithms for learning undirected Gaussian trees and
directed Gaussian polytrees from data. We consider both problems of
distribution learning (i.e. in KL distance) and structure learning (i.e. exact
recovery). The first approach is based on the Chow-Liu algorithm, and learns an
optimal tree-structured distribution efficiently. The second approach is a
modification of the PC algorithm for polytrees that uses partial correlation as
a conditional independence tester for constraint-based structure learning. We
derive explicit finite-sample guarantees for both approaches, and show that
both approaches are optimal by deriving matching lower bounds. Additionally, we
conduct numerical experiments to compare the performance of various algorithms,
providing further insights and empirical evidence.",http://arxiv.org/abs/2402.06380v1,92,20,18,20,18,16
TWIG: Towards pre-hoc Hyperparameter Optimisation and Cross-Graph Generalisation via Simulated KGE Models,"In this paper we introduce TWIG (Topologically-Weighted Intelligence
Generation), a novel, embedding-free paradigm for simulating the output of KGEs
that uses a tiny fraction of the parameters. TWIG learns weights from inputs
that consist of topological features of the graph data, with no coding for
latent representations of entities or edges. Our experiments on the UMLS
dataset show that a single TWIG neural network can predict the results of
state-of-the-art ComplEx-N3 KGE model nearly exactly on across all
hyperparameter configurations. To do this it uses a total of 2590 learnable
parameters, but accurately predicts the results of 1215 different
hyperparameter combinations with a combined cost of 29,322,000 parameters.
Based on these results, we make two claims: 1) that KGEs do not learn latent
semantics, but only latent representations of structural patterns; 2) that
hyperparameter choice in KGEs is a deterministic function of the KGE model and
graph structure. We further hypothesise that, as TWIG can simulate KGEs without
embeddings, that node and edge embeddings are not needed to learn to accurately
predict new facts in KGs. Finally, we formulate all of our findings under the
umbrella of the ``Structural Generalisation Hypothesis"", which suggests that
``twiggy"" embedding-free / data-structure-based learning methods can allow a
single neural network to simulate KGE performance, and perhaps solve the Link
Prediction task, across many KGs from diverse domains and with different
semantics.",http://arxiv.org/abs/2402.06097v1,92,18,20,20,18,16
SubGen: Token Generation in Sublinear Time and Memory,"Despite the significant success of large language models (LLMs), their
extensive memory requirements pose challenges for deploying them in
long-context token generation. The substantial memory footprint of LLM decoders
arises from the necessity to store all previous tokens in the attention module,
a requirement imposed by key-value (KV) caching. In this work, our focus is on
developing an efficient compression technique for the KV cache. Empirical
evidence indicates a significant clustering tendency within key embeddings in
the attention module. Building on this key insight, we have devised a novel
caching method with sublinear complexity, employing online clustering on key
tokens and online $\ell_2$ sampling on values. The result is a provably
accurate and efficient attention decoding algorithm, termed SubGen. Not only
does this algorithm ensure a sublinear memory footprint and sublinear time
complexity, but we also establish a tight error bound for our approach.
Empirical evaluations on long-context question-answering tasks demonstrate that
SubGen significantly outperforms existing and state-of-the-art KV cache
compression methods in terms of performance and efficiency.",http://arxiv.org/abs/2402.06082v1,92,20,20,18,18,16
Improving Token-Based World Models with Parallel Observation Prediction,"Motivated by the success of Transformers when applied to sequences of
discrete symbols, token-based world models (TBWMs) were recently proposed as
sample-efficient methods. In TBWMs, the world model consumes agent experience
as a language-like sequence of tokens, where each observation constitutes a
sub-sequence. However, during imagination, the sequential token-by-token
generation of next observations results in a severe bottleneck, leading to long
training times, poor GPU utilization, and limited representations. To resolve
this bottleneck, we devise a novel Parallel Observation Prediction (POP)
mechanism. POP augments a Retentive Network (RetNet) with a novel forward mode
tailored to our reinforcement learning setting. We incorporate POP in a novel
TBWM agent named REM (Retentive Environment Model), showcasing a 15.4x faster
imagination compared to prior TBWMs. REM attains superhuman performance on 12
out of 26 games of the Atari 100K benchmark, while training in less than 12
hours. Our code is available at \url{https://github.com/leor-c/REM}.",http://arxiv.org/abs/2402.05643v2,92,20,18,20,18,16
Guided Evolution with Binary Discriminators for ML Program Search,"How to automatically design better machine learning programs is an open
problem within AutoML. While evolution has been a popular tool to search for
better ML programs, using learning itself to guide the search has been less
successful and less understood on harder problems but has the promise to
dramatically increase the speed and final performance of the optimization
process. We propose guiding evolution with a binary discriminator, trained
online to distinguish which program is better given a pair of programs. The
discriminator selects better programs without having to perform a costly
evaluation and thus speed up the convergence of evolution. Our method can
encode a wide variety of ML components including symbolic optimizers, neural
architectures, RL loss functions, and symbolic regression equations with the
same directed acyclic graph representation. By combining this representation
with modern GNNs and an adaptive mutation strategy, we demonstrate our method
can speed up evolution across a set of diverse problems including a 3.7x
speedup on the symbolic search for ML optimizers and a 4x speedup for RL loss
functions.",http://arxiv.org/abs/2402.05821v1,92,20,19,19,18,16
FuncGrasp: Learning Object-Centric Neural Grasp Functions from Single Annotated Example Object,"We present FuncGrasp, a framework that can infer dense yet reliable grasp
configurations for unseen objects using one annotated object and single-view
RGB-D observation via categorical priors. Unlike previous works that only
transfer a set of grasp poses, FuncGrasp aims to transfer infinite
configurations parameterized by an object-centric continuous grasp function
across varying instances. To ease the transfer process, we propose Neural
Surface Grasping Fields (NSGF), an effective neural representation defined on
the surface to densely encode grasp configurations. Further, we exploit
function-to-function transfer using sphere primitives to establish semantically
meaningful categorical correspondences, which are learned in an unsupervised
fashion without any expert knowledge. We showcase the effectiveness through
extensive experiments in both simulators and the real world. Remarkably, our
framework significantly outperforms several strong baseline methods in terms of
density and reliability for generated grasps.",http://arxiv.org/abs/2402.05644v1,92,19,18,19,18,18
Real-time Holistic Robot Pose Estimation with Unknown States,"Estimating robot pose from RGB images is a crucial problem in computer vision
and robotics. While previous methods have achieved promising performance, most
of them presume full knowledge of robot internal states, e.g. ground-truth
robot joint angles, which are not always available in real-world scenarios. On
the other hand, existing approaches that estimate robot pose without joint
state priors suffer from heavy computation burdens and thus cannot support
real-time applications. This work addresses the urgent need for efficient robot
pose estimation with unknown states. We propose an end-to-end pipeline for
real-time, holistic robot pose estimation from a single RGB image, even in the
absence of known robot states. Our method decomposes the problem into
estimating camera-to-robot rotation, robot state parameters, keypoint
locations, and root depth. We further design a corresponding neural network
module for each task. This approach allows for learning multi-facet
representations and facilitates sim-to-real transfer through self-supervised
learning. Notably, our method achieves inference with a single feedforward,
eliminating the need for costly test-time iterative optimization. As a result,
it delivers a 12-time speed boost with state-of-the-art accuracy, enabling
real-time holistic robot pose estimation for the first time. Code is available
at https://oliverbansk.github.io/Holistic-Robot-Pose/.",http://arxiv.org/abs/2402.05655v1,92,20,18,20,18,16
Offline Risk-sensitive RL with Partial Observability to Enhance Performance in Human-Robot Teaming,"The integration of physiological computing into mixed-initiative human-robot
interaction systems offers valuable advantages in autonomous task allocation by
incorporating real-time features as human state observations into the
decision-making system. This approach may alleviate the cognitive load on human
operators by intelligently allocating mission tasks between agents.
Nevertheless, accommodating a diverse pool of human participants with varying
physiological and behavioral measurements presents a substantial challenge. To
address this, resorting to a probabilistic framework becomes necessary, given
the inherent uncertainty and partial observability on the human's state. Recent
research suggests to learn a Partially Observable Markov Decision Process
(POMDP) model from a data set of previously collected experiences that can be
solved using Offline Reinforcement Learning (ORL) methods. In the present work,
we not only highlight the potential of partially observable representations and
physiological measurements to improve human operator state estimation and
performance, but also enhance the overall mission effectiveness of a
human-robot team. Importantly, as the fixed data set may not contain enough
information to fully represent complex stochastic processes, we propose a
method to incorporate model uncertainty, thus enabling risk-sensitive
sequential decision-making. Experiments were conducted with a group of
twenty-six human participants within a simulated robot teleoperation
environment, yielding empirical evidence of the method's efficacy. The obtained
adaptive task allocation policy led to statistically significant higher scores
than the one that was used to collect the data set, allowing for generalization
across diverse participants also taking into account risk-sensitive metrics.",http://arxiv.org/abs/2402.05703v1,92,18,18,20,18,18
Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on Vulnerable Patient Populations,"The proliferation of artificial intelligence (AI) in radiology has shed light
on the risk of deep learning (DL) models exacerbating clinical biases towards
vulnerable patient populations. While prior literature has focused on
quantifying biases exhibited by trained DL models, demographically targeted
adversarial bias attacks on DL models and its implication in the clinical
environment remains an underexplored field of research in medical imaging. In
this work, we demonstrate that demographically targeted label poisoning attacks
can introduce adversarial underdiagnosis bias in DL models and degrade
performance on underrepresented groups without impacting overall model
performance. Moreover, our results across multiple performance metrics and
demographic groups like sex, age, and their intersectional subgroups indicate
that a group's vulnerability to undetectable adversarial bias attacks is
directly correlated with its representation in the model's training data.",http://arxiv.org/abs/2402.05713v1,92,19,19,18,18,18
A Framework for Assessing Proportionate Intervention with Face Recognition Systems in Real-Life Scenarios,"Face recognition (FR) has reached a high technical maturity. However, its use
needs to be carefully assessed from an ethical perspective, especially in
sensitive scenarios. This is precisely the focus of this paper: the use of FR
for the identification of specific subjects in moderately to densely crowded
spaces (e.g. public spaces, sports stadiums, train stations) and law
enforcement scenarios. In particular, there is a need to consider the trade-off
between the need to protect privacy and fundamental rights of citizens as well
as their safety. Recent Artificial Intelligence (AI) policies, notably the
European AI Act, propose that such FR interventions should be proportionate and
deployed only when strictly necessary. Nevertheless, concrete guidelines on how
to address the concept of proportional FR intervention are lacking to date.
This paper proposes a framework to contribute to assessing whether an FR
intervention is proportionate or not for a given context of use in the above
mentioned scenarios. It also identifies the main quantitative and qualitative
variables relevant to the FR intervention decision (e.g. number of people in
the scene, level of harm that the person(s) in search could perpetrate,
consequences to individual rights and freedoms) and propose a 2D graphical
model making it possible to balance these variables in terms of ethical cost vs
security gain. Finally, different FR scenarios inspired by real-world
deployments validate the proposed model. The framework is conceived as a simple
support tool for decision makers when confronted with the deployment of an FR
system.",http://arxiv.org/abs/2402.05731v1,92,18,20,18,18,18
Implicit Bias and Fast Convergence Rates for Self-attention,"Self-attention, the core mechanism of transformers, distinguishes them from
traditional neural networks and drives their outstanding performance. Towards
developing the fundamental optimization principles of self-attention, we
investigate the implicit bias of gradient descent (GD) in training a
self-attention layer with fixed linear decoder in binary classification.
Drawing inspiration from the study of GD in linear logistic regression over
separable data, recent work demonstrates that as the number of iterations $t$
approaches infinity, the key-query matrix $W_t$ converges locally (with respect
to the initialization direction) to a hard-margin SVM solution $W_{mm}$. Our
work enhances this result in four aspects. Firstly, we identify non-trivial
data settings for which convergence is provably global, thus shedding light on
the optimization landscape. Secondly, we provide the first finite-time
convergence rate for $W_t$ to $W_{mm}$, along with quantifying the rate of
sparsification in the attention map. Thirdly, through an analysis of normalized
GD and Polyak step-size, we demonstrate analytically that adaptive step-size
rules can accelerate the convergence of self-attention. Additionally, we remove
the restriction of prior work on a fixed linear decoder. Our results reinforce
the implicit-bias perspective of self-attention and strengthen its connections
to implicit-bias in linear logistic regression, despite the intricate
non-convex nature of the former.",http://arxiv.org/abs/2402.05738v1,92,18,20,20,18,16
Examining Gender and Racial Bias in Large Vision-Language Models Using a Novel Dataset of Parallel Images,"Following on recent advances in large language models (LLMs) and subsequent
chat models, a new wave of large vision-language models (LVLMs) has emerged.
Such models can incorporate images as input in addition to text, and perform
tasks such as visual question answering, image captioning, story generation,
etc. Here, we examine potential gender and racial biases in such systems, based
on the perceived characteristics of the people in the input images. To
accomplish this, we present a new dataset PAIRS (PArallel Images for eveRyday
Scenarios). The PAIRS dataset contains sets of AI-generated images of people,
such that the images are highly similar in terms of background and visual
content, but differ along the dimensions of gender (man, woman) and race
(Black, white). By querying the LVLMs with such images, we observe significant
differences in the responses according to the perceived gender or race of the
person depicted.",http://arxiv.org/abs/2402.05779v1,92,18,18,20,18,18
Limits of Transformer Language Models on Learning Algorithmic Compositions,"We analyze the capabilities of Transformer language models on learning
discrete algorithms. To this end, we introduce two new tasks demanding the
composition of several discrete sub-tasks. On both training LLaMA models from
scratch and prompting on GPT-4 and Gemini we measure learning compositions of
learned primitives. We observe that the compositional capabilities of
state-of-the-art Transformer language models are very limited and sample-wise
scale worse than relearning all sub-tasks for a new algorithmic composition. We
also present a theorem in complexity theory, showing that gradient descent on
memorizing feedforward models can be exponentially data inefficient.",http://arxiv.org/abs/2402.05785v2,92,20,18,20,16,18
TaE: Task-aware Expandable Representation for Long Tail Class Incremental Learning,"Class-incremental learning (CIL) aims to train classifiers that learn new
classes without forgetting old ones. Most CIL methods focus on balanced data
distribution for each task, overlooking real-world long-tailed distributions.
Therefore, Long-Tailed Class-Incremental Learning (LT-CIL) has been introduced,
which trains on data where head classes have more samples than tail classes.
Existing methods mainly focus on preserving representative samples from
previous classes to combat catastrophic forgetting. Recently, dynamic network
algorithms frozen old network structures and expanded new ones, achieving
significant performance. However, with the introduction of the long-tail
problem, merely extending task-specific parameters can lead to miscalibrated
predictions, while expanding the entire model results in an explosion of memory
size. To address these issues, we introduce a novel Task-aware Expandable (TaE)
framework, dynamically allocating and updating task-specific trainable
parameters to learn diverse representations from each incremental task, while
resisting forgetting through the majority of frozen model parameters. To
further encourage the class-specific feature representation, we develop a
Centroid-Enhanced (CEd) method to guide the update of these task-aware
parameters. This approach is designed to adaptively minimize the distances
between intra-class features while simultaneously maximizing the distances
between inter-class features across all seen classes. The utility of this
centroid-enhanced method extends to all ""training from scratch"" CIL algorithms.
Extensive experiments were conducted on CIFAR-100 and ImageNet100 under
different settings, which demonstrates that TaE achieves state-of-the-art
performance.",http://arxiv.org/abs/2402.05797v1,92,18,19,19,18,18
Unsupervised Discovery of Clinical Disease Signatures Using Probabilistic Independence,"Insufficiently precise diagnosis of clinical disease is likely responsible
for many treatment failures, even for common conditions and treatments. With a
large enough dataset, it may be possible to use unsupervised machine learning
to define clinical disease patterns more precisely. We present an approach to
learning these patterns by using probabilistic independence to disentangle the
imprint on the medical record of causal latent sources of disease. We inferred
a broad set of 2000 clinical signatures of latent sources from 9195 variables
in 269,099 Electronic Health Records. The learned signatures produced better
discrimination than the original variables in a lung cancer prediction task
unknown to the inference algorithm, predicting 3-year malignancy in patients
with no history of cancer before a solitary lung nodule was discovered. More
importantly, the signatures' greater explanatory power identified pre-nodule
signatures of apparently undiagnosed cancer in many of those patients.",http://arxiv.org/abs/2402.05802v1,92,20,18,18,18,18
AvatarMMC: 3D Head Avatar Generation and Editing with Multi-Modal Conditioning,"We introduce an approach for 3D head avatar generation and editing with
multi-modal conditioning based on a 3D Generative Adversarial Network (GAN) and
a Latent Diffusion Model (LDM). 3D GANs can generate high-quality head avatars
given a single or no condition. However, it is challenging to generate samples
that adhere to multiple conditions of different modalities. On the other hand,
LDMs excel at learning complex conditional distributions. To this end, we
propose to exploit the conditioning capabilities of LDMs to enable multi-modal
control over the latent space of a pre-trained 3D GAN. Our method can generate
and edit 3D head avatars given a mixture of control signals such as RGB input,
segmentation masks, and global attributes. This provides better control over
the generation and editing of synthetic avatars both globally and locally.
Experiments show that our proposed approach outperforms a solely GAN-based
approach both qualitatively and quantitatively on generation and editing tasks.
To the best of our knowledge, our approach is the first to introduce
multi-modal conditioning to 3D avatar generation and editing.
\\href{avatarmmc-sig24.github.io}{Project Page}",http://arxiv.org/abs/2402.05803v1,92,20,20,18,18,16
Discovering Temporally-Aware Reinforcement Learning Algorithms,"Recent advancements in meta-learning have enabled the automatic discovery of
novel reinforcement learning algorithms parameterized by surrogate objective
functions. To improve upon manually designed algorithms, the parameterization
of this learned objective function must be expressive enough to represent novel
principles of learning (instead of merely recovering already established ones)
while still generalizing to a wide range of settings outside of its
meta-training distribution. However, existing methods focus on discovering
objective functions that, like many widely used objective functions in
reinforcement learning, do not take into account the total number of steps
allowed for training, or ""training horizon"". In contrast, humans use a plethora
of different learning objectives across the course of acquiring a new ability.
For instance, students may alter their studying techniques based on the
proximity to exam deadlines and their self-assessed capabilities. This paper
contends that ignoring the optimization time horizon significantly restricts
the expressive potential of discovered learning algorithms. We propose a simple
augmentation to two existing objective discovery approaches that allows the
discovered algorithm to dynamically update its objective function throughout
the agent's training procedure, resulting in expressive schedules and increased
generalization across different training horizons. In the process, we find that
commonly used meta-gradient approaches fail to discover such adaptive objective
functions while evolution strategies discover highly dynamic learning rules. We
demonstrate the effectiveness of our approach on a wide range of tasks and
analyze the resulting learned algorithms, which we find effectively balance
exploration and exploitation by modifying the structure of their learning rules
throughout the agent's lifetime.",http://arxiv.org/abs/2402.05828v1,92,19,18,18,18,19
Scaling Intelligent Agents in Combat Simulations for Wargaming,"Remaining competitive in future conflicts with technologically-advanced
competitors requires us to accelerate our research and development in
artificial intelligence (AI) for wargaming. More importantly, leveraging
machine learning for intelligent combat behavior development will be key to one
day achieving superhuman performance in this domain--elevating the quality and
accelerating the speed of our decisions in future wars. Although deep
reinforcement learning (RL) continues to show promising results in intelligent
agent behavior development in games, it has yet to perform at or above the
human level in the long-horizon, complex tasks typically found in combat
modeling and simulation. Capitalizing on the proven potential of RL and recent
successes of hierarchical reinforcement learning (HRL), our research is
investigating and extending the use of HRL to create intelligent agents capable
of performing effectively in these large and complex simulation environments.
Our ultimate goal is to develop an agent capable of superhuman performance that
could then serve as an AI advisor to military planners and decision-makers.
This papers covers our ongoing approach and the first three of our five
research areas aimed at managing the exponential growth of computations that
have thus far limited the use of AI in combat simulations: (1) developing an
HRL training framework and agent architecture for combat units; (2) developing
a multi-model framework for agent decision-making; (3) developing
dimension-invariant observation abstractions of the state space to manage the
exponential growth of computations; (4) developing an intrinsic rewards engine
to enable long-term planning; and (5) implementing this framework into a
higher-fidelity combat simulation.",http://arxiv.org/abs/2402.06694v1,92,18,20,18,18,18
Sparse-VQ Transformer: An FFN-Free Framework with Vector Quantization for Enhanced Time Series Forecasting,"Time series analysis is vital for numerous applications, and transformers
have become increasingly prominent in this domain. Leading methods customize
the transformer architecture from NLP and CV, utilizing a patching technique to
convert continuous signals into segments. Yet, time series data are uniquely
challenging due to significant distribution shifts and intrinsic noise levels.
To address these two challenges,we introduce the Sparse Vector Quantized
FFN-Free Transformer (Sparse-VQ). Our methodology capitalizes on a sparse
vector quantization technique coupled with Reverse Instance Normalization
(RevIN) to reduce noise impact and capture sufficient statistics for
forecasting, serving as an alternative to the Feed-Forward layer (FFN) in the
transformer architecture. Our FFN-free approach trims the parameter count,
enhancing computational efficiency and reducing overfitting. Through
evaluations across ten benchmark datasets, including the newly introduced CAISO
dataset, Sparse-VQ surpasses leading models with a 7.84% and 4.17% decrease in
MAE for univariate and multivariate time series forecasting, respectively.
Moreover, it can be seamlessly integrated with existing transformer-based
models to elevate their performance.",http://arxiv.org/abs/2402.05830v1,92,20,18,20,18,16
Privacy-Preserving Synthetic Continual Semantic Segmentation for Robotic Surgery,"Deep Neural Networks (DNNs) based semantic segmentation of the robotic
instruments and tissues can enhance the precision of surgical activities in
robot-assisted surgery. However, in biological learning, DNNs cannot learn
incremental tasks over time and exhibit catastrophic forgetting, which refers
to the sharp decline in performance on previously learned tasks after learning
a new one. Specifically, when data scarcity is the issue, the model shows a
rapid drop in performance on previously learned instruments after learning new
data with new instruments. The problem becomes worse when it limits releasing
the dataset of the old instruments for the old model due to privacy concerns
and the unavailability of the data for the new or updated version of the
instruments for the continual learning model. For this purpose, we develop a
privacy-preserving synthetic continual semantic segmentation framework by
blending and harmonizing (i) open-source old instruments foreground to the
synthesized background without revealing real patient data in public and (ii)
new instruments foreground to extensively augmented real background. To boost
the balanced logit distillation from the old model to the continual learning
model, we design overlapping class-aware temperature normalization (CAT) by
controlling model learning utility. We also introduce multi-scale
shifted-feature distillation (SD) to maintain long and short-range spatial
relationships among the semantic objects where conventional short-range spatial
features with limited information reduce the power of feature distillation. We
demonstrate the effectiveness of our framework on the EndoVis 2017 and 2018
instrument segmentation dataset with a generalized continual learning setting.
Code is available at~\url{https://github.com/XuMengyaAmy/Synthetic_CAT_SD}.",http://arxiv.org/abs/2402.05860v1,92,18,20,18,18,18
Memory Consolidation Enables Long-Context Video Understanding,"Most transformer-based video encoders are limited to short temporal contexts
due to their quadratic complexity. While various attempts have been made to
extend this context, this has often come at the cost of both conceptual and
computational complexity. We propose to instead re-purpose existing pre-trained
video transformers by simply fine-tuning them to attend to memories derived
non-parametrically from past activations. By leveraging redundancy reduction,
our memory-consolidated vision transformer (MC-ViT) effortlessly extends its
context far into the past and exhibits excellent scaling behavior when learning
from longer videos. In doing so, MC-ViT sets a new state-of-the-art in
long-context video understanding on EgoSchema, Perception Test, and Diving48,
outperforming methods that benefit from orders of magnitude more parameters.",http://arxiv.org/abs/2402.05861v1,92,20,18,18,18,18
EUGENE: Explainable Unsupervised Approximation of Graph Edit Distance,"The need to identify graphs having small structural distance from a query
arises in biology, chemistry, recommender systems, and social network analysis.
Among several methods to measure inter graph distance, Graph Edit Distance
(GED) is preferred for its comprehensibility, yet hindered by the NP-hardness
of its computation. State-of-the-art GED approximations predominantly employ
neural methods, which, however, (i) lack an explanatory edit path corresponding
to the approximated GED; (ii) require the NP-hard generation of ground-truth
GEDs for training; and (iii) necessitate separate training on each dataset. In
this paper, we propose an efficient algebraic unsuper vised method, EUGENE,
that approximates GED and yields edit paths corresponding to the approx imated
cost, while eliminating the need for ground truth generation and data-specific
training. Extensive experimental evaluation demonstrates that the
aforementioned benefits of EUGENE do not come at the cost of efficacy.
Specifically, EUGENE consistently ranks among the most accurate methods across
all of the benchmark datasets and outperforms majority of the neural
approaches.",http://arxiv.org/abs/2402.05885v1,92,19,19,18,18,18
Risk-Sensitive Multi-Agent Reinforcement Learning in Network Aggregative Markov Games,"Classical multi-agent reinforcement learning (MARL) assumes risk neutrality
and complete objectivity for agents. However, in settings where agents need to
consider or model human economic or social preferences, a notion of risk must
be incorporated into the RL optimization problem. This will be of greater
importance in MARL where other human or non-human agents are involved, possibly
with their own risk-sensitive policies. In this work, we consider
risk-sensitive and non-cooperative MARL with cumulative prospect theory (CPT),
a non-convex risk measure and a generalization of coherent measures of risk.
CPT is capable of explaining loss aversion in humans and their tendency to
overestimate/underestimate small/large probabilities. We propose a distributed
sampling-based actor-critic (AC) algorithm with CPT risk for network
aggregative Markov games (NAMGs), which we call Distributed Nested CPT-AC.
Under a set of assumptions, we prove the convergence of the algorithm to a
subjective notion of Markov perfect Nash equilibrium in NAMGs. The experimental
results show that subjective CPT policies obtained by our algorithm can be
different from the risk-neutral ones, and agents with a higher loss aversion
are more inclined to socially isolate themselves in an NAMG.",http://arxiv.org/abs/2402.05906v1,92,19,18,19,18,18
Efficient Stagewise Pretraining via Progressive Subnetworks,"Recent developments in large language models have sparked interest in
efficient pretraining methods. A recent effective paradigm is to perform
stage-wise training, where the size of the model is gradually increased over
the course of training (e.g. gradual stacking (Reddi et al., 2023)). While the
resource and wall-time savings are appealing, it has limitations, particularly
the inability to evaluate the full model during earlier stages, and degradation
in model quality due to smaller model capacity in the initial stages. In this
work, we propose an alternative framework, progressive subnetwork training,
that maintains the full model throughout training, but only trains subnetworks
within the model in each step. We focus on a simple instantiation of this
framework, Random Path Training (RaPTr) that only trains a sub-path of layers
in each step, progressively increasing the path lengths in stages. RaPTr
achieves better pre-training loss for BERT and UL2 language models while
requiring 20-33% fewer FLOPs compared to standard training, and is competitive
or better than other efficient training methods. Furthermore, RaPTr shows
better downstream performance on UL2, improving QA tasks and SuperGLUE by 1-5%
compared to standard training and stacking. Finally, we provide a theoretical
basis for RaPTr to justify (a) the increasing complexity of subnetworks in
stages, and (b) the stability in loss across stage transitions due to residual
connections and layer norm.",http://arxiv.org/abs/2402.05913v1,92,19,19,19,18,17
An Interactive Agent Foundation Model,"The development of artificial intelligence systems is transitioning from
creating static, task-specific models to dynamic, agent-based systems capable
of performing well in a wide range of applications. We propose an Interactive
Agent Foundation Model that uses a novel multi-task agent training paradigm for
training AI agents across a wide range of domains, datasets, and tasks. Our
training paradigm unifies diverse pre-training strategies, including visual
masked auto-encoders, language modeling, and next-action prediction, enabling a
versatile and adaptable AI framework. We demonstrate the performance of our
framework across three separate domains -- Robotics, Gaming AI, and Healthcare.
Our model demonstrates its ability to generate meaningful and contextually
relevant outputs in each area. The strength of our approach lies in its
generality, leveraging a variety of data sources such as robotics sequences,
gameplay data, large-scale video datasets, and textual information for
effective multimodal and multi-task learning. Our approach provides a promising
avenue for developing generalist, action-taking, multimodal systems.",http://arxiv.org/abs/2402.05929v1,92,18,20,20,18,16
Time Series Diffusion in the Frequency Domain,"Fourier analysis has been an instrumental tool in the development of signal
processing. This leads us to wonder whether this framework could similarly
benefit generative modelling. In this paper, we explore this question through
the scope of time series diffusion models. More specifically, we analyze
whether representing time series in the frequency domain is a useful inductive
bias for score-based diffusion models. By starting from the canonical SDE
formulation of diffusion in the time domain, we show that a dual diffusion
process occurs in the frequency domain with an important nuance: Brownian
motions are replaced by what we call mirrored Brownian motions, characterized
by mirror symmetries among their components. Building on this insight, we show
how to adapt the denoising score matching approach to implement diffusion
models in the frequency domain. This results in frequency diffusion models,
which we compare to canonical time diffusion models. Our empirical evaluation
on real-world datasets, covering various domains like healthcare and finance,
shows that frequency diffusion models better capture the training distribution
than time diffusion models. We explain this observation by showing that time
series from these datasets tend to be more localized in the frequency domain
than in the time domain, which makes them easier to model in the former case.
All our observations point towards impactful synergies between Fourier analysis
and diffusion models.",http://arxiv.org/abs/2402.05933v1,92,20,18,20,18,16
Decision Theory-Guided Deep Reinforcement Learning for Fast Learning,"This paper introduces a novel approach, Decision Theory-guided Deep
Reinforcement Learning (DT-guided DRL), to address the inherent cold start
problem in DRL. By integrating decision theory principles, DT-guided DRL
enhances agents' initial performance and robustness in complex environments,
enabling more efficient and reliable convergence during learning. Our
investigation encompasses two primary problem contexts: the cart pole and maze
navigation challenges. Experimental results demonstrate that the integration of
decision theory not only facilitates effective initial guidance for DRL agents
but also promotes a more structured and informed exploration strategy,
particularly in environments characterized by large and intricate state spaces.
The results of experiment demonstrate that DT-guided DRL can provide
significantly higher rewards compared to regular DRL. Specifically, during the
initial phase of training, the DT-guided DRL yields up to an 184% increase in
accumulated reward. Moreover, even after reaching convergence, it maintains a
superior performance, ending with up to 53% more reward than standard DRL in
large maze problems. DT-guided DRL represents an advancement in mitigating a
fundamental challenge of DRL by leveraging functions informed by human
(designer) knowledge, setting a foundation for further research in this
promising interdisciplinary domain.",http://arxiv.org/abs/2402.06023v1,92,18,20,20,18,16
Direct Acquisition Optimization for Low-Budget Active Learning,"Active Learning (AL) has gained prominence in integrating data-intensive
machine learning (ML) models into domains with limited labeled data. However,
its effectiveness diminishes significantly when the labeling budget is low. In
this paper, we first empirically observe the performance degradation of
existing AL algorithms in the low-budget settings, and then introduce Direct
Acquisition Optimization (DAO), a novel AL algorithm that optimizes sample
selections based on expected true loss reduction. Specifically, DAO utilizes
influence functions to update model parameters and incorporates an additional
acquisition strategy to mitigate bias in loss estimation. This approach
facilitates a more accurate estimation of the overall error reduction, without
extensive computations or reliance on labeled data. Experiments demonstrate
DAO's effectiveness in low budget settings, outperforming state-of-the-arts
approaches across seven benchmarks.",http://arxiv.org/abs/2402.06045v1,92,20,18,18,18,18
ActiveDP: Bridging Active Learning and Data Programming,"Modern machine learning models require large labelled datasets to achieve
good performance, but manually labelling large datasets is expensive and
time-consuming. The data programming paradigm enables users to label large
datasets efficiently but produces noisy labels, which deteriorates the
downstream model's performance. The active learning paradigm, on the other
hand, can acquire accurate labels but only for a small fraction of instances.
In this paper, we propose ActiveDP, an interactive framework bridging active
learning and data programming together to generate labels with both high
accuracy and coverage, combining the strengths of both paradigms. Experiments
show that ActiveDP outperforms previous weak supervision and active learning
approaches and consistently performs well under different labelling budgets.",http://arxiv.org/abs/2402.06056v1,92,18,20,18,18,18
Foundational Inference Models for Dynamical Systems,"Ordinary differential equations (ODEs) underlie dynamical systems which serve
as models for a vast number of natural and social phenomena. Yet inferring the
ODE that best describes a set of noisy observations on one such phenomenon can
be remarkably challenging, and the models available to achieve it tend to be
highly specialized and complex too. In this work we propose a novel supervised
learning framework for zero-shot inference of ODEs from noisy data. We first
generate large datasets of one-dimensional ODEs, by sampling distributions over
the space of initial conditions, and the space of vector fields defining them.
We then learn neural maps between noisy observations on the solutions of these
equations, and their corresponding initial condition and vector fields. The
resulting models, which we call foundational inference models (FIM), can be (i)
copied and matched along the time dimension to increase their resolution; and
(ii) copied and composed to build inference models of any dimensionality,
without the need of any finetuning. We use FIM to model both ground-truth
dynamical systems of different dimensionalities and empirical time series data
in a zero-shot fashion, and outperform state-of-the-art models which are
finetuned to these systems. Our (pretrained) FIMs are available online",http://arxiv.org/abs/2402.07594v1,92,19,19,19,18,17
Co-Pilot for Health: Personalized Algorithmic AI Nudging to Improve Health Outcomes,"The ability to shape health behaviors of large populations automatically,
across wearable types and disease conditions at scale has tremendous potential
to improve global health outcomes. We designed and implemented an AI driven
platform for digital algorithmic nudging, enabled by a Graph-Neural Network
(GNN) based Recommendation System, and granular health behavior data from
wearable fitness devices. Here we describe the efficacy results of this
platform with its capabilities of personalized and contextual nudging to
$n=84,764$ individuals over a 12-week period in Singapore. We statistically
validated that participants in the target group who received such AI optimized
daily nudges increased daily physical activity like step count by 6.17% ($p =
3.09\times10^{-4}$) and weekly minutes of Moderate to Vigorous Physical
Activity (MVPA) by 7.61% ($p = 1.16\times10^{-2}$), compared to matched
participants in control group who did not receive any nudges. Further, such
nudges were very well received, with a 13.1% of nudges sent being opened (open
rate), and 11.7% of the opened nudges rated useful compared to 1.9% rated as
not useful thereby demonstrating significant improvement in population level
engagement metrics.",http://arxiv.org/abs/2401.10816v2,92,20,18,20,18,16
PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models,"Large language models (LLMs) have achieved remarkable success due to their
exceptional generative capabilities. Despite their success, they also have
inherent limitations such as a lack of up-to-date knowledge and hallucination.
Retrieval-Augmented Generation (RAG) is a state-of-the-art technique to
mitigate those limitations. In particular, given a question, RAG retrieves
relevant knowledge from a knowledge database to augment the input of the LLM.
For instance, the retrieved knowledge could be a set of top-k texts that are
most semantically similar to the given question when the knowledge database
contains millions of texts collected from Wikipedia. As a result, the LLM could
utilize the retrieved knowledge as the context to generate an answer for the
given question. Existing studies mainly focus on improving the accuracy or
efficiency of RAG, leaving its security largely unexplored. We aim to bridge
the gap in this work. Particularly, we propose PoisonedRAG , a set of knowledge
poisoning attacks to RAG, where an attacker could inject a few poisoned texts
into the knowledge database such that the LLM generates an attacker-chosen
target answer for an attacker-chosen target question. We formulate knowledge
poisoning attacks as an optimization problem, whose solution is a set of
poisoned texts. Depending on the background knowledge (e.g., black-box and
white-box settings) of an attacker on the RAG, we propose two solutions to
solve the optimization problem, respectively. Our results on multiple benchmark
datasets and LLMs show our attacks could achieve 90% attack success rates when
injecting 5 poisoned texts for each target question into a database with
millions of texts. We also evaluate recent defenses and our results show they
are insufficient to defend against our attacks, highlighting the need for new
defenses.",http://arxiv.org/abs/2402.07867v1,92,19,18,19,18,18
World Model on Million-Length Video And Language With RingAttention,"Current language models fall short in understanding aspects of the world not
easily described in words, and struggle with complex, long-form tasks. Video
sequences offer valuable temporal information absent in language and static
images, making them attractive for joint modeling with language. Such models
could develop a understanding of both human textual knowledge and the physical
world, enabling broader AI capabilities for assisting humans. However, learning
from millions of tokens of video and language sequences poses challenges due to
memory constraints, computational complexity, and limited datasets. To address
these challenges, we curate a large dataset of diverse videos and books,
utilize the RingAttention technique to scalably train on long sequences, and
gradually increase context size from 4K to 1M tokens. This paper makes the
following contributions: (a) Largest context size neural network: We train one
of the largest context size transformers on long video and language sequences,
setting new benchmarks in difficult retrieval tasks and long video
understanding. (b) Solutions for overcoming vision-language training
challenges, including using masked sequence packing for mixing different
sequence lengths, loss weighting to balance language and vision, and
model-generated QA dataset for long sequence chat. (c) A highly-optimized
implementation with RingAttention, masked sequence packing, and other key
features for training on millions-length multimodal sequences. (d) Fully
open-sourced a family of 7B parameter models capable of processing long text
documents (LWM-Text, LWM-Text-Chat) and videos (LWM, LWM-Chat) of over 1M
tokens. This work paves the way for training on massive datasets of long video
and language to develop understanding of both human knowledge and the
multimodal world, and broader capabilities.",http://arxiv.org/abs/2402.08268v1,92,20,20,18,18,16
Contextual Multinomial Logit Bandits with General Value Functions,"Contextual multinomial logit (MNL) bandits capture many real-world assortment
recommendation problems such as online retailing/advertising. However, prior
work has only considered (generalized) linear value functions, which greatly
limits its applicability. Motivated by this fact, in this work, we consider
contextual MNL bandits with a general value function class that contains the
ground truth, borrowing ideas from a recent trend of studies on contextual
bandits. Specifically, we consider both the stochastic and the adversarial
settings, and propose a suite of algorithms, each with different
computation-regret trade-off. When applied to the linear case, our results not
only are the first ones with no dependence on a certain problem-dependent
constant that can be exponentially large, but also enjoy other advantages such
as computational efficiency, dimension-free regret bounds, or the ability to
handle completely adversarial contexts and rewards.",http://arxiv.org/abs/2402.08126v1,92,20,18,18,18,18
Parallel-friendly Spatio-Temporal Graph Learning for Photovoltaic Degradation Analysis at Scale,"We propose a novel Spatio-Temporal Graph Neural Network empowered trend
analysis approach (ST-GTrend) to perform fleet-level performance degradation
analysis for Photovoltaic (PV) power networks. PV power stations have become an
integral component to the global sustainable energy production landscape.
Accurately estimating the performance of PV systems is critical to their
feasibility as a power generation technology and as a financial asset. One of
the most challenging problems in assessing the Levelized Cost of Energy (LCOE)
of a PV system is to understand and estimate the long-term Performance Loss
Rate (PLR) for large fleets of PV inverters. ST-GTrend integrates
spatio-temporal coherence and graph attention to separate PLR as a long-term
""aging"" trend from multiple fluctuation terms in the PV input data. To cope
with diverse degradation patterns in timeseries, ST-GTrend adopts a paralleled
graph autoencoder array to extract aging and fluctuation terms simultaneously.
ST-GTrend imposes flatness and smoothness regularization to ensure the
disentanglement between aging and fluctuation. To scale the analysis to large
PV systems, we also introduce Para-GTrend, a parallel algorithm to accelerate
the training and inference of ST-GTrend. We have evaluated ST-GTrend on three
large-scale PV datasets, spanning a time period of 10 years. Our results show
that ST-GTrend reduces Mean Absolute Percent Error (MAPE) and Euclidean
Distances by 34.74% and 33.66% compared to the SOTA methods. Our results
demonstrate that Para-GTrend can speed up ST-GTrend by up to 7.92 times. We
further verify the generality and effectiveness of ST-GTrend for trend analysis
using financial and economic datasets.",http://arxiv.org/abs/2402.08470v1,92,18,18,20,18,18
Generalizing across Temporal Domains with Koopman Operators,"In the field of domain generalization, the task of constructing a predictive
model capable of generalizing to a target domain without access to target data
remains challenging. This problem becomes further complicated when considering
evolving dynamics between domains. While various approaches have been proposed
to address this issue, a comprehensive understanding of the underlying
generalization theory is still lacking. In this study, we contribute novel
theoretic results that aligning conditional distribution leads to the reduction
of generalization bounds. Our analysis serves as a key motivation for solving
the Temporal Domain Generalization (TDG) problem through the application of
Koopman Neural Operators, resulting in Temporal Koopman Networks (TKNets). By
employing Koopman Operators, we effectively address the time-evolving
distributions encountered in TDG using the principles of Koopman theory, where
measurement functions are sought to establish linear transition relations
between evolving domains. Through empirical evaluations conducted on synthetic
and real-world datasets, we validate the effectiveness of our proposed
approach.",http://arxiv.org/abs/2402.07834v1,92,19,18,19,18,18
Mapping the Ethics of Generative AI: A Comprehensive Scoping Review,"The advent of generative artificial intelligence and the widespread adoption
of it in society engendered intensive debates about its ethical implications
and risks. These risks often differ from those associated with traditional
discriminative machine learning. To synthesize the recent discourse and map its
normative concepts, we conducted a scoping review on the ethics of generative
artificial intelligence, including especially large language models and
text-to-image models. Our analysis provides a taxonomy of 378 normative issues
in 19 topic areas and ranks them according to their prevalence in the
literature. The study offers a comprehensive overview for scholars,
practitioners, or policymakers, condensing the ethical debates surrounding
fairness, safety, harmful content, hallucinations, privacy, interaction risks,
security, alignment, societal impacts, and others. We discuss the results,
evaluate imbalances in the literature, and explore unsubstantiated risk
scenarios.",http://arxiv.org/abs/2402.08323v1,92,18,20,20,18,16
Revealing Decurve Flows for Generalized Graph Propagation,"This study addresses the limitations of the traditional analysis of
message-passing, central to graph learning, by defining {\em
\textbf{generalized propagation}} with directed and weighted graphs. The
significance manifest in two ways. \textbf{Firstly}, we propose {\em
Generalized Propagation Neural Networks} (\textbf{GPNNs}), a framework that
unifies most propagation-based graph neural networks. By generating
directed-weighted propagation graphs with adjacency function and connectivity
function, GPNNs offer enhanced insights into attention mechanisms across
various graph models. We delve into the trade-offs within the design space with
empirical experiments and emphasize the crucial role of the adjacency function
for model expressivity via theoretical analysis. \textbf{Secondly}, we propose
the {\em Continuous Unified Ricci Curvature} (\textbf{CURC}), an extension of
celebrated {\em Ollivier-Ricci Curvature} for directed and weighted graphs.
Theoretically, we demonstrate that CURC possesses continuity, scale invariance,
and a lower bound connection with the Dirichlet isoperimetric constant
validating bottleneck analysis for GPNNs. We include a preliminary exploration
of learned propagation patterns in datasets, a first in the field. We observe
an intriguing ``{\em \textbf{decurve flow}}'' - a curvature reduction during
training for models with learnable propagation, revealing the evolution of
propagation over time and a deeper connection to over-smoothing and bottleneck
trade-off.",http://arxiv.org/abs/2402.08480v1,92,20,18,20,18,16
Sparsity via Sparse Group $k$-max Regularization,"For the linear inverse problem with sparsity constraints, the $l_0$
regularized problem is NP-hard, and existing approaches either utilize greedy
algorithms to find almost-optimal solutions or to approximate the $l_0$
regularization with its convex counterparts. In this paper, we propose a novel
and concise regularization, namely the sparse group $k$-max regularization,
which can not only simultaneously enhance the group-wise and in-group sparsity,
but also casts no additional restraints on the magnitude of variables in each
group, which is especially important for variables at different scales, so that
it approximate the $l_0$ norm more closely. We also establish an iterative soft
thresholding algorithm with local optimality conditions and complexity analysis
provided. Through numerical experiments on both synthetic and real-world
datasets, we verify the effectiveness and flexibility of the proposed method.",http://arxiv.org/abs/2402.08493v1,92,18,20,18,18,18
A Systematic Review of Data-to-Text NLG,"This systematic review aims to provide a comprehensive analysis of the state
of data-to-text generation research, focusing on identifying research gaps,
offering future directions, and addressing challenges found during the review.
We thoroughly examined the literature, including approaches, datasets,
evaluation metrics, applications, multilingualism, and hallucination mitigation
measures. Our review provides a roadmap for future research in this rapidly
evolving field.",http://arxiv.org/abs/2402.08496v1,92,18,20,20,18,16
Provable Traffic Rule Compliance in Safe Reinforcement Learning on the Open Sea,"Autonomous vehicles have to obey traffic rules. These rules are often
formalized using temporal logic, resulting in constraints that are hard to
solve using optimization-based motion planners. Reinforcement Learning (RL) is
a promising method to find motion plans adhering to temporal logic
specifications. However, vanilla RL algorithms are based on random exploration,
which is inherently unsafe. To address this issue, we propose a provably safe
RL approach that always complies with traffic rules. As a specific application
area, we consider vessels on the open sea, which must adhere to the Convention
on the International Regulations for Preventing Collisions at Sea (COLREGS). We
introduce an efficient verification approach that determines the compliance of
actions with respect to the COLREGS formalized using temporal logic. Our action
verification is integrated into the RL process so that the agent only selects
verified actions. In contrast to agents that only integrate the traffic rule
information in the reward function, our provably safe agent always complies
with the formalized rules in critical maritime traffic situations and, thus,
never causes a collision.",http://arxiv.org/abs/2402.08502v1,92,18,20,20,16,18
"TELLER: A Trustworthy Framework for Explainable, Generalizable and Controllable Fake News Detection","The proliferation of fake news has emerged as a severe societal problem,
raising significant interest from industry and academia. While existing
deep-learning based methods have made progress in detecting fake news
accurately, their reliability may be compromised caused by the non-transparent
reasoning processes, poor generalization abilities and inherent risks of
integration with large language models (LLMs). To address this challenge, we
propose {\methodname}, a novel framework for trustworthy fake news detection
that prioritizes explainability, generalizability and controllability of
models. This is achieved via a dual-system framework that integrates cognition
and decision systems, adhering to the principles above. The cognition system
harnesses human expertise to generate logical predicates, which guide LLMs in
generating human-readable logic atoms. Meanwhile, the decision system deduces
generalizable logic rules to aggregate these atoms, enabling the identification
of the truthfulness of the input news across diverse domains and enhancing
transparency in the decision-making process. Finally, we present comprehensive
evaluation results on four datasets, demonstrating the feasibility and
trustworthiness of our proposed framework. Our implementation is available at
\url{https://github.com/less-and-less-bugs/Trust_TELLER}.",http://arxiv.org/abs/2402.07776v1,92,20,19,18,18,17
Towards Meta-Pruning via Optimal Transport,"Structural pruning of neural networks conventionally relies on identifying
and discarding less important neurons, a practice often resulting in
significant accuracy loss that necessitates subsequent fine-tuning efforts.
This paper introduces a novel approach named Intra-Fusion, challenging this
prevailing pruning paradigm. Unlike existing methods that focus on designing
meaningful neuron importance metrics, Intra-Fusion redefines the overlying
pruning procedure. Through utilizing the concepts of model fusion and Optimal
Transport, we leverage an agnostically given importance metric to arrive at a
more effective sparse model representation. Notably, our approach achieves
substantial accuracy recovery without the need for resource-intensive
fine-tuning, making it an efficient and promising tool for neural network
compression.
  Additionally, we explore how fusion can be added to the pruning process to
significantly decrease the training time while maintaining competitive
performance. We benchmark our results for various networks on commonly used
datasets such as CIFAR-10, CIFAR-100, and ImageNet. More broadly, we hope that
the proposed Intra-Fusion approach invigorates exploration into a fresh
alternative to the predominant compression approaches. Our code is available
here: https://github.com/alexandertheus/Intra-Fusion.",http://arxiv.org/abs/2402.07839v2,92,19,20,19,17,17
Confronting Reward Overoptimization for Diffusion Models: A Perspective of Inductive and Primacy Biases,"Bridging the gap between diffusion models and human preferences is crucial
for their integration into practical generative workflows. While optimizing
downstream reward models has emerged as a promising alignment strategy,
concerns arise regarding the risk of excessive optimization with learned reward
models, which potentially compromises ground-truth performance. In this work,
we confront the reward overoptimization problem in diffusion model alignment
through the lenses of both inductive and primacy biases. We first identify the
divergence of current methods from the temporal inductive bias inherent in the
multi-step denoising process of diffusion models as a potential source of
overoptimization. Then, we surprisingly discover that dormant neurons in our
critic model act as a regularization against overoptimization, while active
neurons reflect primacy bias in this setting. Motivated by these observations,
we propose Temporal Diffusion Policy Optimization with critic active neuron
Reset (TDPO-R), a policy gradient algorithm that exploits the temporal
inductive bias of intermediate timesteps, along with a novel reset strategy
that targets active neurons to counteract the primacy bias. Empirical results
demonstrate the superior efficacy of our algorithms in mitigating reward
overoptimization.",http://arxiv.org/abs/2402.08552v1,92,18,20,20,18,16
End-to-End Learning for Fair Multiobjective Optimization Under Uncertainty,"Many decision processes in artificial intelligence and operations research
are modeled by parametric optimization problems whose defining parameters are
unknown and must be inferred from observable data. The Predict-Then-Optimize
(PtO) paradigm in machine learning aims to maximize downstream decision quality
by training the parametric inference model end-to-end with the subsequent
constrained optimization. This requires backpropagation through the
optimization problem using approximation techniques specific to the problem's
form, especially for nondifferentiable linear and mixed-integer programs. This
paper extends the PtO methodology to optimization problems with
nondifferentiable Ordered Weighted Averaging (OWA) objectives, known for their
ability to ensure properties of fairness and robustness in decision models.
Through a collection of training techniques and proposed application settings,
it shows how optimization of OWA functions can be effectively integrated with
parametric prediction for fair and robust optimization under uncertainty.",http://arxiv.org/abs/2402.07772v1,92,20,18,18,18,18
Text Detoxification as Style Transfer in English and Hindi,"This paper focuses on text detoxification, i.e., automatically converting
toxic text into non-toxic text. This task contributes to safer and more
respectful online communication and can be considered a Text Style Transfer
(TST) task, where the text style changes while its content is preserved. We
present three approaches: knowledge transfer from a similar task, multi-task
learning approach, combining sequence-to-sequence modeling with various
toxicity classification tasks, and, delete and reconstruct approach. To support
our research, we utilize a dataset provided by Dementieva et al.(2021), which
contains multiple versions of detoxified texts corresponding to toxic texts. In
our experiments, we selected the best variants through expert human annotators,
creating a dataset where each toxic sentence is paired with a single,
appropriate detoxified version. Additionally, we introduced a small Hindi
parallel dataset, aligning with a part of the English dataset, suitable for
evaluation purposes. Our results demonstrate that our approach effectively
balances text detoxication while preserving the actual content and maintaining
fluency.",http://arxiv.org/abs/2402.07767v1,92,18,20,18,18,18
Do Membership Inference Attacks Work on Large Language Models?,"Membership inference attacks (MIAs) attempt to predict whether a particular
datapoint is a member of a target model's training data. Despite extensive
research on traditional machine learning models, there has been limited work
studying MIA on the pre-training data of large language models (LLMs). We
perform a large-scale evaluation of MIAs over a suite of language models (LMs)
trained on the Pile, ranging from 160M to 12B parameters. We find that MIAs
barely outperform random guessing for most settings across varying LLM sizes
and domains. Our further analyses reveal that this poor performance can be
attributed to (1) the combination of a large dataset and few training
iterations, and (2) an inherently fuzzy boundary between members and
non-members. We identify specific settings where LLMs have been shown to be
vulnerable to membership inference and show that the apparent success in such
settings can be attributed to a distribution shift, such as when members and
non-members are drawn from the seemingly identical domain but with different
temporal ranges. We release our code and data as a unified benchmark package
that includes all existing MIAs, supporting future work.",http://arxiv.org/abs/2402.07841v1,92,19,20,18,18,17
Policy Improvement using Language Feedback Models,"We introduce Language Feedback Models (LFMs) that identify desirable
behaviour - actions that help achieve tasks specified in the instruction - for
imitation learning in instruction following. To train LFMs, we obtain feedback
from Large Language Models (LLMs) on visual trajectories verbalized to language
descriptions. First, by using LFMs to identify desirable behaviour to imitate,
we improve in task-completion rate over strong behavioural cloning baselines on
three distinct language grounding environments (Touchdown, ScienceWorld, and
ALFWorld). Second, LFMs outperform using LLMs as experts to directly predict
actions, when controlling for the number of LLM output tokens. Third, LFMs
generalize to unseen environments, improving task-completion rate by 3.5-12.0%
through one round of adaptation. Finally, LFM can be modified to provide
human-interpretable feedback without performance loss, allowing human
verification of desirable behaviour for imitation learning.",http://arxiv.org/abs/2402.07876v1,92,20,18,20,18,16
Scaling Laws for Fine-Grained Mixture of Experts,"Mixture of Experts (MoE) models have emerged as a primary solution for
reducing the computational cost of Large Language Models. In this work, we
analyze their scaling properties, incorporating an expanded range of variables.
Specifically, we introduce a new hyperparameter, granularity, whose adjustment
enables precise control over the size of the experts. Building on this, we
establish scaling laws for fine-grained MoE, taking into account the number of
training tokens, model size, and granularity. Leveraging these laws, we derive
the optimal training configuration for a given computational budget. Our
findings not only show that MoE models consistently outperform dense
Transformers but also highlight that the efficiency gap between dense and MoE
models widens as we scale up the model size and training budget. Furthermore,
we demonstrate that the common practice of setting the size of experts in MoE
to mirror the feed-forward layer is not optimal at almost any computational
budget.",http://arxiv.org/abs/2402.07871v1,92,18,18,18,18,20
Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models,"Diffusion models have gained attention in text processing, offering many
potential advantages over traditional autoregressive models. This work explores
the integration of diffusion models and Chain-of-Thought (CoT), a
well-established technique to improve the reasoning ability in autoregressive
language models. We propose Diffusion-of-Thought (DoT), allowing reasoning
steps to diffuse over time through the diffusion process. In contrast to
traditional autoregressive language models that make decisions in a
left-to-right, token-by-token manner, DoT offers more flexibility in the
trade-off between computation and reasoning performance. Our experimental
results demonstrate the effectiveness of DoT in multi-digit multiplication and
grade school math problems. Additionally, DoT showcases promising
self-correction abilities and benefits from existing reasoning-enhancing
techniques like self-consistency decoding. Our findings contribute to the
understanding and development of reasoning capabilities in diffusion language
models.",http://arxiv.org/abs/2402.07754v1,92,20,19,19,18,16
Mixed Q-Functionals: Advancing Value-Based Methods in Cooperative MARL with Continuous Action Domains,"Tackling multi-agent learning problems efficiently is a challenging task in
continuous action domains. While value-based algorithms excel in sample
efficiency when applied to discrete action domains, they are usually
inefficient when dealing with continuous actions. Policy-based algorithms, on
the other hand, attempt to address this challenge by leveraging critic networks
for guiding the learning process and stabilizing the gradient estimation. The
limitations in the estimation of true return and falling into local optima in
these methods result in inefficient and often sub-optimal policies. In this
paper, we diverge from the trend of further enhancing critic networks, and
focus on improving the effectiveness of value-based methods in multi-agent
continuous domains by concurrently evaluating numerous actions. We propose a
novel multi-agent value-based algorithm, Mixed Q-Functionals (MQF), inspired
from the idea of Q-Functionals, that enables agents to transform their states
into basis functions. Our algorithm fosters collaboration among agents by
mixing their action-values. We evaluate the efficacy of our algorithm in six
cooperative multi-agent scenarios. Our empirical findings reveal that MQF
outperforms four variants of Deep Deterministic Policy Gradient through rapid
action evaluation and increased sample efficiency.",http://arxiv.org/abs/2402.07752v1,92,19,19,19,18,17
Gaussian Ensemble Belief Propagation for Efficient Inference in High-Dimensional Systems,"Efficient inference in high-dimensional models remains a central challenge in
machine learning. This paper introduces the Gaussian Ensemble Belief
Propagation (GEnBP) algorithm, a fusion of the Ensemble Kalman filter and
Gaussian belief propagation (GaBP) methods. GEnBP updates ensembles by passing
low-rank local messages in a graphical model structure. This combination
inherits favourable qualities from each method. Ensemble techniques allow GEnBP
to handle high-dimensional states, parameters and intricate, noisy, black-box
generation processes. The use of local messages in a graphical model structure
ensures that the approach is suited to distributed computing and can
efficiently handle complex dependence structures. GEnBP is particularly
advantageous when the ensemble size is considerably smaller than the inference
dimension. This scenario often arises in fields such as spatiotemporal
modelling, image processing and physical model inversion. GEnBP can be applied
to general problem structures, including jointly learning system parameters,
observation parameters, and latent state variables.",http://arxiv.org/abs/2402.08193v1,92,20,18,19,18,17
Prompted Contextual Vectors for Spear-Phishing Detection,"Spear-phishing attacks present a significant security challenge, with large
language models (LLMs) escalating the threat by generating convincing emails
and facilitating target reconnaissance. To address this, we propose a detection
approach based on a novel document vectorization method that utilizes an
ensemble of LLMs to create representation vectors. By prompting LLMs to reason
and respond to human-crafted questions, we quantify the presence of common
persuasion principles in the email's content, producing prompted contextual
document vectors for a downstream supervised machine learning model. We
evaluate our method using a unique dataset generated by a proprietary system
that automates target reconnaissance and spear-phishing email creation. Our
method achieves a 91% F1 score in identifying LLM-generated spear-phishing
emails, with the training set comprising only traditional phishing and benign
emails. Key contributions include an innovative document vectorization method
utilizing LLM reasoning, a publicly available dataset of high-quality
spear-phishing emails, and the demonstrated effectiveness of our method in
detecting such emails. This methodology can be utilized for various document
classification tasks, particularly in adversarial problem domains.",http://arxiv.org/abs/2402.08309v1,92,18,20,19,17,18
Subgraphormer: Unifying Subgraph GNNs and Graph Transformers via Graph Products,"In the realm of Graph Neural Networks (GNNs), two exciting research
directions have recently emerged: Subgraph GNNs and Graph Transformers. In this
paper, we propose an architecture that integrates both approaches, dubbed
Subgraphormer, which combines the enhanced expressive power, message-passing
mechanisms, and aggregation schemes from Subgraph GNNs with attention and
positional encodings, arguably the most important components in Graph
Transformers. Our method is based on an intriguing new connection we reveal
between Subgraph GNNs and product graphs, suggesting that Subgraph GNNs can be
formulated as Message Passing Neural Networks (MPNNs) operating on a product of
the graph with itself. We use this formulation to design our architecture:
first, we devise an attention mechanism based on the connectivity of the
product graph. Following this, we propose a novel and efficient positional
encoding scheme for Subgraph GNNs, which we derive as a positional encoding for
the product graph. Our experimental results demonstrate significant performance
improvements over both Subgraph GNNs and Graph Transformers on a wide range of
datasets.",http://arxiv.org/abs/2402.08450v1,92,20,18,20,18,16
Recursive Joint Simulation in Games,"Game-theoretic dynamics between AI agents could differ from traditional
human-human interactions in various ways. One such difference is that it may be
possible to accurately simulate an AI agent, for example because its source
code is known. Our aim is to explore ways of leveraging this possibility to
achieve more cooperative outcomes in strategic settings. In this paper, we
study an interaction between AI agents where the agents run a recursive joint
simulation. That is, the agents first jointly observe a simulation of the
situation they face. This simulation in turn recursively includes additional
simulations (with a small chance of failure, to avoid infinite recursion), and
the results of all these nested simulations are observed before an action is
chosen. We show that the resulting interaction is strategically equivalent to
an infinitely repeated version of the original game, allowing a direct transfer
of existing results such as the various folk theorems.",http://arxiv.org/abs/2402.08128v1,92,19,20,19,18,16
H2O-SDF: Two-phase Learning for 3D Indoor Reconstruction using Object Surface Fields,"Advanced techniques using Neural Radiance Fields (NeRF), Signed Distance
Fields (SDF), and Occupancy Fields have recently emerged as solutions for 3D
indoor scene reconstruction. We introduce a novel two-phase learning approach,
H2O-SDF, that discriminates between object and non-object regions within indoor
environments. This method achieves a nuanced balance, carefully preserving the
geometric integrity of room layouts while also capturing intricate surface
details of specific objects. A cornerstone of our two-phase learning framework
is the introduction of the Object Surface Field (OSF), a novel concept designed
to mitigate the persistent vanishing gradient problem that has previously
hindered the capture of high-frequency details in other methods. Our proposed
approach is validated through several experiments that include ablation
studies.",http://arxiv.org/abs/2402.08138v1,92,18,20,18,18,18
From Uncertainty to Precision: Enhancing Binary Classifier Performance through Calibration,"The assessment of binary classifier performance traditionally centers on
discriminative ability using metrics, such as accuracy. However, these metrics
often disregard the model's inherent uncertainty, especially when dealing with
sensitive decision-making domains, such as finance or healthcare. Given that
model-predicted scores are commonly seen as event probabilities, calibration is
crucial for accurate interpretation. In our study, we analyze the sensitivity
of various calibration measures to score distortions and introduce a refined
metric, the Local Calibration Score. Comparing recalibration methods, we
advocate for local regressions, emphasizing their dual role as effective
recalibration tools and facilitators of smoother visualizations. We apply these
findings in a real-world scenario using Random Forest classifier and regressor
to predict credit default while simultaneously measuring calibration during
performance optimization.",http://arxiv.org/abs/2402.07790v1,92,18,19,19,18,18
Advancing Data-driven Weather Forecasting: Time-Sliding Data Augmentation of ERA5,"Modern deep learning techniques, which mimic traditional numerical weather
prediction (NWP) models and are derived from global atmospheric reanalysis
data, have caused a significant revolution within a few years. In this new
paradigm, our research introduces a novel strategy that deviates from the
common dependence on high-resolution data, which is often constrained by
computational resources, and instead utilizes low-resolution data (2.5 degrees)
for global weather prediction and climate data analysis. Our main focus is
evaluating data-driven weather prediction (DDWP) frameworks, specifically
addressing sample size adequacy, structural improvements to the model, and the
ability of climate data to represent current climatic trends. By using the
Adaptive Fourier Neural Operator (AFNO) model via FourCastNet and a proposed
time-sliding method to inflate the dataset of the ECMWF Reanalysis v5 (ERA5),
this paper improves on conventional approaches by adding more variables and a
novel approach to data augmentation and processing. Our findings reveal that
despite the lower resolution, the proposed approach demonstrates considerable
accuracy in predicting atmospheric conditions, effectively rivaling
higher-resolution models. Furthermore, the study confirms the model's
proficiency in reflecting current climate trends and its potential in
predicting future climatic events, underscoring its utility in climate change
strategies. This research marks a pivotal step in the realm of meteorological
forecasting, showcasing the feasibility of lower-resolution data in producing
reliable predictions and opening avenues for more accessible and inclusive
climate modeling. The insights gleaned from this study not only contribute to
the advancement of climate science but also lay the groundwork for future
innovations in the field.",http://arxiv.org/abs/2402.08185v1,92,18,20,18,18,18
Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning,"Finetuning on task-specific datasets is a widely-embraced paradigm of
harnessing the powerful capability of pretrained LLMs for various downstream
tasks. Due to the popularity of LLMs finetuning and its accompanying privacy
concerns, differentially private (DP) finetuning of pretrained LLMs has
garnered increasing attention to safeguarding the privacy of task-specific
datasets. Lying at the design core of DP LLM finetuning methods is the
satisfactory tradeoff between privacy, utility, and scalability. Most existing
methods build upon the seminal work of DP-SGD. Despite pushing the scalability
of DP-SGD to its limit, DP-SGD-based finetuning methods are unfortunately
limited by the inherent inefficiency of SGD. In this paper, we investigate the
potential of DP zeroth-order methods for LLM pretraining, which avoids the
scalability bottleneck of SGD by approximating the gradient with the more
efficient zeroth-order gradient. Rather than treating the zeroth-order method
as a drop-in replacement for SGD, this paper presents a comprehensive study
both theoretically and empirically. First, we propose the stagewise DP
zeroth-order method that dynamically schedules key hyperparameters. This design
is grounded on the synergy between DP random perturbation and the gradient
approximation error of the zeroth-order method, and its effect on finetuning
trajectory. Second, we further enhance the scalability by reducing the
trainable parameters that are identified by repurposing a data-free pruning
technique requiring no additional data or extra privacy budget. We provide
theoretical analysis for both proposed methods. We conduct extensive empirical
analysis on both encoder-only masked language model and decoder-only
autoregressive language model, achieving impressive results in terms of
scalability and utility.",http://arxiv.org/abs/2402.07818v1,92,20,18,20,18,16
WildfireGPT: Tailored Large Language Model for Wildfire Analysis,"The recent advancement of large language models (LLMs) represents a
transformational capability at the frontier of artificial intelligence (AI) and
machine learning (ML). However, LLMs are generalized models, trained on
extensive text corpus, and often struggle to provide context-specific
information, particularly in areas requiring specialized knowledge such as
wildfire details within the broader context of climate change. For
decision-makers and policymakers focused on wildfire resilience and adaptation,
it is crucial to obtain responses that are not only precise but also
domain-specific, rather than generic. To that end, we developed WildfireGPT, a
prototype LLM agent designed to transform user queries into actionable insights
on wildfire risks. We enrich WildfireGPT by providing additional context such
as climate projections and scientific literature to ensure its information is
current, relevant, and scientifically accurate. This enables WildfireGPT to be
an effective tool for delivering detailed, user-specific insights on wildfire
risks to support a diverse set of end users, including researchers, engineers,
urban planners, emergency managers, and infrastructure operators.",http://arxiv.org/abs/2402.07877v1,92,20,18,20,18,16
MAVRL: Learn to Fly in Cluttered Environments with Varying Speed,"Many existing obstacle avoidance algorithms overlook the crucial balance
between safety and agility, especially in environments of varying complexity.
In our study, we introduce an obstacle avoidance pipeline based on
reinforcement learning. This pipeline enables drones to adapt their flying
speed according to the environmental complexity. Moreover, to improve the
obstacle avoidance performance in cluttered environments, we propose a novel
latent space. The latent space in this representation is explicitly trained to
retain memory of previous depth map observations. Our findings confirm that
varying speed leads to a superior balance of success rate and agility in
cluttered environments. Additionally, our memory-augmented latent
representation outperforms the latent representation commonly used in
reinforcement learning. Finally, after minimal fine-tuning, we successfully
deployed our network on a real drone for enhanced obstacle avoidance.",http://arxiv.org/abs/2402.08381v1,92,20,18,20,16,18
Punctuation Restoration Improves Structure Understanding without Supervision,"Unsupervised learning objectives like language modeling and de-noising
constitute a significant part in producing pre-trained models that perform
various downstream applications from natural language understanding to
conversational tasks. However, despite impressive conversational capabilities
of recent large language model, their abilities to capture syntactic or
semantic structure within text lag behind. We hypothesize that the mismatch
between linguistic performance and competence in machines is attributable to
insufficient transfer of linguistic structure knowledge to computational
systems with currently popular pre-training objectives. We show that
punctuation restoration transfers to improvements in in- and
out-of-distribution performance on structure-related tasks like named entity
recognition, open information extraction, chunking, and part-of-speech tagging.
Punctuation restoration is an effective learning objective that can improve
structure understanding and yield a more robust structure-aware representations
of natural language.",http://arxiv.org/abs/2402.08382v1,92,18,20,20,17,17
Variational Continual Test-Time Adaptation,"The prior drift is crucial in Continual Test-Time Adaptation (CTTA) methods
that only use unlabeled test data, as it can cause significant error
propagation. In this paper, we introduce VCoTTA, a variational Bayesian
approach to measure uncertainties in CTTA. At the source stage, we transform a
pre-trained deterministic model into a Bayesian Neural Network (BNN) via a
variational warm-up strategy, injecting uncertainties into the model. During
the testing time, we employ a mean-teacher update strategy using variational
inference for the student model and exponential moving average for the teacher
model. Our novel approach updates the student model by combining priors from
both the source and teacher models. The evidence lower bound is formulated as
the cross-entropy between the student and teacher models, along with the
Kullback-Leibler (KL) divergence of the prior mixture. Experimental results on
three datasets demonstrate the method's effectiveness in mitigating prior drift
within the CTTA framework.",http://arxiv.org/abs/2402.08182v1,92,18,20,18,18,18
NeuRes: Learning Proofs of Propositional Satisfiability,"We introduce NeuRes, a neuro-symbolic proof-based SAT solver. Unlike other
neural SAT solving methods, NeuRes is capable of proving unsatisfiability as
opposed to merely predicting it. By design, NeuRes operates in a
certificate-driven fashion by employing propositional resolution to prove
unsatisfiability and to accelerate the process of finding satisfying truth
assignments in case of unsat and sat formulas, respectively. To realize this,
we propose a novel architecture that adapts elements from Graph Neural Networks
and Pointer Networks to autoregressively select pairs of nodes from a dynamic
graph structure, which is essential to the generation of resolution proofs. Our
model is trained and evaluated on a dataset of teacher proofs and truth
assignments that we compiled with the same random formula distribution used by
NeuroSAT. In our experiments, we show that NeuRes solves more test formulas
than NeuroSAT by a rather wide margin on different distributions while being
much more data-efficient. Furthermore, we show that NeuRes is capable of
largely shortening teacher proofs by notable proportions. We use this feature
to devise a bootstrapped training procedure that manages to reduce a dataset of
proofs generated by an advanced solver by ~23% after training on it with no
extra guidance.",http://arxiv.org/abs/2402.08365v1,92,20,18,20,18,16
Visual Question Answering Instruction: Unlocking Multimodal Large Language Model To Domain-Specific Visual Multitasks,"Having revolutionized natural language processing (NLP) applications, large
language models (LLMs) are expanding into the realm of multimodal inputs. Owing
to their ability to interpret images, multimodal LLMs (MLLMs) have been
primarily used for vision-language tasks. Currently, MLLMs have not yet been
extended for domain-specific visual tasks, which require a more explicit
understanding of visual information. We developed a method to transform
domain-specific visual and vision-language datasets into a unified question
answering format called Visual Question Answering Instruction (VQA-IN), thereby
extending MLLM to domain-specific tasks. The VQA-IN was applied to train
multiple MLLM architectures using smaller versions of LLMs (sLLMs). The
experimental results indicated that the proposed method achieved a high score
metric on domainspecific visual tasks while also maintaining its performance on
vision-language tasks in a multitask manner.",http://arxiv.org/abs/2402.08360v1,92,19,18,19,19,17
Selective Learning: Towards Robust Calibration with Dynamic Regularization,"Miscalibration in deep learning refers to there is a discrepancy between the
predicted confidence and performance. This problem usually arises due to the
overfitting problem, which is characterized by learning everything presented in
the training set, resulting in overconfident predictions during testing.
Existing methods typically address overfitting and mitigate the miscalibration
by adding a maximum-entropy regularizer to the objective function. The
objective can be understood as seeking a model that fits the ground-truth
labels by increasing the confidence while also maximizing the entropy of
predicted probabilities by decreasing the confidence. However, previous methods
lack clear guidance on confidence adjustment, leading to conflicting objectives
(increasing but also decreasing confidence). Therefore, we introduce a method
called Dynamic Regularization (DReg), which aims to learn what should be
learned during training thereby circumventing the confidence adjusting
trade-off. At a high level, DReg aims to obtain a more reliable model capable
of acknowledging what it knows and does not know. Specifically, DReg
effectively fits the labels for in-distribution samples (samples that should be
learned) while applying regularization dynamically to samples beyond model
capabilities (e.g., outliers), thereby obtaining a robust calibrated model
especially on the samples beyond model capabilities. Both theoretical and
empirical analyses sufficiently demonstrate the superiority of DReg compared
with previous methods.",http://arxiv.org/abs/2402.08384v1,92,18,19,19,18,18
LLaGA: Large Language and Graph Assistant,"Graph Neural Networks (GNNs) have empowered the advance in graph-structured
data analysis. Recently, the rise of Large Language Models (LLMs) like GPT-4
has heralded a new era in deep learning. However, their application to graph
data poses distinct challenges due to the inherent difficulty of translating
graph structures to language. To this end, we introduce the \textbf{L}arge
\textbf{L}anguage \textbf{a}nd \textbf{G}raph \textbf{A}ssistant
(\textbf{LLaGA}), an innovative model that effectively integrates LLM
capabilities to handle the complexities of graph-structured data. LLaGA retains
the general-purpose nature of LLMs while adapting graph data into a format
compatible with LLM input. LLaGA achieves this by reorganizing graph nodes to
structure-aware sequences and then mapping these into the token embedding space
through a versatile projector. LLaGA excels in versatility, generalizability
and interpretability, allowing it to perform consistently well across different
datasets and tasks, extend its ability to unseen datasets or tasks, and provide
explanations for graphs. Our extensive experiments across popular graph
benchmarks show that LLaGA delivers outstanding performance across four
datasets and three tasks using one single model, surpassing state-of-the-art
graph models in both supervised and zero-shot scenarios. Our code is available
at \url{https://github.com/ChenRunjin/LLaGA}",http://arxiv.org/abs/2402.08170v1,92,20,19,19,18,16
Adaptive Hierarchical Certification for Segmentation using Randomized Smoothing,"Common certification methods operate on a flat pre-defined set of
fine-grained classes. In this paper, however, we propose a novel, more general,
and practical setting, namely adaptive hierarchical certification for image
semantic segmentation. In this setting, the certification can be within a
multi-level hierarchical label space composed of fine to coarse levels. Unlike
classic methods where the certification would abstain for unstable components,
our approach adaptively relaxes the certification to a coarser level within the
hierarchy. This relaxation lowers the abstain rate whilst providing more
certified semantically meaningful information. We mathematically formulate the
problem setup and introduce, for the first time, an adaptive hierarchical
certification algorithm for image semantic segmentation, that certifies image
pixels within a hierarchy and prove the correctness of its guarantees. Since
certified accuracy does not take the loss of information into account when
traversing into a coarser hierarchy level, we introduce a novel evaluation
paradigm for adaptive hierarchical certification, namely the certified
information gain metric, which is proportional to the class granularity level.
Our evaluation experiments on real-world challenging datasets such as
Cityscapes and ACDC demonstrate that our adaptive algorithm achieves a higher
certified information gain and a lower abstain rate compared to the current
state-of-the-art certification method, as well as other non-adaptive versions
of it.",http://arxiv.org/abs/2402.08400v1,92,18,18,20,18,18
Vision-Based Hand Gesture Customization from a Single Demonstration,"Hand gesture recognition is becoming a more prevalent mode of human-computer
interaction, especially as cameras proliferate across everyday devices. Despite
continued progress in this field, gesture customization is often underexplored.
Customization is crucial since it enables users to define and demonstrate
gestures that are more natural, memorable, and accessible. However,
customization requires efficient usage of user-provided data. We introduce a
method that enables users to easily design bespoke gestures with a monocular
camera from one demonstration. We employ transformers and meta-learning
techniques to address few-shot learning challenges. Unlike prior work, our
method supports any combination of one-handed, two-handed, static, and dynamic
gestures, including different viewpoints. We evaluated our customization method
through a user study with 20 gestures collected from 21 participants, achieving
up to 97% average recognition accuracy from one demonstration. Our work
provides a viable path for vision-based gesture customization, laying the
foundation for future advancements in this domain.",http://arxiv.org/abs/2402.08420v1,92,20,19,19,17,17
Conservative and Risk-Aware Offline Multi-Agent Reinforcement Learning for Digital Twins,"Digital twin (DT) platforms are increasingly regarded as a promising
technology for controlling, optimizing, and monitoring complex engineering
systems such as next-generation wireless networks. An important challenge in
adopting DT solutions is their reliance on data collected offline, lacking
direct access to the physical environment. This limitation is particularly
severe in multi-agent systems, for which conventional multi-agent reinforcement
(MARL) requires online interactions with the environment. A direct application
of online MARL schemes to an offline setting would generally fail due to the
epistemic uncertainty entailed by the limited availability of data. In this
work, we propose an offline MARL scheme for DT-based wireless networks that
integrates distributional RL and conservative Q-learning to address the
environment's inherent aleatoric uncertainty and the epistemic uncertainty
arising from limited data. To further exploit the offline data, we adapt the
proposed scheme to the centralized training decentralized execution framework,
allowing joint training of the agents' policies. The proposed MARL scheme,
referred to as multi-agent conservative quantile regression (MA-CQR) addresses
general risk-sensitive design criteria and is applied to the trajectory
planning problem in drone networks, showcasing its advantages.",http://arxiv.org/abs/2402.08421v1,92,20,18,18,18,18
Vehicle Behavior Prediction by Episodic-Memory Implanted NDT,"In autonomous driving, predicting the behavior (turning left, stopping, etc.)
of target vehicles is crucial for the self-driving vehicle to make safe
decisions and avoid accidents. Existing deep learning-based methods have shown
excellent and accurate performance, but the black-box nature makes it
untrustworthy to apply them in practical use. In this work, we explore the
interpretability of behavior prediction of target vehicles by an Episodic
Memory implanted Neural Decision Tree (abbrev. eMem-NDT). The structure of
eMem-NDT is constructed by hierarchically clustering the text embedding of
vehicle behavior descriptions. eMem-NDT is a neural-backed part of a
pre-trained deep learning model by changing the soft-max layer of the deep
model to eMem-NDT, for grouping and aligning the memory prototypes of the
historical vehicle behavior features in training data on a neural decision
tree. Each leaf node of eMem-NDT is modeled by a neural network for aligning
the behavior memory prototypes. By eMem-NDT, we infer each instance in behavior
prediction of vehicles by bottom-up Memory Prototype Matching (MPM) (searching
the appropriate leaf node and the links to the root node) and top-down Leaf
Link Aggregation (LLA) (obtaining the probability of future behaviors of
vehicles for certain instances). We validate eMem-NDT on BLVD and LOKI
datasets, and the results show that our model can obtain a superior performance
to other methods with clear explainability. The code is available at
https://github.com/JWFangit/eMem-NDT.",http://arxiv.org/abs/2402.08423v1,92,18,20,20,18,16
Conditional Neural Expert Processes for Learning from Demonstration,"Learning from Demonstration (LfD) is a widely used technique for skill
acquisition in robotics. However, demonstrations of the same skill may exhibit
significant variances, or learning systems may attempt to acquire different
means of the same skill simultaneously, making it challenging to encode these
motions into movement primitives. To address these challenges, we propose an
LfD framework, namely the Conditional Neural Expert Processes (CNEP), that
learns to assign demonstrations from different modes to distinct expert
networks utilizing the inherent information within the latent space to match
experts with the encoded representations. CNEP does not require supervision on
which mode the trajectories belong to. Provided experiments on artificially
generated datasets demonstrate the efficacy of CNEP. Furthermore, we compare
the performance of CNEP with another LfD framework, namely Conditional Neural
Movement Primitives (CNMP), on a range of tasks, including experiments on a
real robot. The results reveal enhanced modeling performance for movement
primitives, leading to the synthesis of trajectories that more accurately
reflect those demonstrated by experts, particularly when the model inputs
include intersection points from various trajectories. Additionally, CNEP
offers improved interpretability and faster convergence by promoting expert
specialization. Furthermore, we show that the CNEP model accomplishes obstacle
avoidance tasks with a real manipulator when provided with novel start and
destination points, in contrast to the CNMP model, which leads to collisions
with the obstacle.",http://arxiv.org/abs/2402.08424v1,92,19,20,18,18,17
Frequency-aware Graph Signal Processing for Collaborative Filtering,"Graph Signal Processing (GSP) based recommendation algorithms have recently
attracted lots of attention due to its high efficiency. However, these methods
failed to consider the importance of various interactions that reflect unique
user/item characteristics and failed to utilize user and item high-order
neighborhood information to model user preference, thus leading to sub-optimal
performance. To address the above issues, we propose a frequency-aware graph
signal processing method (FaGSP) for collaborative filtering. Firstly, we
design a Cascaded Filter Module, consisting of an ideal high-pass filter and an
ideal low-pass filter that work in a successive manner, to capture both unique
and common user/item characteristics to more accurately model user preference.
Then, we devise a Parallel Filter Module, consisting of two low-pass filters
that can easily capture the hierarchy of neighborhood, to fully utilize
high-order neighborhood information of users/items for more accurate user
preference modeling. Finally, we combine these two modules via a linear model
to further improve recommendation accuracy. Extensive experiments on six public
datasets demonstrate the superiority of our method from the perspectives of
prediction accuracy and training efficiency compared with state-of-the-art
GCN-based recommendation methods and GSP-based recommendation methods.",http://arxiv.org/abs/2402.08426v1,92,18,20,18,18,18
On Computationally Efficient Multi-Class Calibration,"Consider a multi-class labelling problem, where the labels can take values in
$[k]$, and a predictor predicts a distribution over the labels. In this work,
we study the following foundational question: Are there notions of multi-class
calibration that give strong guarantees of meaningful predictions and can be
achieved in time and sample complexities polynomial in $k$? Prior notions of
calibration exhibit a tradeoff between computational efficiency and
expressivity: they either suffer from having sample complexity exponential in
$k$, or needing to solve computationally intractable problems, or give rather
weak guarantees.
  Our main contribution is a notion of calibration that achieves all these
desiderata: we formulate a robust notion of projected smooth calibration for
multi-class predictions, and give new recalibration algorithms for efficiently
calibrating predictors under this definition with complexity polynomial in $k$.
Projected smooth calibration gives strong guarantees for all downstream
decision makers who want to use the predictor for binary classification
problems of the form: does the label belong to a subset $T \subseteq [k]$: e.g.
is this an image of an animal? It ensures that the probabilities predicted by
summing the probabilities assigned to labels in $T$ are close to some perfectly
calibrated binary predictor for that task. We also show that natural
strengthenings of our definition are computationally hard to achieve: they run
into information theoretic barriers or computational intractability. Underlying
both our upper and lower bounds is a tight connection that we prove between
multi-class calibration and the well-studied problem of agnostic learning in
the (standard) binary prediction setting.",http://arxiv.org/abs/2402.07821v1,92,18,20,18,18,18
Epistemic Exploration for Generalizable Planning and Learning in Non-Stationary Settings,"This paper introduces a new approach for continual planning and model
learning in non-stationary stochastic environments expressed using relational
representations. Such capabilities are essential for the deployment of
sequential decision-making systems in the uncertain, constantly evolving real
world. Working in such practical settings with unknown (and non-stationary)
transition systems and changing tasks, the proposed framework models gaps in
the agent's current state of knowledge and uses them to conduct focused,
investigative explorations. Data collected using these explorations is used for
learning generalizable probabilistic models for solving the current task
despite continual changes in the environment dynamics. Empirical evaluations on
several benchmark domains show that this approach significantly outperforms
planning and RL baselines in terms of sample complexity in non-stationary
settings. Theoretical results show that the system reverts to exhibit desirable
convergence properties when stationarity holds.",http://arxiv.org/abs/2402.08145v1,92,20,20,18,18,16
Continuous Assurance of Autonomous Vehicle Behavior Through Machine Learned Correctness Properties,"Correctness properties are critical to conducting verification and validation
on software systems, especially those cyberphysical systems whose functionality
changes frequently due to software updates, changes in the operating
environment, or newly learned behaviors. We detail a novel method to
automatically construct expressive, executable correctness properties in the
form of machine-learned correctness properties which can be used to ensure that
a system's behavior is correct with respect to its design and operating
requirements. We propose a method to bootstrap the creation of these
correctness properties using a novel simulation-based generation of training
and testing data using multiple extensions to the Cross Entropy algorithm for
search-based optimization. Then, we apply this method to a software-in-the-loop
evaluation of an autonomous vehicle to demonstrate that such models can assert
about important properties of multi-agent cyberphysical systems. We demonstrate
that this process brings the task of developing robust correctness properties
from the realm of formal methods experts into the domain of system developers
and engineers, and that machine-learned correctness properties are expressive
enough to capture the correct behavior of cyberphysical systems in their
complex environments. This advancement can provide evidence of dependability to
system designers and users, enhancing trust in the deployment of autonomous
vehicles and other intelligent transportation systems.",http://arxiv.org/abs/2402.07791v1,92,20,18,20,18,16
"Towards Unified Alignment Between Agents, Humans, and Environment","The rapid progress of foundation models has led to the prosperity of
autonomous agents, which leverage the universal capabilities of foundation
models to conduct reasoning, decision-making, and environmental interaction.
However, the efficacy of agents remains limited when operating in intricate,
realistic environments. In this work, we introduce the principles of
$\mathbf{U}$nified $\mathbf{A}$lignment for $\mathbf{A}$gents
($\mathbf{UA}^2$), which advocate for the simultaneous alignment of agents with
human intentions, environmental dynamics, and self-constraints such as the
limitation of monetary budgets. From the perspective of $\mathbf{UA}^2$, we
review the current agent research and highlight the neglected factors in
existing agent benchmarks and method candidates. We also conduct
proof-of-concept studies by introducing realistic features to WebShop,
including user profiles to demonstrate intentions, personalized reranking for
complex environmental dynamics, and runtime cost statistics to reflect
self-constraints. We then follow the principles of $\mathbf{UA}^2$ to propose
an initial design of our agent, and benchmark its performance with several
candidate baselines in the retrofitted WebShop. The extensive experimental
results further prove the importance of the principles of $\mathbf{UA}^2$. Our
research sheds light on the next steps of autonomous agent research with
improved general problem-solving abilities.",http://arxiv.org/abs/2402.07744v1,92,18,20,20,18,16
Predictive Churn with the Set of Good Models,"Machine learning models in modern mass-market applications are often updated
over time. One of the foremost challenges faced is that, despite increasing
overall performance, these updates may flip specific model predictions in
unpredictable ways. In practice, researchers quantify the number of unstable
predictions between models pre and post update -- i.e., predictive churn. In
this paper, we study this effect through the lens of predictive multiplicity --
i.e., the prevalence of conflicting predictions over the set of near-optimal
models (the Rashomon set). We show how traditional measures of predictive
multiplicity can be used to examine expected churn over this set of prospective
models -- i.e., the set of models that may be used to replace a baseline model
in deployment. We present theoretical results on the expected churn between
models within the Rashomon set from different perspectives. And we characterize
expected churn over model updates via the Rashomon set, pairing our analysis
with empirical results on real-world datasets -- showing how our approach can
be used to better anticipate, reduce, and avoid churn in consumer-facing
applications. Further, we show that our approach is useful even for models
enhanced with uncertainty awareness.",http://arxiv.org/abs/2402.07745v1,92,19,19,19,18,17
Denoising Diffusion Restoration Tackles Forward and Inverse Problems for the Laplace Operator,"Diffusion models have emerged as a promising class of generative models that
map noisy inputs to realistic images. More recently, they have been employed to
generate solutions to partial differential equations (PDEs). However, they
still struggle with inverse problems in the Laplacian operator, for instance,
the Poisson equation, because the eigenvalues that are large in magnitude
amplify the measurement noise. This paper presents a novel approach for the
inverse and forward solution of PDEs through the use of denoising diffusion
restoration models (DDRM). DDRMs were used in linear inverse problems to
restore original clean signals by exploiting the singular value decomposition
(SVD) of the linear operator. Equivalently, we present an approach to restore
the solution and the parameters in the Poisson equation by exploiting the
eigenvalues and the eigenfunctions of the Laplacian operator. Our results show
that using denoising diffusion restoration significantly improves the
estimation of the solution and parameters. Our research, as a result, pioneers
the integration of diffusion models with the principles of underlying physics
to solve PDEs.",http://arxiv.org/abs/2402.08563v1,92,20,18,20,16,18
A Generalized Approach to Online Convex Optimization,"In this paper, we analyze the problem of online convex optimization in
different settings. We show that any algorithm for online linear optimization
with fully adaptive adversaries is an algorithm for online convex optimization.
We also show that any such algorithm that requires full-information feedback
may be transformed to an algorithm with semi-bandit feedback with comparable
regret bound. We further show that algorithms that are designed for fully
adaptive adversaries using deterministic semi-bandit feedback can obtain
similar bounds using only stochastic semi-bandit feedback when facing oblivious
adversaries. We use this to describe general meta-algorithms to convert first
order algorithms to zeroth order algorithms with comparable regret bounds. Our
framework allows us to analyze online optimization in various settings, such
full-information feedback, bandit feedback, stochastic regret, adversarial
regret and various forms of non-stationary regret. Using our analysis, we
provide the first efficient projection-free online convex optimization
algorithm using linear optimization oracles.",http://arxiv.org/abs/2402.08621v1,92,20,18,20,18,16
Boundary Exploration for Bayesian Optimization With Unknown Physical Constraints,"Bayesian optimization has been successfully applied to optimize black-box
functions where the number of evaluations is severely limited. However, in many
real-world applications, it is hard or impossible to know in advance which
designs are feasible due to some physical or system limitations. These issues
lead to an even more challenging problem of optimizing an unknown function with
unknown constraints. In this paper, we observe that in such scenarios optimal
solution typically lies on the boundary between feasible and infeasible regions
of the design space, making it considerably more difficult than that with
interior optima. Inspired by this observation, we propose BE-CBO, a new
Bayesian optimization method that efficiently explores the boundary between
feasible and infeasible designs. To identify the boundary, we learn the
constraints with an ensemble of neural networks that outperform the standard
Gaussian Processes for capturing complex boundaries. Our method demonstrates
superior performance against state-of-the-art methods through comprehensive
experiments on synthetic and real-world benchmarks.",http://arxiv.org/abs/2402.07692v1,92,18,20,18,18,18
MetaTra: Meta-Learning for Generalized Trajectory Prediction in Unseen Domain,"Trajectory prediction has garnered widespread attention in different fields,
such as autonomous driving and robotic navigation. However, due to the
significant variations in trajectory patterns across different scenarios,
models trained in known environments often falter in unseen ones. To learn a
generalized model that can directly handle unseen domains without requiring any
model updating, we propose a novel meta-learning-based trajectory prediction
method called MetaTra. This approach incorporates a Dual Trajectory Transformer
(Dual-TT), which enables a thorough exploration of the individual intention and
the interactions within group motion patterns in diverse scenarios. Building on
this, we propose a meta-learning framework to simulate the generalization
process between source and target domains. Furthermore, to enhance the
stability of our prediction outcomes, we propose a Serial and Parallel Training
(SPT) strategy along with a feature augmentation method named MetaMix.
Experimental results on several real-world datasets confirm that MetaTra not
only surpasses other state-of-the-art methods but also exhibits plug-and-play
capabilities, particularly in the realm of domain generalization.",http://arxiv.org/abs/2402.08221v1,92,18,20,18,18,18
Beyond LLMs: Advancing the Landscape of Complex Reasoning,"Since the advent of Large Language Models a few years ago, they have often
been considered the de facto solution for many AI problems. However, in
addition to the many deficiencies of LLMs that prevent them from broad industry
adoption, such as reliability, cost, and speed, there is a whole class of
common real world problems that Large Language Models perform poorly on,
namely, constraint satisfaction and optimization problems. These problems are
ubiquitous and current solutions are highly specialized and expensive to
implement. At Elemental Cognition, we developed our EC AI platform which takes
a neuro-symbolic approach to solving constraint satisfaction and optimization
problems. The platform employs, at its core, a precise and high performance
logical reasoning engine, and leverages LLMs for knowledge acquisition and user
interaction. This platform supports developers in specifying application logic
in natural and concise language while generating application user interfaces to
interact with users effectively. We evaluated LLMs against systems built on the
EC AI platform in three domains and found the EC AI systems to significantly
outperform LLMs on constructing valid and optimal solutions, on validating
proposed solutions, and on repairing invalid solutions.",http://arxiv.org/abs/2402.08064v1,92,20,19,19,18,16
Peeking Behind the Curtains of Residual Learning,"The utilization of residual learning has become widespread in deep and
scalable neural nets. However, the fundamental principles that contribute to
the success of residual learning remain elusive, thus hindering effective
training of plain nets with depth scalability. In this paper, we peek behind
the curtains of residual learning by uncovering the ""dissipating inputs""
phenomenon that leads to convergence failure in plain neural nets: the input is
gradually compromised through plain layers due to non-linearities, resulting in
challenges of learning feature representations. We theoretically demonstrate
how plain neural nets degenerate the input to random noise and emphasize the
significance of a residual connection that maintains a better lower bound of
surviving neurons as a solution. With our theoretical discoveries, we propose
""The Plain Neural Net Hypothesis"" (PNNH) that identifies the internal path
across non-linear layers as the most critical part in residual learning, and
establishes a paradigm to support the training of deep plain neural nets devoid
of residual connections. We thoroughly evaluate PNNH-enabled CNN architectures
and Transformers on popular vision benchmarks, showing on-par accuracy, up to
0.3% higher training throughput, and 2x better parameter efficiency compared to
ResNets and vision Transformers.",http://arxiv.org/abs/2402.08645v1,92,20,18,20,18,16
Improving Black-box Robustness with In-Context Rewriting,"Machine learning models often excel on in-distribution (ID) data but struggle
with unseen out-of-distribution (OOD) inputs. Most techniques for improving OOD
robustness are not applicable to settings where the model is effectively a
black box, such as when the weights are frozen, retraining is costly, or the
model is leveraged via an API. Test-time augmentation (TTA) is a simple
post-hoc technique for improving robustness that sidesteps black-box
constraints by aggregating predictions across multiple augmentations of the
test input. TTA has seen limited use in NLP due to the challenge of generating
effective natural language augmentations. In this work, we propose LLM-TTA,
which uses LLM-generated augmentations as TTA's augmentation function. LLM-TTA
outperforms conventional augmentation functions across sentiment, toxicity, and
news classification tasks for BERT and T5 models, with BERT's OOD robustness
improving by an average of 4.30 percentage points without regressing average ID
performance. We explore selectively augmenting inputs based on prediction
entropy to reduce the rate of expensive LLM augmentations, allowing us to
maintain performance gains while reducing the average number of generated
augmentations by 57.76%. LLM-TTA is agnostic to the task model architecture,
does not require OOD labels, and is effective across low and high-resource
settings. We share our data, models, and code for reproducibility.",http://arxiv.org/abs/2402.08225v1,92,19,18,19,18,18
Locality Sensitive Hashing for Network Traffic Fingerprinting,"The advent of the Internet of Things (IoT) has brought forth additional
intricacies and difficulties to computer networks. These gadgets are
particularly susceptible to cyber-attacks because of their simplistic design.
Therefore, it is crucial to recognise these devices inside a network for the
purpose of network administration and to identify any harmful actions. Network
traffic fingerprinting is a crucial technique for identifying devices and
detecting anomalies. Currently, the predominant methods for this depend heavily
on machine learning (ML). Nevertheless, machine learning (ML) methods need the
selection of features, adjustment of hyperparameters, and retraining of models
to attain optimal outcomes and provide resilience to concept drifts detected in
a network. In this research, we suggest using locality-sensitive hashing (LSH)
for network traffic fingerprinting as a solution to these difficulties. Our
study focuses on examining several design options for the Nilsimsa LSH
function. We then use this function to create unique fingerprints for network
data, which may be used to identify devices. We also compared it with ML-based
traffic fingerprinting and observed that our method increases the accuracy of
state-of-the-art by 12% achieving around 94% accuracy in identifying devices in
a network.",http://dx.doi.org/10.1109/LANMAN58293.2023.10189810,92,18,20,20,16,18
Contrastive Multiple Instance Learning for Weakly Supervised Person ReID,"The acquisition of large-scale, precisely labeled datasets for person
re-identification (ReID) poses a significant challenge. Weakly supervised ReID
has begun to address this issue, although its performance lags behind fully
supervised methods. In response, we introduce Contrastive Multiple Instance
Learning (CMIL), a novel framework tailored for more effective weakly
supervised ReID. CMIL distinguishes itself by requiring only a single model and
no pseudo labels while leveraging contrastive losses -- a technique that has
significantly enhanced traditional ReID performance yet is absent in all prior
MIL-based approaches. Through extensive experiments and analysis across three
datasets, CMIL not only matches state-of-the-art performance on the large-scale
SYSU-30k dataset with fewer assumptions but also consistently outperforms all
baselines on the WL-market1501 and Weakly Labeled MUddy racer re-iDentification
dataset (WL-MUDD) datasets. We introduce and release the WL-MUDD dataset, an
extension of the MUDD dataset featuring naturally occurring weak labels from
the real-world application at PerformancePhoto.co. All our code and data are
accessible at
https://drive.google.com/file/d/1rjMbWB6m-apHF3Wg_cfqc8QqKgQ21AsT/view?usp=drive_link.",http://arxiv.org/abs/2402.07685v1,92,18,20,20,16,18
DeformNet: Latent Space Modeling and Dynamics Prediction for Deformable Object Manipulation,"Manipulating deformable objects is a ubiquitous task in household
environments, demanding adequate representation and accurate dynamics
prediction due to the objects' infinite degrees of freedom. This work proposes
DeformNet, which utilizes latent space modeling with a learned 3D
representation model to tackle these challenges effectively. The proposed
representation model combines a PointNet encoder and a conditional neural
radiance field (NeRF), facilitating a thorough acquisition of object
deformations and variations in lighting conditions. To model the complex
dynamics, we employ a recurrent state-space model (RSSM) that accurately
predicts the transformation of the latent representation over time. Extensive
simulation experiments with diverse objectives demonstrate the generalization
capabilities of DeformNet for various deformable object manipulation tasks,
even in the presence of previously unseen goals. Finally, we deploy DeformNet
on an actual UR5 robotic arm to demonstrate its capability in real-world
scenarios.",http://arxiv.org/abs/2402.07648v1,92,20,20,18,18,16
Generating Universal Adversarial Perturbations for Quantum Classifiers,"Quantum Machine Learning (QML) has emerged as a promising field of research,
aiming to leverage the capabilities of quantum computing to enhance existing
machine learning methodologies. Recent studies have revealed that, like their
classical counterparts, QML models based on Parametrized Quantum Circuits
(PQCs) are also vulnerable to adversarial attacks. Moreover, the existence of
Universal Adversarial Perturbations (UAPs) in the quantum domain has been
demonstrated theoretically in the context of quantum classifiers. In this work,
we introduce QuGAP: a novel framework for generating UAPs for quantum
classifiers. We conceptualize the notion of additive UAPs for PQC-based
classifiers and theoretically demonstrate their existence. We then utilize
generative models (QuGAP-A) to craft additive UAPs and experimentally show that
quantum classifiers are susceptible to such attacks. Moreover, we formulate a
new method for generating unitary UAPs (QuGAP-U) using quantum generative
models and a novel loss function based on fidelity constraints. We evaluate the
performance of the proposed framework and show that our method achieves
state-of-the-art misclassification rates, while maintaining high fidelity
between legitimate and adversarial samples.",http://arxiv.org/abs/2402.08648v1,92,19,19,18,18,18
SAGMAN: Stability Analysis of Graph Neural Networks on the Manifolds,"Modern graph neural networks (GNNs) can be sensitive to changes in the input
graph structure and node features, potentially resulting in unpredictable
behavior and degraded performance. In this work, we introduce a spectral
framework known as SAGMAN for examining the stability of GNNs. This framework
assesses the distance distortions that arise from the nonlinear mappings of
GNNs between the input and output manifolds: when two nearby nodes on the input
manifold are mapped (through a GNN model) to two distant ones on the output
manifold, it implies a large distance distortion and thus a poor GNN stability.
We propose a distance-preserving graph dimension reduction (GDR) approach that
utilizes spectral graph embedding and probabilistic graphical models (PGMs) to
create low-dimensional input/output graph-based manifolds for meaningful
stability analysis. Our empirical evaluations show that SAGMAN effectively
assesses the stability of each node when subjected to various edge or feature
perturbations, offering a scalable approach for evaluating the stability of
GNNs, extending to applications within recommendation systems. Furthermore, we
illustrate its utility in downstream tasks, notably in enhancing GNN stability
and facilitating adversarial targeted attacks.",http://arxiv.org/abs/2402.08653v1,92,18,20,20,18,16
Learning Continuous 3D Words for Text-to-Image Generation,"Current controls over diffusion models (e.g., through text or ControlNet) for
image generation fall short in recognizing abstract, continuous attributes like
illumination direction or non-rigid shape change. In this paper, we present an
approach for allowing users of text-to-image models to have fine-grained
control of several attributes in an image. We do this by engineering special
sets of input tokens that can be transformed in a continuous manner -- we call
them Continuous 3D Words. These attributes can, for example, be represented as
sliders and applied jointly with text prompts for fine-grained control over
image generation. Given only a single mesh and a rendering engine, we show that
our approach can be adopted to provide continuous user control over several
3D-aware attributes, including time-of-day illumination, bird wing orientation,
dollyzoom effect, and object poses. Our method is capable of conditioning image
creation with multiple Continuous 3D Words and text descriptions simultaneously
while adding no overhead to the generative process. Project Page:
https://ttchengab.github.io/continuous_3d_words",http://arxiv.org/abs/2402.08654v1,92,20,19,18,18,17
Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models,"Visually-conditioned language models (VLMs) have seen growing adoption in
applications such as visual dialogue, scene understanding, and robotic task
planning; adoption that has fueled a wealth of new models such as LLaVa,
InstructBLIP, and PaLI-3. Despite the volume of new releases, key design
decisions around image preprocessing, architecture, and optimization are
under-explored, making it challenging to understand what factors account for
model performance $-$ a challenge further complicated by the lack of objective,
consistent evaluations. To address these gaps, we first compile a suite of
standardized evaluations spanning visual question answering, object
localization from language, and targeted challenge sets that probe properties
such as hallucination; evaluations that provide calibrated, fine-grained
insight into a VLM's capabilities. Second, we rigorously investigate VLMs along
key design axes, including pretrained visual representations and quantifying
the tradeoffs of using base vs. instruct-tuned language models, amongst others.
We couple our analysis with three resource contributions: (1) a unified
framework for evaluating VLMs, (2) optimized, flexible code for VLM training,
and (3) checkpoints for all models, including a family of VLMs at the 7-13B
scale that strictly outperform InstructBLIP and LLaVa v1.5, the
state-of-the-art in open-source VLMs.",http://arxiv.org/abs/2402.07865v1,92,19,18,19,18,18
Object Detection in Thermal Images Using Deep Learning for Unmanned Aerial Vehicles,"This work presents a neural network model capable of recognizing small and
tiny objects in thermal images collected by unmanned aerial vehicles. Our model
consists of three parts, the backbone, the neck, and the prediction head. The
backbone is developed based on the structure of YOLOv5 combined with the use of
a transformer encoder at the end. The neck includes a BI-FPN block combined
with the use of a sliding window and a transformer to increase the information
fed into the prediction head. The prediction head carries out the detection by
evaluating feature maps with the Sigmoid function. The use of transformers with
attention and sliding windows increases recognition accuracy while keeping the
model at a reasonable number of parameters and computation requirements for
embedded systems. Experiments conducted on public dataset VEDAI and our
collected datasets show that our model has a higher accuracy than
state-of-the-art methods such as ResNet, Faster RCNN, ComNet, ViT, YOLOv5,
SMPNet, and DPNetV3. Experiments on the embedded computer Jetson AGX show that
our model achieves a real-time computation speed with a stability rate of over
90%.",http://dx.doi.org/10.1109/SII58957.2024.10417611,92,18,20,20,16,18
The Last JITAI? The Unreasonable Effectiveness of Large Language Models in Issuing Just-in-Time Adaptive Interventions: Fostering Physical Activity in a Prospective Cardiac Rehabilitation Setting,"We explored the viability of Large Language Models (LLMs) for triggering and
personalizing content for Just-in-Time Adaptive Interventions (JITAIs) in
digital health. JITAIs are being explored as a key mechanism for sustainable
behavior change, adapting interventions to an individual's current context and
needs. However, traditional rule-based and machine learning models for JITAI
implementation face scalability and reliability limitations, such as lack of
personalization, difficulty in managing multi-parametric systems, and issues
with data sparsity. To investigate JITAI implementation via LLMs, we tested the
contemporary overall performance-leading model 'GPT-4' with examples grounded
in the use case of fostering heart-healthy physical activity in outpatient
cardiac rehabilitation. Three personas and five sets of context information per
persona were used as a basis of triggering and personalizing JITAIs.
Subsequently, we generated a total of 450 proposed JITAI decisions and message
content, divided equally into JITAIs generated by 10 iterations with GPT-4, a
baseline provided by 10 laypersons (LayPs), and a gold standard set by 10
healthcare professionals (HCPs). Ratings from 27 LayPs indicated that JITAIs
generated by GPT-4 were superior to those by HCPs and LayPs over all assessed
scales: i.e., appropriateness, engagement, effectiveness, and professionality.
This study indicates that LLMs have significant potential for implementing
JITAIs as a building block of personalized or ""precision"" health, offering
scalability, effective personalization based on opportunistically sampled
information, and good acceptability.",http://arxiv.org/abs/2402.08658v1,92,20,20,18,18,16
UGMAE: A Unified Framework for Graph Masked Autoencoders,"Generative self-supervised learning on graphs, particularly graph masked
autoencoders, has emerged as a popular learning paradigm and demonstrated its
efficacy in handling non-Euclidean data. However, several remaining issues
limit the capability of existing methods: 1) the disregard of uneven node
significance in masking, 2) the underutilization of holistic graph information,
3) the ignorance of semantic knowledge in the representation space due to the
exclusive use of reconstruction loss in the output space, and 4) the unstable
reconstructions caused by the large volume of masked contents. In light of
this, we propose UGMAE, a unified framework for graph masked autoencoders to
address these issues from the perspectives of adaptivity, integrity,
complementarity, and consistency. Specifically, we first develop an adaptive
feature mask generator to account for the unique significance of nodes and
sample informative masks (adaptivity). We then design a ranking-based structure
reconstruction objective joint with feature reconstruction to capture holistic
graph information and emphasize the topological proximity between neighbors
(integrity). After that, we present a bootstrapping-based similarity module to
encode the high-level semantic knowledge in the representation space,
complementary to the low-level reconstruction in the output space
(complementarity). Finally, we build a consistency assurance module to provide
reconstruction objectives with extra stabilized consistency targets
(consistency). Extensive experiments demonstrate that UGMAE outperforms both
contrastive and generative state-of-the-art baselines on several tasks across
multiple datasets.",http://arxiv.org/abs/2402.08023v1,92,18,20,18,18,18
AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts,"To improve language models' proficiency in mathematical reasoning via
continual pretraining, we introduce a novel strategy that leverages base
language models for autonomous data selection. Departing from conventional
supervised fine-tuning or trained classifiers with human-annotated data, our
approach utilizes meta-prompted language models as zero-shot verifiers to
autonomously evaluate and select high-quality mathematical content, and we
release the curated open-source AutoMathText dataset encompassing over 200GB of
data. To demonstrate the efficacy of our method, we continuously pretrained a
7B-parameter Mistral language model on the AutoMathText dataset, achieving
substantial improvements in downstream performance on the MATH dataset with a
token amount reduced by orders of magnitude compared to previous continuous
pretraining works. Our method showcases a 2 times increase in pretraining token
efficiency compared to baselines, underscoring the potential of our approach in
enhancing models' mathematical reasoning capabilities. The AutoMathText dataset
is available at https://huggingface.co/datasets/math-ai/AutoMathText. The code
is available at https://github.com/yifanzhang-pro/AutoMathText.",http://arxiv.org/abs/2402.07625v1,92,20,20,18,18,16
"Learning Emergent Gaits with Decentralized Phase Oscillators: on the role of Observations, Rewards, and Feedback","We present a minimal phase oscillator model for learning quadrupedal
locomotion. Each of the four oscillators is coupled only to itself and its
corresponding leg through local feedback of the ground reaction force, which
can be interpreted as an observer feedback gain. We interpret the oscillator
itself as a latent contact state-estimator. Through a systematic ablation
study, we show that the combination of phase observations, simple phase-based
rewards, and the local feedback dynamics induces policies that exhibit emergent
gait preferences, while using a reduced set of simple rewards, and without
prescribing a specific gait. The code is open-source, and a video synopsis
available at https://youtu.be/1NKQ0rSV3jU.",http://arxiv.org/abs/2402.08662v1,92,18,20,20,16,18
Nearest Neighbour Score Estimators for Diffusion Generative Models,"Score function estimation is the cornerstone of both training and sampling
from diffusion generative models. Despite this fact, the most commonly used
estimators are either biased neural network approximations or high variance
Monte Carlo estimators based on the conditional score. We introduce a novel
nearest neighbour score function estimator which utilizes multiple samples from
the training set to dramatically decrease estimator variance. We leverage our
low variance estimator in two compelling applications. Training consistency
models with our estimator, we report a significant increase in both convergence
speed and sample quality. In diffusion models, we show that our estimator can
replace a learned network for probability-flow ODE integration, opening
promising new avenues of future research.",http://arxiv.org/abs/2402.08018v1,92,20,18,19,17,18
Human Curriculum Effects Emerge with In-Context Learning in Neural Networks,"Human learning is sensitive to rule-like structure and the curriculum of
examples used for training. In tasks governed by succinct rules, learning is
more robust when related examples are blocked across trials, but in the absence
of such rules, interleaving is more effective. To date, no neural model has
simultaneously captured these seemingly contradictory effects. Here we show
that this same tradeoff spontaneously emerges with ""in-context learning"" (ICL)
both in neural networks trained with metalearning and in large language models
(LLMs). ICL is the ability to learn new tasks ""in context"" - without weight
changes - via an inner-loop algorithm implemented in activation dynamics.
Experiments with pretrained LLMs and metalearning transformers show that ICL
exhibits the blocking advantage demonstrated in humans on a task involving
rule-like structure, and conversely, that concurrent in-weight learning
reproduces the interleaving advantage observed in humans on tasks lacking such
structure.",http://arxiv.org/abs/2402.08674v1,92,20,18,20,18,16
COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability,"Jailbreaks on Large language models (LLMs) have recently received increasing
attention. For a comprehensive assessment of LLM safety, it is essential to
consider jailbreaks with diverse attributes, such as contextual coherence and
sentiment/stylistic variations, and hence it is beneficial to study
controllable jailbreaking, i.e. how to enforce control on LLM attacks. In this
paper, we formally formulate the controllable attack generation problem, and
build a novel connection between this problem and controllable text generation,
a well-explored topic of natural language processing. Based on this connection,
we adapt the Energy-based Constrained Decoding with Langevin Dynamics (COLD), a
state-of-the-art, highly efficient algorithm in controllable text generation,
and introduce the COLD-Attack framework which unifies and automates the search
of adversarial LLM attacks under a variety of control requirements such as
fluency, stealthiness, sentiment, and left-right-coherence. The controllability
enabled by COLD-Attack leads to diverse new jailbreak scenarios which not only
cover the standard setting of generating fluent suffix attacks, but also allow
us to address new controllable attack settings such as revising a user query
adversarially with minimal paraphrasing, and inserting stealthy attacks in
context with left-right-coherence. Our extensive experiments on various LLMs
(Llama-2, Mistral, Vicuna, Guanaco, GPT-3.5) show COLD-Attack's broad
applicability, strong controllability, high success rate, and attack
transferability. Our code is available at
https://github.com/Yu-Fangxu/COLD-Attack.",http://arxiv.org/abs/2402.08679v1,92,20,20,18,16,18
Developing a Multi-variate Prediction Model For COVID-19 From Crowd-sourced Respiratory Voice Data,"COVID-19 has affected more than 223 countries worldwide and in the Post-COVID
Era, there is a pressing need for non-invasive, low-cost, and highly scalable
solutions to detect COVID-19. We develop a deep learning model to identify
COVID-19 from voice recording data. The novelty of this work is in the
development of deep learning models for COVID-19 identification from only voice
recordings. We use the Cambridge COVID-19 Sound database which contains 893
speech samples, crowd-sourced from 4352 participants via a COVID-19 Sounds app.
Voice features including Mel-spectrograms and Mel-frequency cepstral
coefficients (MFCC) and CNN Encoder features are extracted. Based on the voice
data, we develop deep learning classification models to detect COVID-19 cases.
These models include Long Short-Term Memory (LSTM) and Convolutional Neural
Network (CNN) and Hidden-Unit BERT (HuBERT). We compare their predictive power
to baseline machine learning models. HuBERT achieves the highest accuracy of
86\% and the highest AUC of 0.93. The results achieved with the proposed models
suggest promising results in COVID-19 diagnosis from voice recordings when
compared to the results obtained from the state-of-the-art.",http://arxiv.org/abs/2402.07619v1,92,20,18,18,18,18
Universal link predictor by In-context Learning,"Link prediction is a crucial task in graph machine learning, where the goal
is to infer missing or future links within a graph. Traditional approaches
leverage heuristic methods based on widely observed connectivity patterns,
offering broad applicability and generalizability without the need for model
training. Despite their utility, these methods are limited by their reliance on
human-derived heuristics and lack the adaptability of data-driven approaches.
Conversely, parametric link predictors excel in automatically learning the
connectivity patterns from data and achieving state-of-the-art but fail short
to directly transfer across different graphs. Instead, it requires the cost of
extensive training and hyperparameter optimization to adapt to the target
graph. In this work, we introduce the Universal Link Predictor (UniLP), a novel
model that combines the generalizability of heuristic approaches with the
pattern learning capabilities of parametric models. UniLP is designed to
autonomously identify connectivity patterns across diverse graphs, ready for
immediate application to any unseen graph dataset without targeted training. We
address the challenge of conflicting connectivity patterns-arising from the
unique distributions of different graphs-through the implementation of
In-context Learning (ICL). This approach allows UniLP to dynamically adjust to
various target graphs based on contextual demonstrations, thereby avoiding
negative transfer. Through rigorous experimentation, we demonstrate UniLP's
effectiveness in adapting to new, unseen graphs at test time, showcasing its
ability to perform comparably or even outperform parametric models that have
been finetuned for specific datasets. Our findings highlight UniLP's potential
to set a new standard in link prediction, combining the strengths of heuristic
and parametric methods in a single, versatile framework.",http://arxiv.org/abs/2402.07738v1,92,18,18,18,20,18
FAST: Factorizable Attention for Speeding up Transformers,"Motivated by the factorization inherent in the original fast multipole method
and the improved fast Gauss transform we introduce a factorable form of
attention that operates efficiently in high dimensions. This approach reduces
the computational and memory complexity of the attention mechanism in
transformers from $O(N^2)$ to $O(N)$. In comparison to previous attempts, our
work presents a linearly scaled attention mechanism that maintains the full
representation of the attention matrix without compromising on sparsification
and incorporates the all-to-all relationship between tokens. We explore the
properties of our new attention metric and conduct tests in various standard
settings. Results indicate that our attention mechanism has a robust
performance and holds significant promise for diverse applications where
self-attention is used.",http://arxiv.org/abs/2402.07901v1,92,20,18,18,18,18
BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models,"Adapting state-of-the-art Large Language Models (LLMs) like GPT-4 and Gemini
for specific tasks is challenging. Due to the opacity in their parameters,
embeddings, and even output probabilities, existing fine-tuning adaptation
methods are inapplicable. Consequently, adapting these black-box LLMs is only
possible through their API services, raising concerns about transparency,
privacy, and cost. To address these challenges, we introduce BBox-Adapter, a
novel lightweight adapter for black-box LLMs. BBox-Adapter distinguishes target
and source domain data by treating target data as positive and source data as
negative. It employs a ranking-based Noise Contrastive Estimation (NCE) loss to
promote the likelihood of target domain data while penalizing that of the
source domain. Furthermore, it features an online adaptation mechanism, which
incorporates real-time positive data sampling from ground-truth, human, or AI
feedback, coupled with negative data from previous adaptations. Extensive
experiments demonstrate BBox-Adapter's effectiveness and cost efficiency. It
improves model performance by up to 6.77% across diverse tasks and domains,
while reducing training and inference costs by 31.30x and 1.84x, respectively.",http://arxiv.org/abs/2402.08219v1,92,20,20,20,16,16
FedLPS: Heterogeneous Federated Learning for Multiple Tasks with Local Parameter Sharing,"Federated Learning (FL) has emerged as a promising solution in Edge Computing
(EC) environments to process the proliferation of data generated by edge
devices. By collaboratively optimizing the global machine learning models on
distributed edge devices, FL circumvents the need for transmitting raw data and
enhances user privacy. Despite practical successes, FL still confronts
significant challenges including constrained edge device resources, multiple
tasks deployment, and data heterogeneity. However, existing studies focus on
mitigating the FL training costs of each single task whereas neglecting the
resource consumption across multiple tasks in heterogeneous FL scenarios. In
this paper, we propose Heterogeneous Federated Learning with Local Parameter
Sharing (FedLPS) to fill this gap. FedLPS leverages principles from transfer
learning to facilitate the deployment of multiple tasks on a single device by
dividing the local model into a shareable encoder and task-specific encoders.
To further reduce resource consumption, a channel-wise model pruning algorithm
that shrinks the footprint of local models while accounting for both data and
system heterogeneity is employed in FedLPS. Additionally, a novel heterogeneous
model aggregation algorithm is proposed to aggregate the heterogeneous
predictors in FedLPS. We implemented the proposed FedLPS on a real FL platform
and compared it with state-of-the-art (SOTA) FL frameworks. The experimental
results on five popular datasets and two modern DNN models illustrate that the
proposed FedLPS significantly outperforms the SOTA FL frameworks by up to 4.88%
and reduces the computational resource consumption by 21.3%. Our code is
available at:https://github.com/jyzgh/FedLPS.",http://arxiv.org/abs/2402.08578v1,92,19,19,18,18,18
ChatCell: Facilitating Single-Cell Analysis with Natural Language,"As Large Language Models (LLMs) rapidly evolve, their influence in science is
becoming increasingly prominent. The emerging capabilities of LLMs in task
generalization and free-form dialogue can significantly advance fields like
chemistry and biology. However, the field of single-cell biology, which forms
the foundational building blocks of living organisms, still faces several
challenges. High knowledge barriers and limited scalability in current methods
restrict the full exploitation of LLMs in mastering single-cell data, impeding
direct accessibility and rapid iteration. To this end, we introduce ChatCell,
which signifies a paradigm shift by facilitating single-cell analysis with
natural language. Leveraging vocabulary adaptation and unified sequence
generation, ChatCell has acquired profound expertise in single-cell biology and
the capability to accommodate a diverse range of analysis tasks. Extensive
experiments further demonstrate ChatCell's robust performance and potential to
deepen single-cell insights, paving the way for more accessible and intuitive
exploration in this pivotal field. Our project homepage is available at
https://zjunlp.github.io/project/ChatCell.",http://arxiv.org/abs/2402.08303v1,92,20,20,18,18,16
Learning Neural Contracting Dynamics: Extended Linearization and Global Guarantees,"Global stability and robustness guarantees in learned dynamical systems are
essential to ensure well-behavedness of the systems in the face of uncertainty.
We present Extended Linearized Contracting Dynamics (ELCD), the first neural
network-based dynamical system with global contractivity guarantees in
arbitrary metrics. The key feature of ELCD is a parametrization of the extended
linearization of the nonlinear vector field. In its most basic form, ELCD is
guaranteed to be (i) globally exponentially stable, (ii) equilibrium
contracting, and (iii) globally contracting with respect to some metric. To
allow for contraction with respect to more general metrics in the data space,
we train diffeomorphisms between the data space and a latent space and enforce
contractivity in the latent space, which ensures global contractivity in the
data space. We demonstrate the performance of ELCD on the $2$D, $4$D, and $8$D
LASA datasets.",http://arxiv.org/abs/2402.08090v1,92,20,18,20,18,16
Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast,"A multimodal large language model (MLLM) agent can receive instructions,
capture images, retrieve histories from memory, and decide which tools to use.
Nonetheless, red-teaming efforts have revealed that adversarial images/prompts
can jailbreak an MLLM and cause unaligned behaviors. In this work, we report an
even more severe safety issue in multi-agent environments, referred to as
infectious jailbreak. It entails the adversary simply jailbreaking a single
agent, and without any further intervention from the adversary, (almost) all
agents will become infected exponentially fast and exhibit harmful behaviors.
To validate the feasibility of infectious jailbreak, we simulate multi-agent
environments containing up to one million LLaVA-1.5 agents, and employ
randomized pair-wise chat as a proof-of-concept instantiation for multi-agent
interaction. Our results show that feeding an (infectious) adversarial image
into the memory of any randomly chosen agent is sufficient to achieve
infectious jailbreak. Finally, we derive a simple principle for determining
whether a defense mechanism can provably restrain the spread of infectious
jailbreak, but how to design a practical defense that meets this principle
remains an open question to investigate. Our project page is available at
https://sail-sg.github.io/Agent-Smith/.",http://arxiv.org/abs/2402.08567v1,92,20,20,18,18,16
Online Foundation Model Selection in Robotics,"Foundation models have recently expanded into robotics after excelling in
computer vision and natural language processing. The models are accessible in
two ways: open-source or paid, closed-source options. Users with access to both
face a problem when deciding between effective yet costly closed-source models
and free but less powerful open-source alternatives. We call it the model
selection problem. Existing supervised-learning methods are impractical due to
the high cost of collecting extensive training data from closed-source models.
Hence, we focus on the online learning setting where algorithms learn while
collecting data, eliminating the need for large pre-collected datasets. We thus
formulate a user-centric online model selection problem and propose a novel
solution that combines an open-source encoder to output context and an online
learning algorithm that processes this context. The encoder distills vast data
distributions into low-dimensional features, i.e., the context, without
additional training. The online learning algorithm aims to maximize a composite
reward that includes model performance, execution time, and costs based on the
context extracted from the data. It results in an improved trade-off between
selecting open-source and closed-source models compared to non-contextual
methods, as validated by our theoretical analysis. Experiments across
language-based robotic tasks such as Waymo Open Dataset, ALFRED, and Open
X-Embodiment demonstrate real-world applications of the solution. The results
show that the solution significantly improves the task success rate by up to
14%.",http://arxiv.org/abs/2402.08570v1,92,19,18,20,18,17
Unsupervised Sign Language Translation and Generation,"Motivated by the success of unsupervised neural machine translation (UNMT),
we introduce an unsupervised sign language translation and generation network
(USLNet), which learns from abundant single-modality (text and video) data
without parallel sign language data. USLNet comprises two main components:
single-modality reconstruction modules (text and video) that rebuild the input
from its noisy version in the same modality and cross-modality back-translation
modules (text-video-text and video-text-video) that reconstruct the input from
its noisy version in the different modality using back-translation
procedure.Unlike the single-modality back-translation procedure in text-based
UNMT, USLNet faces the cross-modality discrepancy in feature representation, in
which the length and the feature dimension mismatch between text and video
sequences. We propose a sliding window method to address the issues of aligning
variable-length text with video sequences. To our knowledge, USLNet is the
first unsupervised sign language translation and generation model capable of
generating both natural language text and sign language video in a unified
manner. Experimental results on the BBC-Oxford Sign Language dataset (BOBSL)
and Open-Domain American Sign Language dataset (OpenASL) reveal that USLNet
achieves competitive results compared to supervised baseline models, indicating
its effectiveness in sign language translation and generation.",http://arxiv.org/abs/2402.07726v1,92,20,18,18,18,18
Out-of-Distribution Detection and Data Drift Monitoring using Statistical Process Control,"Background: Machine learning (ML) methods often fail with data that deviates
from their training distribution. This is a significant concern for ML-enabled
devices in clinical settings, where data drift may cause unexpected performance
that jeopardizes patient safety.
  Method: We propose a ML-enabled Statistical Process Control (SPC) framework
for out-of-distribution (OOD) detection and drift monitoring. SPC is
advantageous as it visually and statistically highlights deviations from the
expected distribution. To demonstrate the utility of the proposed framework for
monitoring data drift in radiological images, we investigated different design
choices, including methods for extracting feature representations, drift
quantification, and SPC parameter selection.
  Results: We demonstrate the effectiveness of our framework for two tasks: 1)
differentiating axial vs. non-axial computed tomography (CT) images and 2)
separating chest x-ray (CXR) from other modalities. For both tasks, we achieved
high accuracy in detecting OOD inputs, with 0.913 in CT and 0.995 in CXR, and
sensitivity of 0.980 in CT and 0.984 in CXR. Our framework was also adept at
monitoring data streams and identifying the time a drift occurred. In a
simulation with 100 daily CXR cases, we detected a drift in OOD input
percentage from 0-1% to 3-5% within two days, maintaining a low false-positive
rate. Through additional experimental results, we demonstrate the framework's
data-agnostic nature and independence from the underlying model's structure.
  Conclusion: We propose a framework for OOD detection and drift monitoring
that is agnostic to data, modality, and model. The framework is customizable
and can be adapted for specific applications.",http://arxiv.org/abs/2402.08088v1,92,18,20,18,18,18
Test-Time Backdoor Attacks on Multimodal Large Language Models,"Backdoor attacks are commonly executed by contaminating training data, such
that a trigger can activate predetermined harmful effects during the test
phase. In this work, we present AnyDoor, a test-time backdoor attack against
multimodal large language models (MLLMs), which involves injecting the backdoor
into the textual modality using adversarial test images (sharing the same
universal perturbation), without requiring access to or modification of the
training data. AnyDoor employs similar techniques used in universal adversarial
attacks, but distinguishes itself by its ability to decouple the timing of
setup and activation of harmful effects. In our experiments, we validate the
effectiveness of AnyDoor against popular MLLMs such as LLaVA-1.5, MiniGPT-4,
InstructBLIP, and BLIP-2, as well as provide comprehensive ablation studies.
Notably, because the backdoor is injected by a universal perturbation, AnyDoor
can dynamically change its backdoor trigger prompts/harmful effects, exposing a
new challenge for defending against backdoor attacks. Our project page is
available at https://sail-sg.github.io/AnyDoor/.",http://arxiv.org/abs/2402.08577v1,92,18,20,20,18,16
AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy,"Large language models (LLMs) show impressive capabilities, matching and
sometimes exceeding human performance in many domains. This study explores the
potential of LLMs to augment judgement in forecasting tasks. We evaluated the
impact on forecasting accuracy of two GPT-4-Turbo assistants: one designed to
provide high-quality advice ('superforecasting'), and the other designed to be
overconfident and base-rate-neglecting. Participants (N = 991) had the option
to consult their assigned LLM assistant throughout the study, in contrast to a
control group that used a less advanced model (DaVinci-003) without direct
forecasting support. Our preregistered analyses reveal that LLM augmentation
significantly enhances forecasting accuracy by 23% across both types of
assistants, compared to the control group. This improvement occurs despite the
superforecasting assistant's higher accuracy in predictions, indicating the
augmentation's benefit is not solely due to model prediction accuracy.
Exploratory analyses showed a pronounced effect in one forecasting item,
without which we find that the superforecasting assistant increased accuracy by
43%, compared with 28% for the biased assistant. We further examine whether LLM
augmentation disproportionately benefits less skilled forecasters, degrades the
wisdom-of-the-crowd by reducing prediction diversity, or varies in
effectiveness with question difficulty. Our findings do not consistently
support these hypotheses. Our results suggest that access to an LLM assistant,
even a biased one, can be a helpful decision aid in cognitively demanding tasks
where the answer is not known at the time of interaction.",http://arxiv.org/abs/2402.07862v1,92,18,19,20,18,17
THE COLOSSEUM: A Benchmark for Evaluating Generalization for Robotic Manipulation,"To realize effective large-scale, real-world robotic applications, we must
evaluate how well our robot policies adapt to changes in environmental
conditions. Unfortunately, a majority of studies evaluate robot performance in
environments closely resembling or even identical to the training setup. We
present THE COLOSSEUM, a novel simulation benchmark, with 20 diverse
manipulation tasks, that enables systematical evaluation of models across 12
axes of environmental perturbations. These perturbations include changes in
color, texture, and size of objects, table-tops, and backgrounds; we also vary
lighting, distractors, and camera pose. Using THE COLOSSEUM, we compare 4
state-of-the-art manipulation models to reveal that their success rate degrades
between 30-50% across these perturbation factors. When multiple perturbations
are applied in unison, the success rate degrades $\geq$75%. We identify that
changing the number of distractor objects, target object color, or lighting
conditions are the perturbations that reduce model performance the most. To
verify the ecological validity of our results, we show that our results in
simulation are correlated ($\bar{R}^2 = 0.614$) to similar perturbations in
real-world experiments. We open source code for others to use THE COLOSSEUM,
and also release code to 3D print the objects used to replicate the real-world
perturbations. Ultimately, we hope that THE COLOSSEUM will serve as a benchmark
to identify modeling decisions that systematically improve generalization for
manipulation. See https://robot-colosseum.github.io/ for more details.",http://arxiv.org/abs/2402.08191v1,92,19,20,19,17,17
FESS Loss: Feature-Enhanced Spatial Segmentation Loss for Optimizing Medical Image Analysis,"Medical image segmentation is a critical process in the field of medical
imaging, playing a pivotal role in diagnosis, treatment, and research. It
involves partitioning of an image into multiple regions, representing distinct
anatomical or pathological structures. Conventional methods often grapple with
the challenge of balancing spatial precision and comprehensive feature
representation due to their reliance on traditional loss functions. To overcome
this, we propose Feature-Enhanced Spatial Segmentation Loss (FESS Loss), that
integrates the benefits of contrastive learning (which extracts intricate
features, particularly in the nuanced domain of medical imaging) with the
spatial accuracy inherent in the Dice loss. The objective is to augment both
spatial precision and feature-based representation in the segmentation of
medical images. FESS Loss signifies a notable advancement, offering a more
accurate and refined segmentation process, ultimately contributing to
heightened precision in the analysis of medical images. Further, FESS loss
demonstrates superior performance in limited annotated data availability
scenarios often present in the medical domain.",http://arxiv.org/abs/2402.08582v1,92,18,19,19,18,18
Graph Feature Preprocessor: Real-time Extraction of Subgraph-based Features from Transaction Graphs,"In this paper, we present ""Graph Feature Preprocessor"", a software library
for detecting typical money laundering and fraud patterns in financial
transaction graphs in real time. These patterns are used to produce a rich set
of transaction features for downstream machine learning training and inference
tasks such as money laundering detection. We show that our enriched transaction
features dramatically improve the prediction accuracy of
gradient-boosting-based machine learning models. Our library exploits multicore
parallelism, maintains a dynamic in-memory graph, and efficiently mines
subgraph patterns in the incoming transaction stream, which enables it to be
operated in a streaming manner. We evaluate our library using highly-imbalanced
synthetic anti-money laundering (AML) and real-life Ethereum phishing datasets.
In these datasets, the proportion of illicit transactions is very small, which
makes the learning process challenging. Our solution, which combines our Graph
Feature Preprocessor and gradient-boosting-based machine learning models, is
able to detect these illicit transactions with higher minority-class F1 scores
than standard graph neural networks. In addition, the end-to-end throughput
rate of our solution executed on a multicore CPU outperforms the graph neural
network baselines executed on a powerful V100 GPU. Overall, the combination of
high accuracy, a high throughput rate, and low latency of our solution
demonstrates the practical value of our library in real-world applications.
Graph Feature Preprocessor has been integrated into IBM mainframe software
products, namely ""IBM Cloud Pak for Data on Z"" and ""AI Toolkit for IBM Z and
LinuxONE"".",http://arxiv.org/abs/2402.08593v1,92,19,18,19,18,18
Text-centric Alignment for Multi-Modality Learning,"This research paper addresses the challenge of modality mismatch in
multimodal learning, where the modalities available during inference differ
from those available at training. We propose the Text-centric Alignment for
Multi-Modality Learning (TAMML) approach, an innovative method that utilizes
Large Language Models (LLMs) with in-context learning and foundation models to
enhance the generalizability of multimodal systems under these conditions. By
leveraging the unique properties of text as a unified semantic space, TAMML
demonstrates significant improvements in handling unseen, diverse, and
unpredictable modality combinations. TAMML not only adapts to varying
modalities but also maintains robust performance, showcasing the potential of
foundation models in overcoming the limitations of traditional fixed-modality
frameworks in embedding representations. This study contributes to the field by
offering a flexible, effective solution for real-world applications where
modality availability is dynamic and uncertain.",http://arxiv.org/abs/2402.08086v1,92,20,19,19,18,16
Homomorphism Counts for Graph Neural Networks: All About That Basis,"Graph neural networks are architectures for learning invariant functions over
graphs. A large body of work has investigated the properties of graph neural
networks and identified several limitations, particularly pertaining to their
expressive power. Their inability to count certain patterns (e.g., cycles) in a
graph lies at the heart of such limitations, since many functions to be learned
rely on the ability of counting such patterns. Two prominent paradigms aim to
address this limitation by enriching the graph features with subgraph or
homomorphism pattern counts. In this work, we show that both of these
approaches are sub-optimal in a certain sense and argue for a more fine-grained
approach, which incorporates the homomorphism counts of all structures in the
""basis"" of the target pattern. This yields strictly more expressive
architectures without incurring any additional overhead in terms of
computational complexity compared to existing approaches. We prove a series of
theoretical results on node-level and graph-level motif parameters and
empirically validate them on standard benchmark datasets.",http://arxiv.org/abs/2402.08595v1,92,20,19,19,17,17
Message Detouring: A Simple Yet Effective Cycle Representation for Expressive Graph Learning,"Graph learning is crucial in the fields of bioinformatics, social networks,
and chemicals. Although high-order graphlets, such as cycles, are critical to
achieving an informative graph representation for node classification, edge
prediction, and graph recognition, modeling high-order topological
characteristics poses significant computational challenges, restricting its
widespread applications in machine learning. To address this limitation, we
introduce the concept of \textit{message detouring} to hierarchically
characterize cycle representation throughout the entire graph, which
capitalizes on the contrast between the shortest and longest pathways within a
range of local topologies associated with each graph node. The topological
feature representations derived from our message detouring landscape
demonstrate comparable expressive power to high-order
\textit{Weisfeiler-Lehman} (WL) tests but much less computational demands. In
addition to the integration with graph kernel and message passing neural
networks, we present a novel message detouring neural network, which uses
Transformer backbone to integrate cycle representations across nodes and edges.
Aside from theoretical results, experimental results on expressiveness, graph
classification, and node classification show message detouring can
significantly outperform current counterpart approaches on various benchmark
datasets.",http://arxiv.org/abs/2402.08085v1,92,19,20,19,18,16
CycPUF: Cyclic Physical Unclonable Function,"Physical Unclonable Functions (PUFs) leverage manufacturing process
imperfections that cause propagation delay discrepancies for the signals
traveling along these paths. While PUFs can be used for device authentication
and chip-specific key generation, strong PUFs have been shown to be vulnerable
to machine learning modeling attacks. Although there is an impression that
combinational circuits must be designed without any loops, cyclic combinational
circuits have been shown to increase design security against hardware
intellectual property theft. In this paper, we introduce feedback signals into
traditional delay-based PUF designs such as arbiter PUF, ring oscillator PUF,
and butterfly PUF to give them a wider range of possible output behaviors and
thus an edge against modeling attacks. Based on our analysis, cyclic PUFs
produce responses that can be binary, steady-state, oscillating, or
pseudo-random under fixed challenges. The proposed cyclic PUFs are implemented
in field programmable gate arrays, and their power and area overhead, in
addition to functional metrics, are reported compared with their traditional
counterparts. The security gain of the proposed cyclic PUFs is also shown
against state-of-the-art attacks.",http://arxiv.org/abs/2402.08084v1,92,20,18,18,18,18
ReNeLiB: Real-time Neural Listening Behavior Generation for Socially Interactive Agents,"Flexible and natural nonverbal reactions to human behavior remain a challenge
for socially interactive agents (SIAs) that are predominantly animated using
hand-crafted rules. While recently proposed machine learning based approaches
to conversational behavior generation are a promising way to address this
challenge, they have not yet been employed in SIAs. The primary reason for this
is the lack of a software toolkit integrating such approaches with SIA
frameworks that conforms to the challenging real-time requirements of
human-agent interaction scenarios. In our work, we for the first time present
such a toolkit consisting of three main components: (1) real-time feature
extraction capturing multi-modal social cues from the user; (2) behavior
generation based on a recent state-of-the-art neural network approach; (3)
visualization of the generated behavior supporting both FLAME-based and Apple
ARKit-based interactive agents. We comprehensively evaluate the real-time
performance of the whole framework and its components. In addition, we
introduce pre-trained behavioral generation models derived from psychotherapy
sessions for domain-specific listening behaviors. Our software toolkit, pivotal
for deploying and assessing SIAs' listening behavior in real-time, is publicly
available. Resources, including code, behavioural multi-modal features
extracted from therapeutic interactions, are hosted at
https://daksitha.github.io/ReNeLib",http://dx.doi.org/10.1145/3577190.3614133,92,18,20,18,18,18
Large Language Models as Agents in Two-Player Games,"By formally defining the training processes of large language models (LLMs),
which usually encompasses pre-training, supervised fine-tuning, and
reinforcement learning with human feedback, within a single and unified machine
learning paradigm, we can glean pivotal insights for advancing LLM
technologies. This position paper delineates the parallels between the training
methods of LLMs and the strategies employed for the development of agents in
two-player games, as studied in game theory, reinforcement learning, and
multi-agent systems. We propose a re-conceptualization of LLM learning
processes in terms of agent learning in language-based games. This framework
unveils innovative perspectives on the successes and challenges in LLM
development, offering a fresh understanding of addressing alignment issues
among other strategic considerations. Furthermore, our two-player game approach
sheds light on novel data preparation and machine learning techniques for
training LLMs.",http://arxiv.org/abs/2402.08078v1,92,19,19,19,17,18
CaPS: Collaborative and Private Synthetic Data Generation from Distributed Sources,"Data is the lifeblood of the modern world, forming a fundamental part of AI,
decision-making, and research advances. With increase in interest in data,
governments have taken important steps towards a regulated data world,
drastically impacting data sharing and data usability and resulting in massive
amounts of data confined within the walls of organizations. While synthetic
data generation (SDG) is an appealing solution to break down these walls and
enable data sharing, the main drawback of existing solutions is the assumption
of a trusted aggregator for generative model training. Given that many data
holders may not want to, or be legally allowed to, entrust a central entity
with their raw data, we propose a framework for the collaborative and private
generation of synthetic tabular data from distributed data holders. Our
solution is general, applicable to any marginal-based SDG, and provides input
privacy by replacing the trusted aggregator with secure multi-party computation
(MPC) protocols and output privacy via differential privacy (DP). We
demonstrate the applicability and scalability of our approach for the
state-of-the-art select-measure-generate SDG algorithms MWEM+PGM and AIM.",http://arxiv.org/abs/2402.08614v1,92,18,20,18,18,18
Revealing Multimodal Contrastive Representation Learning through Latent Partial Causal Models,"Multimodal contrastive representation learning methods have proven successful
across a range of domains, partly due to their ability to generate meaningful
shared representations of complex phenomena. To enhance the depth of analysis
and understanding of these acquired representations, we introduce a unified
causal model specifically designed for multimodal data. By examining this
model, we show that multimodal contrastive representation learning excels at
identifying latent coupled variables within the proposed unified model, up to
linear or permutation transformations resulting from different assumptions. Our
findings illuminate the potential of pre-trained multimodal models, eg, CLIP,
in learning disentangled representations through a surprisingly simple yet
highly effective tool: linear independent component analysis. Experiments
demonstrate the robustness of our findings, even when the assumptions are
violated, and validate the effectiveness of the proposed method in learning
disentangled representations.",http://arxiv.org/abs/2402.06223v1,90,18,19,19,17,17
"The Generative AI Paradox on Evaluation: What It Can Solve, It May Not Evaluate","This paper explores the assumption that Large Language Models (LLMs) skilled
in generation tasks are equally adept as evaluators. We assess the performance
of three LLMs and one open-source LM in Question-Answering (QA) and evaluation
tasks using the TriviaQA (Joshi et al., 2017) dataset. Results indicate a
significant disparity, with LLMs exhibiting lower performance in evaluation
tasks compared to generation tasks. Intriguingly, we discover instances of
unfaithful evaluation where models accurately evaluate answers in areas where
they lack competence, underscoring the need to examine the faithfulness and
trustworthiness of LLMs as evaluators. This study contributes to the
understanding of ""the Generative AI Paradox"" (West et al., 2023), highlighting
a need to explore the correlation between generative excellence and evaluation
proficiency, and the necessity to scrutinize the faithfulness aspect in model
evaluations.",http://arxiv.org/abs/2402.06204v1,90,18,20,18,18,16
GS-CLIP: Gaussian Splatting for Contrastive Language-Image-3D Pretraining from Real-World Data,"3D Shape represented as point cloud has achieve advancements in multimodal
pre-training to align image and language descriptions, which is curial to
object identification, classification, and retrieval. However, the discrete
representations of point cloud lost the object's surface shape information and
creates a gap between rendering results and 2D correspondences. To address this
problem, we propose GS-CLIP for the first attempt to introduce 3DGS (3D
Gaussian Splatting) into multimodal pre-training to enhance 3D representation.
GS-CLIP leverages a pre-trained vision-language model for a learned common
visual and textual space on massive real world image-text pairs and then learns
a 3D Encoder for aligning 3DGS optimized per object. Additionally, a novel
Gaussian-Aware Fusion is proposed to extract and fuse global explicit feature.
As a general framework for language-image-3D pre-training, GS-CLIP is agnostic
to 3D backbone networks. Experiments on challenging shows that GS-CLIP
significantly improves the state-of-the-art, outperforming the previously best
results.",http://arxiv.org/abs/2402.06198v2,90,19,18,19,17,17
Insomnia Identification via Electroencephalography,"Insomnia is a serious sleep disorder caused by abnormal or excessive neural
activity in the brain. An estimated 50 million people worldwide are thought to
be affected by this condition, which is the second most severe neurological
disease after stroke. In order to ensure a quick recovery, an early and
accurate diagnosis of insomnia enables more effective drug and treatment
administration. This study proposes a method that uses deep learning to
automatically identify patients with insomnia. A set of optimal features are
extracted from spectral and temporal domains, including the relative power of
{\sigma}, \b{eta} and {\gamma} bands, the total power, the absolute slow wave
power, the power ratios of {\theta}, {\alpha}, {\gamma}, \b{eta},
{\theta}/{\alpha}, {\theta}/\b{eta}, {\alpha}/{\gamma} and {\alpha}/\b{eta},
mean, zero crossing rate, mobility, complexity, sleep efficiency and total
sleep time, to accurately quantify the differences between insomnia patients
and healthy subjects and develops a 1D CNN model for the classification
process. With the experiments use Fp2 and C4 EEG channels with 50 insomnia
patients and 50 healthy subjects, the proposed model arrives 99.34% accuracy
without sleep stage annotation. Using the features only from a single channel,
the study proposes a smart solution for insomnia patients which allows machine
learning to be to simplify current sleep monitoring hardware and improve
in-home ambulatory monitoring.",http://arxiv.org/abs/2402.06251v1,90,18,18,20,20,14
Transformer Mechanisms Mimic Frontostriatal Gating Operations When Trained on Human Working Memory Tasks,"Models based on the Transformer neural network architecture have seen success
on a wide variety of tasks that appear to require complex ""cognitive branching""
-- or the ability to maintain pursuit of one goal while accomplishing others.
In cognitive neuroscience, success on such tasks is thought to rely on
sophisticated frontostriatal mechanisms for selective \textit{gating}, which
enable role-addressable updating -- and later readout -- of information to and
from distinct ""addresses"" of memory, in the form of clusters of neurons.
However, Transformer models have no such mechanisms intentionally built-in. It
is thus an open question how Transformers solve such tasks, and whether the
mechanisms that emerge to help them to do so bear any resemblance to the gating
mechanisms in the human brain. In this work, we analyze the mechanisms that
emerge within a vanilla attention-only Transformer trained on a simple sequence
modeling task inspired by a task explicitly designed to study working memory
gating in computational cognitive neuroscience. We find that, as a result of
training, the self-attention mechanism within the Transformer specializes in a
way that mirrors the input and output gating mechanisms which were explicitly
incorporated into earlier, more biologically-inspired architectures. These
results suggest opportunities for future research on computational similarities
between modern AI architectures and models of the human brain.",http://arxiv.org/abs/2402.08211v1,90,18,20,18,18,16
Anchor-based Large Language Models,"Large language models (LLMs) predominantly employ decoder-only transformer
architectures, necessitating the retention of keys/values information for
historical tokens to provide contextual information and avoid redundant
computation. However, the substantial size and parameter volume of these LLMs
require massive GPU memory. This memory demand increases with the length of the
input text, leading to an urgent need for more efficient methods of information
storage and processing. This study introduces the Anchor-based LLM (AnLLM),
which utilizes an innovative anchor-based self-attention network (AnSAN) and
also an anchor-based inference strategy. This approach enables LLMs to compress
sequence information into an anchor token, reducing the keys/values cache and
enhancing inference efficiency. Experiments show that the AnLLM maintains
comparable accuracy with up to 99% keys/values cache reduction and up to 3.5
times faster inference. Despite a minor compromise in accuracy, the AnLLM
significantly improves computational efficiency and resource utilization,
demonstrating the potential of the anchor-based attention approach in the
context of LLMs for real-time inference in practical applications.",http://arxiv.org/abs/2402.07616v1,90,18,18,20,18,16
Near-Minimax-Optimal Distributional Reinforcement Learning with a Generative Model,"We propose a new algorithm for model-based distributional reinforcement
learning (RL), and prove that it is minimax-optimal for approximating return
distributions with a generative model (up to logarithmic factors), resolving an
open question of Zhang et al. (2023). Our analysis provides new theoretical
results on categorical approaches to distributional RL, and also introduces a
new distributional Bellman equation, the stochastic categorical CDF Bellman
equation, which we expect to be of independent interest. We also provide an
experimental study comparing several model-based distributional RL algorithms,
with several takeaways for practitioners.",http://arxiv.org/abs/2402.07598v1,90,19,17,19,16,19
Enabling Multi-Agent Transfer Reinforcement Learning via Scenario Independent Representation,"Multi-Agent Reinforcement Learning (MARL) algorithms are widely adopted in
tackling complex tasks that require collaboration and competition among agents
in dynamic Multi-Agent Systems (MAS). However, learning such tasks from scratch
is arduous and may not always be feasible, particularly for MASs with a large
number of interactive agents due to the extensive sample complexity. Therefore,
reusing knowledge gained from past experiences or other agents could
efficiently accelerate the learning process and upscale MARL algorithms. In
this study, we introduce a novel framework that enables transfer learning for
MARL through unifying various state spaces into fixed-size inputs that allow
one unified deep-learning policy viable in different scenarios within a MAS. We
evaluated our approach in a range of scenarios within the StarCraft Multi-Agent
Challenge (SMAC) environment, and the findings show significant enhancements in
multi-agent learning performance using maneuvering skills learned from other
scenarios compared to agents learning from scratch. Furthermore, we adopted
Curriculum Transfer Learning (CTL), enabling our deep learning policy to
progressively acquire knowledge and skills across pre-designed homogeneous
learning scenarios organized by difficulty levels. This process promotes inter-
and intra-agent knowledge transfer, leading to high multi-agent learning
performance in more complicated heterogeneous scenarios.",http://dx.doi.org/10.1109/CoG57401.2023.10333236,90,18,19,18,16,19
Modelling Human Values for AI Reasoning,"One of today's most significant societal challenges is building AI systems
whose behaviour, or the behaviour it enables within communities of interacting
agents (human and artificial), aligns with human values. To address this
challenge, we detail a formal model of human values for their explicit
computational representation. To our knowledge, this has not been attempted as
yet, which is surprising given the growing volume of research integrating
values within AI. Taking as our starting point the wealth of research
investigating the nature of human values from social psychology over the last
few decades, we set out to provide such a formal model. We show how this model
can provide the foundational apparatus for AI-based reasoning over values, and
demonstrate its applicability in real-world use cases. We illustrate how our
model captures the key ideas from social psychology research and propose a
roadmap for future integrated, and interdisciplinary, research into human
values in AI. The ability to automatically reason over values not only helps
address the value alignment problem but also facilitates the design of AI
systems that can support individuals and communities in making more informed,
value-aligned decisions. More and more, individuals and organisations are
motivated to understand their values more explicitly and explore whether their
behaviours and attitudes properly reflect them. Our work on modelling human
values will enable AI systems to be designed and deployed to meet this growing
need.",http://arxiv.org/abs/2402.06359v1,90,20,18,20,16,16
Generating Chain-of-Thoughts with a Direct Pairwise-Comparison Approach to Searching for the Most Promising Intermediate Thought,"To improve the ability of the large language model (LLMs) to handle complex
reasoning problems, chain-of-thoughts (CoT) methods were proposed to guide LLMs
to reason step-by-step, facilitating problem solving from simple to complex
tasks. State-of-the-art approaches for generating such a chain involve
interactive collaboration, where the learner generates candidate intermediate
thoughts, evaluated by the LLM, guiding the generation of subsequent thoughts.
However, a widespread yet understudied problem is that the evaluation from the
LLM is typically noisy and unreliable, potentially misleading the generation
process in selecting promising intermediate thoughts. In this paper, motivated
by Vapnik's principle, we propose a novel comparison-based CoT generation
algorithm that directly identifies the most promising thoughts with the noisy
feedback from the LLM. In each round, we randomly pair intermediate thoughts
and directly prompt the LLM to select the more promising one from each pair,
allowing us to identify the most promising thoughts through an iterative
process. To further model the noise in the comparison, we resort to the
techniques of ensemble and dueling bandits and propose two variants of the
proposed algorithm. Experiments on three real-world mathematical and reasoning
tasks demonstrate the effectiveness of our proposed algorithm and verify the
rationale of the direct pairwise comparison.",http://arxiv.org/abs/2402.06918v1,90,18,19,18,18,17
Non-autoregressive Generative Models for Reranking Recommendation,"In a multi-stage recommendation system, reranking plays a crucial role by
modeling the intra-list correlations among items.The key challenge of reranking
lies in the exploration of optimal sequences within the combinatorial space of
permutations. Recent research proposes a generator-evaluator learning paradigm,
where the generator generates multiple feasible sequences and the evaluator
picks out the best sequence based on the estimated listwise score. Generator is
of vital importance, and generative models are well-suited for the generator
function. Current generative models employ an autoregressive strategy for
sequence generation. However, deploying autoregressive models in real-time
industrial systems is challenging. Hence, we propose a Non-AutoRegressive
generative model for reranking Recommendation (NAR4Rec) designed to enhance
efficiency and effectiveness. To address challenges related to sparse training
samples and dynamic candidates impacting model convergence, we introduce a
matching model. Considering the diverse nature of user feedback, we propose a
sequence-level unlikelihood training objective to distinguish feasible from
unfeasible sequences. Additionally, to overcome the lack of dependency modeling
in non-autoregressive models regarding target items, we introduce contrastive
decoding to capture correlations among these items. Extensive offline
experiments on publicly available datasets validate the superior performance of
our proposed approach compared to the existing state-of-the-art reranking
methods. Furthermore, our method has been fully deployed in a popular video app
Kuaishou with over 300 million daily active users, significantly enhancing
online recommendation quality, and demonstrating the effectiveness and
efficiency of our approach.",http://arxiv.org/abs/2402.06871v1,90,20,18,18,18,16
NavFormer: A Transformer Architecture for Robot Target-Driven Navigation in Unknown and Dynamic Environments,"In unknown cluttered and dynamic environments such as disaster scenes, mobile
robots need to perform target-driven navigation in order to find people or
objects of interest, while being solely guided by images of the targets. In
this paper, we introduce NavFormer, a novel end-to-end transformer architecture
developed for robot target-driven navigation in unknown and dynamic
environments. NavFormer leverages the strengths of both 1) transformers for
sequential data processing and 2) self-supervised learning (SSL) for visual
representation to reason about spatial layouts and to perform
collision-avoidance in dynamic settings. The architecture uniquely combines
dual-visual encoders consisting of a static encoder for extracting invariant
environment features for spatial reasoning, and a general encoder for dynamic
obstacle avoidance. The primary robot navigation task is decomposed into two
sub-tasks for training: single robot exploration and multi-robot collision
avoidance. We perform cross-task training to enable the transfer of learned
skills to the complex primary navigation task without the need for
task-specific fine-tuning. Simulated experiments demonstrate that NavFormer can
effectively navigate a mobile robot in diverse unknown environments,
outperforming existing state-of-the-art methods in terms of success rate and
success weighted by (normalized inverse) path length. Furthermore, a
comprehensive ablation study is performed to evaluate the impact of the main
design choices of the structure and training of NavFormer, further validating
their effectiveness in the overall system.",http://arxiv.org/abs/2402.06838v1,90,18,19,18,17,18
Estimating Player Performance in Different Contexts Using Fine-tuned Large Events Models,"This paper introduces an innovative application of Large Event Models (LEMs),
akin to Large Language Models, to the domain of soccer analytics. By learning
the ""language"" of soccer - predicting variables for subsequent events rather
than words LEMs facilitate the simulation of matches and offer various
applications, including player performance prediction across different team
contexts. We focus on fine-tuning LEMs with the WyScout dataset for the
2017-2018 Premier League season to derive specific insights into player
contributions and team strategies. Our methodology involves adapting these
models to reflect the nuanced dynamics of soccer, enabling the evaluation of
hypothetical transfers. Our findings confirm the effectiveness and limitations
of LEMs in soccer analytics, highlighting the model's capability to forecast
teams' expected standings and explore high-profile scenarios, such as the
potential effects of transferring Cristiano Ronaldo or Lionel Messi to
different teams in the Premier League. This analysis underscores the importance
of context in evaluating player quality. While general metrics may suggest
significant differences between players, contextual analyses reveal narrower
gaps in performance within specific team frameworks.",http://arxiv.org/abs/2402.06815v1,90,18,20,18,18,16
A Kalman Filter Based Framework for Monitoring the Performance of In-Hospital Mortality Prediction Models Over Time,"Unlike in a clinical trial, where researchers get to determine the least
number of positive and negative samples required, or in a machine learning
study where the size and the class distribution of the validation set is static
and known, in a real-world scenario, there is little control over the size and
distribution of incoming patients. As a result, when measured during different
time periods, evaluation metrics like Area under the Receiver Operating Curve
(AUCROC) and Area Under the Precision-Recall Curve(AUCPR) may not be directly
comparable. Therefore, in this study, for binary classifiers running in a long
time period, we proposed to adjust these performance metrics for sample size
and class distribution, so that a fair comparison can be made between two time
periods. Note that the number of samples and the class distribution, namely the
ratio of positive samples, are two robustness factors which affect the variance
of AUCROC. To better estimate the mean of performance metrics and understand
the change of performance over time, we propose a Kalman filter based framework
with extrapolated variance adjusted for the total number of samples and the
number of positive samples during different time periods. The efficacy of this
method is demonstrated first on a synthetic dataset and then retrospectively
applied to a 2-days ahead in-hospital mortality prediction model for COVID-19
patients during 2021 and 2022. Further, we conclude that our prediction model
is not significantly affected by the evolution of the disease, improved
treatments and changes in hospital operational plans.",http://arxiv.org/abs/2402.06812v1,90,18,19,19,17,17
Multi-Attribute Vision Transformers are Efficient and Robust Learners,"Since their inception, Vision Transformers (ViTs) have emerged as a
compelling alternative to Convolutional Neural Networks (CNNs) across a wide
spectrum of tasks. ViTs exhibit notable characteristics, including global
attention, resilience against occlusions, and adaptability to distribution
shifts. One underexplored aspect of ViTs is their potential for multi-attribute
learning, referring to their ability to simultaneously grasp multiple
attribute-related tasks. In this paper, we delve into the multi-attribute
learning capability of ViTs, presenting a straightforward yet effective
strategy for training various attributes through a single ViT network as
distinct tasks. We assess the resilience of multi-attribute ViTs against
adversarial attacks and compare their performance against ViTs designed for
single attributes. Moreover, we further evaluate the robustness of
multi-attribute ViTs against a recent transformer based attack called
Patch-Fool. Our empirical findings on the CelebA dataset provide validation for
our assertion.",http://arxiv.org/abs/2402.08070v1,90,18,20,18,18,16
BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data,"We introduce a text-to-speech (TTS) model called BASE TTS, which stands for
$\textbf{B}$ig $\textbf{A}$daptive $\textbf{S}$treamable TTS with
$\textbf{E}$mergent abilities. BASE TTS is the largest TTS model to-date,
trained on 100K hours of public domain speech data, achieving a new
state-of-the-art in speech naturalness. It deploys a 1-billion-parameter
autoregressive Transformer that converts raw texts into discrete codes
(""speechcodes"") followed by a convolution-based decoder which converts these
speechcodes into waveforms in an incremental, streamable manner. Further, our
speechcodes are built using a novel speech tokenization technique that features
speaker ID disentanglement and compression with byte-pair encoding. Echoing the
widely-reported ""emergent abilities"" of large language models when trained on
increasing volume of data, we show that BASE TTS variants built with 10K+ hours
and 500M+ parameters begin to demonstrate natural prosody on textually complex
sentences. We design and share a specialized dataset to measure these emergent
abilities for text-to-speech. We showcase state-of-the-art naturalness of BASE
TTS by evaluating against baselines that include publicly available large-scale
text-to-speech systems: YourTTS, Bark and TortoiseTTS. Audio samples generated
by the model can be heard at https://amazon-ltts-paper.com/.",http://arxiv.org/abs/2402.08093v1,90,18,20,18,16,18
On the Out-Of-Distribution Generalization of Multimodal Large Language Models,"We investigate the generalization boundaries of current Multimodal Large
Language Models (MLLMs) via comprehensive evaluation under out-of-distribution
scenarios and domain-specific tasks. We evaluate their zero-shot generalization
across synthetic images, real-world distributional shifts, and specialized
datasets like medical and molecular imagery. Empirical results indicate that
MLLMs struggle with generalization beyond common training domains, limiting
their direct application without adaptation. To understand the cause of
unreliable performance, we analyze three hypotheses: semantic
misinterpretation, visual feature extraction insufficiency, and mapping
deficiency. Results identify mapping deficiency as the primary hurdle. To
address this problem, we show that in-context learning (ICL) can significantly
enhance MLLMs' generalization, opening new avenues for overcoming
generalization barriers. We further explore the robustness of ICL under
distribution shifts and show its vulnerability to domain shifts, label shifts,
and spurious correlation shifts between in-context examples and test data.",http://arxiv.org/abs/2402.06599v1,90,19,18,18,17,18
Predictive representations: building blocks of intelligence,"Adaptive behavior often requires predicting future events. The theory of
reinforcement learning prescribes what kinds of predictive representations are
useful and how to compute them. This paper integrates these theoretical ideas
with work on cognition and neuroscience. We pay special attention to the
successor representation (SR) and its generalizations, which have been widely
applied both as engineering tools and models of brain function. This
convergence suggests that particular kinds of predictive representations may
function as versatile building blocks of intelligence.",http://arxiv.org/abs/2402.06590v1,90,18,20,18,18,16
More than the Sum of Its Parts: Ensembling Backbone Networks for Few-Shot Segmentation,"Semantic segmentation is a key prerequisite to robust image understanding for
applications in \acrlong{ai} and Robotics. \acrlong{fss}, in particular,
concerns the extension and optimization of traditional segmentation methods in
challenging conditions where limited training examples are available. A
predominant approach in \acrlong{fss} is to rely on a single backbone for
visual feature extraction. Choosing which backbone to leverage is a deciding
factor contributing to the overall performance. In this work, we interrogate on
whether fusing features from different backbones can improve the ability of
\acrlong{fss} models to capture richer visual features. To tackle this
question, we propose and compare two ensembling techniques-Independent Voting
and Feature Fusion. Among the available \acrlong{fss} methods, we implement the
proposed ensembling techniques on PANet. The module dedicated to predicting
segmentation masks from the backbone embeddings in PANet avoids trainable
parameters, creating a controlled `in vitro' setting for isolating the impact
of different ensembling strategies. Leveraging the complementary strengths of
different backbones, our approach outperforms the original single-backbone
PANet across standard benchmarks even in challenging one-shot learning
scenarios. Specifically, it achieved a performance improvement of +7.37\% on
PASCAL-5\textsuperscript{i} and of +10.68\% on COCO-20\textsuperscript{i} in
the top-performing scenario where three backbones are combined. These results,
together with the qualitative inspection of the predicted subject masks,
suggest that relying on multiple backbones in PANet leads to a more
comprehensive feature representation, thus expediting the successful
application of \acrlong{fss} methods in challenging, data-scarce environments.",http://arxiv.org/abs/2402.06581v1,90,18,18,18,20,16
SAE: Single Architecture Ensemble Neural Networks,"Ensembles of separate neural networks (NNs) have shown superior accuracy and
confidence calibration over single NN across tasks. Recent methods compress
ensembles within a single network via early exits or multi-input multi-output
frameworks. However, the landscape of these methods is fragmented thus far,
making it difficult to choose the right approach for a given task. Furthermore,
the algorithmic performance of these methods is behind the ensemble of separate
NNs and requires extensive architecture tuning. We propose a novel methodology
unifying these approaches into a Single Architecture Ensemble (SAE). Our method
learns the optimal number and depth of exits per ensemble input in a single NN.
This enables the SAE framework to flexibly tailor its configuration for a given
architecture or application. We evaluate SAEs on image classification and
regression across various network architecture types and sizes. We demonstrate
competitive accuracy or confidence calibration to baselines while reducing the
compute operations or parameter count by up to $1.5{\sim}3.7\times$.",http://arxiv.org/abs/2402.06580v1,90,18,18,20,18,16
On the Universality of Coupling-based Normalizing Flows,"We present a novel theoretical framework for understanding the expressive
power of coupling-based normalizing flows such as RealNVP. Despite their
prevalence in scientific applications, a comprehensive understanding of
coupling flows remains elusive due to their restricted architectures. Existing
theorems fall short as they require the use of arbitrarily ill-conditioned
neural networks, limiting practical applicability. Additionally, we demonstrate
that these constructions inherently lead to volume-preserving flows, a property
which we show to be a fundamental constraint for expressivity. We propose a new
distributional universality theorem for coupling-based normalizing flows, which
overcomes several limitations of prior work. Our results support the general
wisdom that the coupling architecture is expressive and provide a nuanced view
for choosing the expressivity of coupling functions, bridging a gap between
empirical results and theoretical understanding.",http://arxiv.org/abs/2402.06578v1,90,20,18,18,18,16
Diffusion-ES: Gradient-free Planning with Diffusion for Autonomous Driving and Zero-Shot Instruction Following,"Diffusion models excel at modeling complex and multimodal trajectory
distributions for decision-making and control. Reward-gradient guided denoising
has been recently proposed to generate trajectories that maximize both a
differentiable reward function and the likelihood under the data distribution
captured by a diffusion model. Reward-gradient guided denoising requires a
differentiable reward function fitted to both clean and noised samples,
limiting its applicability as a general trajectory optimizer. In this paper, we
propose DiffusionES, a method that combines gradient-free optimization with
trajectory denoising to optimize black-box non-differentiable objectives while
staying in the data manifold. Diffusion-ES samples trajectories during
evolutionary search from a diffusion model and scores them using a black-box
reward function. It mutates high-scoring trajectories using a truncated
diffusion process that applies a small number of noising and denoising steps,
allowing for much more efficient exploration of the solution space. We show
that DiffusionES achieves state-of-the-art performance on nuPlan, an
established closed-loop planning benchmark for autonomous driving. Diffusion-ES
outperforms existing sampling-based planners, reactive deterministic or
diffusion-based policies, and reward-gradient guidance. Additionally, we show
that unlike prior guidance methods, our method can optimize non-differentiable
language-shaped reward functions generated by few-shot LLM prompting. When
guided by a human teacher that issues instructions to follow, our method can
generate novel, highly complex behaviors, such as aggressive lane weaving,
which are not present in the training data. This allows us to solve the hardest
nuPlan scenarios which are beyond the capabilities of existing trajectory
optimization methods and driving policies.",http://arxiv.org/abs/2402.06559v1,90,19,18,18,17,18
A Universal Non-Parametric Approach For Improved Molecular Sequence Analysis,"In the field of biological research, it is essential to comprehend the
characteristics and functions of molecular sequences. The classification of
molecular sequences has seen widespread use of neural network-based techniques.
Despite their astounding accuracy, these models often require a substantial
number of parameters and more data collection. In this work, we present a novel
approach based on the compression-based Model, motivated from
\cite{jiang2023low}, which combines the simplicity of basic compression
algorithms like Gzip and Bz2, with Normalized Compression Distance (NCD)
algorithm to achieve better performance on classification tasks without relying
on handcrafted features or pre-trained models. Firstly, we compress the
molecular sequence using well-known compression algorithms, such as Gzip and
Bz2. By leveraging the latent structure encoded in compressed files, we compute
the Normalized Compression Distance between each pair of molecular sequences,
which is derived from the Kolmogorov complexity. This gives us a distance
matrix, which is the input for generating a kernel matrix using a Gaussian
kernel. Next, we employ kernel Principal Component Analysis (PCA) to get the
vector representations for the corresponding molecular sequence, capturing
important structural and functional information. The resulting vector
representations provide an efficient yet effective solution for molecular
sequence analysis and can be used in ML-based downstream tasks. The proposed
approach eliminates the need for computationally intensive Deep Neural Networks
(DNNs), with their large parameter counts and data requirements. Instead, it
leverages a lightweight and universally accessible compression-based model.",http://arxiv.org/abs/2402.08117v1,90,18,18,20,18,16
Calibrating Long-form Generations from Large Language Models,"To enhance Large Language Models' (LLMs) reliability, calibration is
essential -- the model's assessed confidence scores should align with the
actual likelihood of its responses being correct. However, current confidence
elicitation methods and calibration metrics typically rely on a binary
true/false assessment of response correctness. This approach does not apply to
long-form generation, where an answer can be partially correct. Addressing this
gap, we introduce a unified calibration framework, in which both the
correctness of the LLMs' responses and their associated confidence levels are
treated as distributions across a range of scores. Within this framework, we
develop three metrics to precisely evaluate LLM calibration and further propose
two confidence elicitation methods based on self-consistency and
self-evaluation. Our experiments, which include long-form QA and summarization
tasks, demonstrate that larger models don't necessarily guarantee better
calibration, that calibration performance is found to be metric-dependent, and
that self-consistency methods excel in factoid datasets. We also find that
calibration can be enhanced through techniques such as fine-tuning, integrating
relevant source documents, scaling the temperature, and combining
self-consistency with self-evaluation. Lastly, we showcase a practical
application of our system: selecting and cascading open-source models and
ChatGPT to optimize correctness given a limited API budget. This research not
only challenges existing notions of LLM calibration but also offers practical
methodologies for improving trustworthiness in long-form generation.",http://arxiv.org/abs/2402.06544v1,90,18,20,18,16,18
Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty,"Large language models (LLMs) exhibit advanced reasoning skills, enabling
robots to comprehend natural language instructions and strategically plan
high-level actions through proper grounding. However, LLM hallucination may
result in robots confidently executing plans that are misaligned with user
goals or, in extreme cases, unsafe. Additionally, inherent ambiguity in natural
language instructions can induce task uncertainty, particularly in situations
where multiple valid options exist. To address this issue, LLMs must identify
such uncertainty and proactively seek clarification. This paper explores the
concept of introspective planning as a systematic method for guiding LLMs in
forming uncertainty--aware plans for robotic task execution without the need
for fine-tuning. We investigate uncertainty quantification in task-level robot
planning and demonstrate that introspection significantly improves both success
rates and safety compared to state-of-the-art LLM-based planning approaches.
Furthermore, we assess the effectiveness of introspective planning in
conjunction with conformal prediction, revealing that this combination yields
tighter confidence bounds, thereby maintaining statistical success guarantees
with fewer superfluous user clarification queries.",http://arxiv.org/abs/2402.06529v1,90,18,20,18,18,16
Multimodal Clinical Trial Outcome Prediction with Large Language Models,"The clinical trial is a pivotal and costly process, often spanning multiple
years and requiring substantial financial resources. Therefore, the development
of clinical trial outcome prediction models aims to exclude drugs likely to
fail and holds the potential for significant cost savings. Recent data-driven
attempts leverage deep learning methods to integrate multimodal data for
predicting clinical trial outcomes. However, these approaches rely on manually
designed modal-specific encoders, which limits both the extensibility to adapt
new modalities and the ability to discern similar information patterns across
different modalities. To address these issues, we propose a multimodal
mixture-of-experts (LIFTED) approach for clinical trial outcome prediction.
Specifically, LIFTED unifies different modality data by transforming them into
natural language descriptions. Then, LIFTED constructs unified noise-resilient
encoders to extract information from modal-specific language descriptions.
Subsequently, a sparse Mixture-of-Experts framework is employed to further
refine the representations, enabling LIFTED to identify similar information
patterns across different modalities and extract more consistent
representations from those patterns using the same expert model. Finally, a
mixture-of-experts module is further employed to dynamically integrate
different modality representations for prediction, which gives LIFTED the
ability to automatically weigh different modalities and pay more attention to
critical information. The experiments demonstrate that LIFTED significantly
enhances performance in predicting clinical trial outcomes across all three
phases compared to the best baseline, showcasing the effectiveness of our
proposed key components.",http://arxiv.org/abs/2402.06512v1,90,18,20,18,18,16
Where is the Truth? The Risk of Getting Confounded in a Continual World,"A dataset is confounded if it is most easily solved via a spurious
correlation which fails to generalize to new data. We will show that, in a
continual learning setting where confounders may vary in time across tasks, the
resulting challenge far exceeds the standard forgetting problem normally
considered. In particular, we derive mathematically the effect of such
confounders on the space of valid joint solutions to sets of confounded tasks.
Interestingly, our theory predicts that for many such continual datasets,
spurious correlations are easily ignored when the tasks are trained on jointly,
but it is far harder to avoid confounding when they are considered
sequentially. We construct such a dataset and demonstrate empirically that
standard continual learning methods fail to ignore confounders, while training
jointly on all tasks is successful. Our continually confounded dataset, ConCon,
is based on CLEVR images and demonstrates the need for continual learning
methods with more robust behavior with respect to confounding.",http://arxiv.org/abs/2402.06434v1,90,20,18,18,18,16
Verified Multi-Step Synthesis using Large Language Models and Monte Carlo Tree Search,"We present an approach using Monte Carlo Tree Search (MCTS) to guide Large
Language Models (LLMs) to generate verified programs in Dafny, Lean and Coq.
Our method, which we call VMCTS, leverages the verifier inside the search
algorithm by checking partial programs at each step. In combination with the
LLM prior, the verifier feedback raises the synthesis capabilities of open
source models. On a set of five verified programming problems, we find that in
four problems where the base model cannot solve the question even when
re-sampling solutions for one hour, VMCTS can solve the problems within 6
minutes. The base model with VMCTS is even competitive with ChatGPT4 augmented
with plugins and multiple re-tries on these problems. Our code and benchmarks
are available at
https://github.com/namin/llm-verified-with-monte-carlo-tree-search .",http://arxiv.org/abs/2402.08147v1,90,18,20,18,18,16
ImplicitDeepfake: Plausible Face-Swapping through Implicit Deepfake Generation using NeRF and Gaussian Splatting,"Numerous emerging deep-learning techniques have had a substantial impact on
computer graphics. Among the most promising breakthroughs are the recent rise
of Neural Radiance Fields (NeRFs) and Gaussian Splatting (GS). NeRFs encode the
object's shape and color in neural network weights using a handful of images
with known camera positions to generate novel views. In contrast, GS provides
accelerated training and inference without a decrease in rendering quality by
encoding the object's characteristics in a collection of Gaussian
distributions. These two techniques have found many use cases in spatial
computing and other domains. On the other hand, the emergence of deepfake
methods has sparked considerable controversy. Such techniques can have a form
of artificial intelligence-generated videos that closely mimic authentic
footage. Using generative models, they can modify facial features, enabling the
creation of altered identities or facial expressions that exhibit a remarkably
realistic appearance to a real person. Despite these controversies, deepfake
can offer a next-generation solution for avatar creation and gaming when of
desirable quality. To that end, we show how to combine all these emerging
technologies to obtain a more plausible outcome. Our ImplicitDeepfake1 uses the
classical deepfake algorithm to modify all training images separately and then
train NeRF and GS on modified faces. Such relatively simple strategies can
produce plausible 3D deepfake-based avatars.",http://arxiv.org/abs/2402.06390v1,90,18,18,20,18,16
TEE4EHR: Transformer Event Encoder for Better Representation Learning in Electronic Health Records,"Irregular sampling of time series in electronic health records (EHRs) is one
of the main challenges for developing machine learning models. Additionally,
the pattern of missing data in certain clinical variables is not at random but
depends on the decisions of clinicians and the state of the patient. Point
process is a mathematical framework for analyzing event sequence data that is
consistent with irregular sampling patterns. Our model, TEE4EHR, is a
transformer event encoder (TEE) with point process loss that encodes the
pattern of laboratory tests in EHRs. The utility of our TEE has been
investigated in a variety of benchmark event sequence datasets. Additionally,
we conduct experiments on two real-world EHR databases to provide a more
comprehensive evaluation of our model. Firstly, in a self-supervised learning
approach, the TEE is jointly learned with an existing attention-based deep
neural network which gives superior performance in negative log-likelihood and
future event prediction. Besides, we propose an algorithm for aggregating
attention weights that can reveal the interaction between the events. Secondly,
we transfer and freeze the learned TEE to the downstream task for the outcome
prediction, where it outperforms state-of-the-art models for handling
irregularly sampled time series. Furthermore, our results demonstrate that our
approach can improve representation learning in EHRs and can be useful for
clinical prediction tasks.",http://arxiv.org/abs/2402.06367v1,90,17,18,18,19,18
Unmasking honey adulteration : a breakthrough in quality assurance through cutting-edge convolutional neural network analysis of thermal images,"Honey, a natural product generated from organic sources, is widely recognized
for its revered reputation. Nevertheless, honey is susceptible to adulteration,
a situation that has substantial consequences for both the well-being of the
general population and the financial well-being of a country. Conventional
approaches for detecting honey adulteration are often associated with extensive
time requirements and restricted sensitivity. This paper presents a novel
approach to address the aforementioned issue by employing Convolutional Neural
Networks (CNNs) for the classification of honey samples based on thermal
images. The use of thermal imaging technique offers a significant advantage in
detecting adulterants, as it can reveal differences in temperature in honey
samples caused by variations in sugar composition, moisture levels, and other
substances used for adulteration. To establish a meticulous approach to
categorizing honey, a thorough dataset comprising thermal images of authentic
and tainted honey samples was collected. Several state-of-the-art Convolutional
Neural Network (CNN) models were trained and optimized using the dataset that
was gathered. Within this set of models, there exist pre-trained models such as
InceptionV3, Xception, VGG19, and ResNet that have exhibited exceptional
performance, achieving classification accuracies ranging from 88% to 98%.
Furthermore, we have implemented a more streamlined and less complex
convolutional neural network (CNN) model, outperforming comparable models with
an outstanding accuracy rate of 99%. This simplification offers not only the
sole advantage of the model, but it also concurrently offers a more efficient
solution in terms of resources and time. This approach offers a viable way to
implement quality control measures in the honey business, so guaranteeing the
genuineness and safety of this valuable organic commodity.",http://arxiv.org/abs/2402.08122v1,90,20,18,18,18,16
SpiRit-LM: Interleaved Spoken and Written Language Model,"We introduce SPIRIT-LM, a foundation multimodal language model that freely
mixes text and speech. Our model is based on a pretrained text language model
that we extend to the speech modality by continuously training it on text and
speech units. Speech and text sequences are concatenated as a single set of
tokens, and trained with a word-level interleaving method using a small
automatically-curated speech-text parallel corpus. SPIRIT-LM comes in two
versions: a BASE version that uses speech semantic units and an EXPRESSIVE
version that models expressivity using pitch and style units in addition to the
semantic units. For both versions, the text is encoded with subword BPE tokens.
The resulting model displays both the semantic abilities of text models and the
expressive abilities of speech models. Additionally, we demonstrate that
SPIRIT-LM is able to learn new tasks in a few-shot fashion across modalities
(i.e. ASR, TTS, Speech Classification).",http://arxiv.org/abs/2402.05755v1,90,19,19,18,17,17
AI Assistance for UX: A Literature Review Through Human-Centered AI,"Recent advancements in HCI and AI research attempt to support user experience
(UX) practitioners with AI-enabled tools. Despite the potential of emerging
models and new interaction mechanisms, mainstream adoption of such tools
remains limited. We took the lens of Human-Centered AI and presented a
systematic literature review of 359 papers, aiming to synthesize the current
landscape, identify trends, and uncover UX practitioners' unmet needs in AI
support. Guided by the Double Diamond design framework, our analysis uncovered
that UX practitioners' unique focuses on empathy building and experiences
across UI screens are often overlooked. Simplistic AI automation can obstruct
the valuable empathy-building process. Furthermore, focusing solely on
individual UI screens without considering interactions and user flows reduces
the system's practical value for UX designers. Based on these findings, we call
for a deeper understanding of UX mindsets and more designer-centric datasets
and evaluation metrics, for HCI and AI communities to collaboratively work
toward effective AI support for UX.",http://arxiv.org/abs/2402.06089v2,90,18,20,18,18,16
Learning Uncertainty-Aware Temporally-Extended Actions,"In reinforcement learning, temporal abstraction in the action space,
exemplified by action repetition, is a technique to facilitate policy learning
through extended actions. However, a primary limitation in previous studies of
action repetition is its potential to degrade performance, particularly when
sub-optimal actions are repeated. This issue often negates the advantages of
action repetition. To address this, we propose a novel algorithm named
Uncertainty-aware Temporal Extension (UTE). UTE employs ensemble methods to
accurately measure uncertainty during action extension. This feature allows
policies to strategically choose between emphasizing exploration or adopting an
uncertainty-averse approach, tailored to their specific needs. We demonstrate
the effectiveness of UTE through experiments in Gridworld and Atari 2600
environments. Our findings show that UTE outperforms existing action repetition
algorithms, effectively mitigating their inherent limitations and significantly
enhancing policy learning efficiency.",http://arxiv.org/abs/2402.05439v1,90,18,20,18,16,18
Task-customized Masked AutoEncoder via Mixture of Cluster-conditional Experts,"Masked Autoencoder~(MAE) is a prevailing self-supervised learning method that
achieves promising results in model pre-training. However, when the various
downstream tasks have data distributions different from the pre-training data,
the semantically irrelevant pre-training information might result in negative
transfer, impeding MAE's scalability. To address this issue, we propose a novel
MAE-based pre-training paradigm, Mixture of Cluster-conditional Experts (MoCE),
which can be trained once but provides customized pre-training models for
diverse downstream tasks. Different from the mixture of experts (MoE), our MoCE
trains each expert only with semantically relevant images by using
cluster-conditional gates. Thus, each downstream task can be allocated to its
customized model pre-trained with data most similar to the downstream data.
Experiments on a collection of 11 downstream tasks show that MoCE outperforms
the vanilla MAE by 2.45\% on average. It also obtains new state-of-the-art
self-supervised learning results on detection and segmentation.",http://arxiv.org/abs/2402.05382v1,90,19,18,19,17,17
Noise Contrastive Alignment of Language Models with Explicit Rewards,"User intentions are typically formalized as evaluation rewards to be
maximized when fine-tuning language models (LMs). Existing alignment methods,
such as Direct Preference Optimization (DPO), are mainly tailored for pairwise
preference data where rewards are implicitly defined rather than explicitly
given. In this paper, we introduce a general framework for LM alignment,
leveraging Noise Contrastive Estimation (NCE) to bridge the gap in handling
reward datasets explicitly annotated with scalar evaluations. Our framework
comprises two parallel algorithms, NCA and InfoNCA, both enabling the direct
extraction of an LM policy from reward data as well as preference data.
Notably, we show that the DPO loss is a special case of our proposed InfoNCA
objective under pairwise preference settings, thereby integrating and extending
current alignment theories. By contrasting NCA and InfoNCA, we show that
InfoNCA and DPO adjust relative likelihood across different responses to a
single instruction, while NCA optimizes absolute likelihood for each response.
We apply our methods to align a 7B language model with a GPT-4 annotated reward
dataset. Experimental results suggest that InfoNCA surpasses the DPO baseline
in GPT-4 evaluations, while NCA enjoys better training stability with
competitive performance.",http://arxiv.org/abs/2402.05369v1,90,18,18,20,18,16
Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving,"Foundation models, such as Large language Models (LLMs), have attracted
significant amount of interest due to their large number of applications.
Existing works show that appropriate prompt design, such as Chain-of-Thoughts,
can unlock LLM's powerful capacity in diverse areas. However, when handling
tasks involving repetitive sub-tasks and/or deceptive contents, such as
arithmetic calculation and article-level fake news detection, existing
prompting strategies either suffers from insufficient expressive power or
intermediate errors triggered by hallucination. To make LLM more discerning to
such intermediate errors, we propose to guide LLM with a Divide-and-Conquer
program that simultaneously ensures superior expressive power and disentangles
task decomposition, sub-task resolution, and resolution assembly process.
Theoretic analysis reveals that our strategy can guide LLM to extend the
expressive power of fixed-depth Transformer. Experiments indicate that our
proposed method can achieve better performance than typical prompting
strategies in tasks bothered by intermediate errors and deceptive contents,
such as large integer multiplication, hallucination detection and
misinformation detection.",http://arxiv.org/abs/2402.05359v1,90,18,20,18,16,18
Approximately Piecewise E(3) Equivariant Point Networks,"Integrating a notion of symmetry into point cloud neural networks is a
provably effective way to improve their generalization capability. Of
particular interest are $E(3)$ equivariant point cloud networks where Euclidean
transformations applied to the inputs are preserved in the outputs. Recent
efforts aim to extend networks that are $E(3)$ equivariant, to accommodate
inputs made of multiple parts, each of which exhibits local $E(3)$ symmetry. In
practical settings, however, the partitioning into individually transforming
regions is unknown a priori. Errors in the partition prediction would
unavoidably map to errors in respecting the true input symmetry. Past works
have proposed different ways to predict the partition, which may exhibit
uncontrolled errors in their ability to maintain equivariance to the actual
partition. To this end, we introduce APEN: a general framework for constructing
approximate piecewise-$E(3)$ equivariant point networks. Our primary insight is
that functions that are equivariant with respect to a finer partition will also
maintain equivariance in relation to the true partition. Leveraging this
observation, we propose a design where the equivariance approximation error at
each layers can be bounded solely in terms of (i) uncertainty quantification of
the partition prediction, and (ii) bounds on the probability of failing to
suggest a proper subpartition of the ground truth one. We demonstrate the
effectiveness of APEN using two data types exemplifying part-based symmetry:
(i) real-world scans of room scenes containing multiple furniture-type objects;
and, (ii) human motions, characterized by articulated parts exhibiting rigid
movement. Our empirical results demonstrate the advantage of integrating
piecewise $E(3)$ symmetry into network design, showing a distinct improvement
in generalization compared to prior works for both classification and
segmentation tasks.",http://arxiv.org/abs/2402.08529v1,90,18,18,18,18,18
A Distributional Analogue to the Successor Representation,"This paper contributes a new approach for distributional reinforcement
learning which elucidates a clean separation of transition structure and reward
in the learning process. Analogous to how the successor representation (SR)
describes the expected consequences of behaving according to a given policy,
our distributional successor measure (SM) describes the distributional
consequences of this behaviour. We formulate the distributional SM as a
distribution over distributions and provide theory connecting it with
distributional and model-based reinforcement learning. Moreover, we propose an
algorithm that learns the distributional SM from data by minimizing a two-level
maximum mean discrepancy. Key to our method are a number of algorithmic
techniques that are independently valuable for learning generative models of
state. As an illustration of the usefulness of the distributional SM, we show
that it enables zero-shot risk-sensitive policy evaluation in a way that was
not previously possible.",http://arxiv.org/abs/2402.08530v1,90,19,18,19,17,17
Domain-Agnostic Hardware Fingerprinting-Based Device Identifier for Zero-Trust IoT Security,"Next-generation networks aim for comprehensive connectivity, interconnecting
humans, machines, devices, and systems seamlessly. This interconnectivity
raises concerns about privacy and security, given the potential network-wide
impact of a single compromise. To address this challenge, the Zero Trust (ZT)
paradigm emerges as a key method for safeguarding network integrity and data
confidentiality. This work introduces EPS-CNN, a novel deep-learning-based
wireless device identification framework designed to serve as the device
authentication layer within the ZT architecture, with a focus on
resource-constrained IoT devices. At the core of EPS-CNN, a Convolutional
Neural Network (CNN) is utilized to generate the device identity from a unique
RF signal representation, known as the Double-Sided Envelope Power Spectrum
(EPS), which effectively captures the device-specific hardware characteristics
while ignoring device-unrelated information. Experimental evaluations show that
the proposed framework achieves over 99%, 93%, and 95% testing accuracy when
tested in same-domain (day, location, and channel), cross-day, and
cross-location scenarios, respectively. Our findings demonstrate the
superiority of the proposed framework in enhancing the accuracy, robustness,
and adaptability of deep learning-based methods, thus offering a pioneering
solution for enabling ZT IoT device identification.",http://arxiv.org/abs/2402.05332v1,90,18,20,18,18,16
Learning on Multimodal Graphs: A Survey,"Multimodal data pervades various domains, including healthcare, social media,
and transportation, where multimodal graphs play a pivotal role. Machine
learning on multimodal graphs, referred to as multimodal graph learning (MGL),
is essential for successful artificial intelligence (AI) applications. The
burgeoning research in this field encompasses diverse graph data types and
modalities, learning techniques, and application scenarios. This survey paper
conducts a comparative analysis of existing works in multimodal graph learning,
elucidating how multimodal learning is achieved across different graph types
and exploring the characteristics of prevalent learning techniques.
Additionally, we delineate significant applications of multimodal graph
learning and offer insights into future directions in this domain.
Consequently, this paper serves as a foundational resource for researchers
seeking to comprehend existing MGL techniques and their applicability across
diverse scenarios.",http://arxiv.org/abs/2402.05322v1,90,18,20,18,18,16
Navigating the Knowledge Sea: Planet-scale answer retrieval using LLMs,"Information retrieval is a rapidly evolving field of information retrieval,
which is characterized by a continuous refinement of techniques and
technologies, from basic hyperlink-based navigation to sophisticated
algorithm-driven search engines. This paper aims to provide a comprehensive
overview of the evolution of Information Retrieval Technology, with a
particular focus on the role of Large Language Models (LLMs) in bridging the
gap between traditional search methods and the emerging paradigm of answer
retrieval. The integration of LLMs in the realms of response retrieval and
indexing signifies a paradigm shift in how users interact with information
systems. This paradigm shift is driven by the integration of large language
models (LLMs) like GPT-4, which are capable of understanding and generating
human-like text, thus enabling them to provide more direct and contextually
relevant answers to user queries. Through this exploration, we seek to
illuminate the technological milestones that have shaped this journey and the
potential future directions in this rapidly changing field.",http://arxiv.org/abs/2402.05318v1,90,18,18,18,18,18
SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models,"In the rapidly evolving landscape of Large Language Models (LLMs), ensuring
robust safety measures is paramount. To meet this crucial need, we propose
\emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating
LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench
transcends conventional benchmarks through its large scale, rich diversity,
intricate taxonomy spanning three levels, and versatile
functionalities.SALAD-Bench is crafted with a meticulous array of questions,
from standard queries to complex ones enriched with attack, defense
modifications and multiple-choice. To effectively manage the inherent
complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for
QA pairs with a particular focus on attack-enhanced queries, ensuring a
seamless, and reliable evaluation. Above components extend SALAD-Bench from
standard LLM safety evaluation to both LLM attack and defense methods
evaluation, ensuring the joint-purpose utility. Our extensive experiments shed
light on the resilience of LLMs against emerging threats and the efficacy of
contemporary defense tactics. Data and evaluator are released under
https://github.com/OpenSafetyLab/SALAD-BENCH.",http://arxiv.org/abs/2402.05044v2,90,19,19,18,18,16
A Survey on Domain Generalization for Medical Image Analysis,"Medical Image Analysis (MedIA) has emerged as a crucial tool in
computer-aided diagnosis systems, particularly with the advancement of deep
learning (DL) in recent years. However, well-trained deep models often
experience significant performance degradation when deployed in different
medical sites, modalities, and sequences, known as a domain shift issue. In
light of this, Domain Generalization (DG) for MedIA aims to address the domain
shift challenge by generalizing effectively and performing robustly across
unknown data distributions. This paper presents the a comprehensive review of
substantial developments in this area. First, we provide a formal definition of
domain shift and domain generalization in medical field, and discuss several
related settings. Subsequently, we summarize the recent methods from three
viewpoints: data manipulation level, feature representation level, and model
training level, and present some algorithms in detail for each viewpoints.
Furthermore, we introduce the commonly used datasets. Finally, we summarize
existing literature and present some potential research topics for the future.
For this survey, we also created a GitHub project by collecting the supporting
resources, at the link: https://github.com/Ziwei-Niu/DG_for_MedIA",http://arxiv.org/abs/2402.05035v2,90,18,20,18,18,16
Fast Timing-Conditioned Latent Audio Diffusion,"Generating long-form 44.1kHz stereo audio from text prompts can be
computationally demanding. Further, most previous works do not tackle that
music and sound effects naturally vary in their duration. Our research focuses
on the efficient generation of long-form, variable-length stereo music and
sounds at 44.1kHz using text prompts with a generative model. Stable Audio is
based on latent diffusion, with its latent defined by a fully-convolutional
variational autoencoder. It is conditioned on text prompts as well as timing
embeddings, allowing for fine control over both the content and length of the
generated music and sounds. Stable Audio is capable of rendering stereo signals
of up to 95 sec at 44.1kHz in 8 sec on an A100 GPU. Despite its compute
efficiency and fast inference, it is one of the best in two public
text-to-music and -audio benchmarks and, differently from state-of-the-art
models, can generate music with structure and stereo sounds.",http://arxiv.org/abs/2402.04825v2,90,18,20,18,20,14
Bayesian Multi-Task Transfer Learning for Soft Prompt Tuning,"Prompt tuning, in which prompts are optimized to adapt large-scale
pre-trained language models to downstream tasks instead of fine-tuning the full
model parameters, has been shown to be particularly effective when the prompts
are trained in a multi-task transfer learning setting. These methods generally
involve individually training prompts for each source task and then aggregating
them to provide the initialization of the prompt for the target task. However,
this approach critically ignores the fact that some of the source tasks could
be negatively or positively interfering with each other. We argue that when we
extract knowledge from source tasks via training source prompts, we need to
consider this correlation among source tasks for better transfer to target
tasks. To this end, we propose a Bayesian approach where we work with the
posterior distribution of prompts across source tasks. We obtain representative
source prompts corresponding to the samples from the posterior utilizing Stein
Variational Gradient Descent, which are then aggregated to constitute the
initial target prompt. We show extensive experimental results on the standard
benchmark NLP tasks, where our Bayesian multi-task transfer learning approach
outperforms the state-of-the-art methods in many settings. Furthermore, our
approach requires no auxiliary models other than the prompt itself, achieving a
high degree of parameter efficiency.",http://dx.doi.org/10.18653/v1/2023.findings-emnlp.329,90,17,19,18,19,17
C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models,"Despite the impressive capabilities of large language models (LLMs) across
diverse applications, they still suffer from trustworthiness issues, such as
hallucinations and misalignments. Retrieval-augmented language models (RAG)
have been proposed to enhance the credibility of generations by grounding
external knowledge, but the theoretical understandings of their generation
risks remains unexplored. In this paper, we answer: 1) whether RAG can indeed
lead to low generation risks, 2) how to provide provable guarantees on the
generation risks of RAG and vanilla LLMs, and 3) what sufficient conditions
enable RAG models to reduce generation risks. We propose C-RAG, the first
framework to certify generation risks for RAG models. Specifically, we provide
conformal risk analysis for RAG models and certify an upper confidence bound of
generation risks, which we refer to as conformal generation risk. We also
provide theoretical guarantees on conformal generation risks for general
bounded risk functions under test distribution shifts. We prove that RAG
achieves a lower conformal generation risk than that of a single LLM when the
quality of the retrieval model and transformer is non-trivial. Our intensive
empirical results demonstrate the soundness and tightness of our conformal
generation risk guarantees across four widely-used NLP datasets on four
state-of-the-art retrieval models.",http://arxiv.org/abs/2402.03181v2,90,18,20,18,18,16
Evading Data Contamination Detection for Language Models is (too) Easy,"Large language models are widespread, with their performance on benchmarks
frequently guiding user preferences for one model over another. However, the
vast amount of data these models are trained on can inadvertently lead to
contamination with public benchmarks, thus compromising performance
measurements. While recently developed contamination detection methods try to
address this issue, they overlook the possibility of deliberate contamination
by malicious model providers aiming to evade detection. We argue that this
setting is of crucial importance as it casts doubt on the reliability of public
benchmarks. To more rigorously study this issue, we propose a categorization of
both model providers and contamination detection methods. This reveals
vulnerabilities in existing methods that we exploit with EAL, a simple yet
effective contamination technique that significantly inflates benchmark
performance while completely evading current detection methods.",http://arxiv.org/abs/2402.02823v2,90,18,20,18,18,16
Review of multimodal machine learning approaches in healthcare,"Machine learning methods in healthcare have traditionally focused on using
data from a single modality, limiting their ability to effectively replicate
the clinical practice of integrating multiple sources of information for
improved decision making. Clinicians typically rely on a variety of data
sources including patients' demographic information, laboratory data, vital
signs and various imaging data modalities to make informed decisions and
contextualise their findings. Recent advances in machine learning have
facilitated the more efficient incorporation of multimodal data, resulting in
applications that better represent the clinician's approach. Here, we provide a
review of multimodal machine learning approaches in healthcare, offering a
comprehensive overview of recent literature. We discuss the various data
modalities used in clinical diagnosis, with a particular emphasis on imaging
data. We evaluate fusion techniques, explore existing multimodal datasets and
examine common training strategies.",http://arxiv.org/abs/2402.02460v2,90,18,20,18,18,16
MetaOptimize: A Framework for Optimizing Step Sizes and Other Meta-parameters,"This paper addresses the challenge of optimizing meta-parameters (i.e.,
hyperparameters) in machine learning algorithms, a critical factor influencing
training efficiency and model performance. Moving away from the computationally
expensive traditional meta-parameter search methods, we introduce MetaOptimize
framework that dynamically adjusts meta-parameters, particularly step sizes
(also known as learning rates), during training. More specifically,
MetaOptimize can wrap around any first-order optimization algorithm, tuning
step sizes on the fly to minimize a specific form of regret that accounts for
long-term effect of step sizes on training, through a discounted sum of future
losses. We also introduce low complexity variants of MetaOptimize that, in
conjunction with its adaptability to multiple optimization algorithms,
demonstrate performance competitive to those of best hand-crafted learning rate
schedules across various machine learning applications.",http://arxiv.org/abs/2402.02342v2,90,18,20,18,16,18
SudokuSens: Enhancing Deep Learning Robustness for IoT Sensing Applications using a Generative Approach,"This paper introduces SudokuSens, a generative framework for automated
generation of training data in machine-learning-based Internet-of-Things (IoT)
applications, such that the generated synthetic data mimic experimental
configurations not encountered during actual sensor data collection. The
framework improves the robustness of resulting deep learning models, and is
intended for IoT applications where data collection is expensive. The work is
motivated by the fact that IoT time-series data entangle the signatures of
observed objects with the confounding intrinsic properties of the surrounding
environment and the dynamic environmental disturbances experienced. To
incorporate sufficient diversity into the IoT training data, one therefore
needs to consider a combinatorial explosion of training cases that are
multiplicative in the number of objects considered and the possible
environmental conditions in which such objects may be encountered. Our
framework substantially reduces these multiplicative training needs. To
decouple object signatures from environmental conditions, we employ a
Conditional Variational Autoencoder (CVAE) that allows us to reduce data
collection needs from multiplicative to (nearly) linear, while synthetically
generating (data for) the missing conditions. To obtain robustness with respect
to dynamic disturbances, a session-aware temporal contrastive learning approach
is taken. Integrating the aforementioned two approaches, SudokuSens
significantly improves the robustness of deep learning for IoT applications. We
explore the degree to which SudokuSens benefits downstream inference tasks in
different data sets and discuss conditions under which the approach is
particularly effective.",http://dx.doi.org/10.1145/3625687.3625785,90,18,20,18,18,16
MinMaxMin $Q$-learning,"MinMaxMin $Q$-learning is a novel optimistic Actor-Critic algorithm that
addresses the problem of overestimation bias ($Q$-estimations are
overestimating the real $Q$-values) inherent in conservative RL algorithms. Its
core formula relies on the disagreement among $Q$-networks in the form of the
min-batch MaxMin $Q$-networks distance which is added to the $Q$-target and
used as the priority experience replay sampling-rule. We implement MinMaxMin on
top of TD3 and TD7, subjecting it to rigorous testing against state-of-the-art
continuous-space algorithms-DDPG, TD3, and TD7-across popular MuJoCo and Bullet
environments. The results show a consistent performance improvement of
MinMaxMin over DDPG, TD3, and TD7 across all tested tasks.",http://arxiv.org/abs/2402.05951v2,90,18,18,18,20,16
SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 14 Languages,"Exploring and quantifying semantic relatedness is central to representing
language. It holds significant implications across various NLP tasks, including
offering insights into the capabilities and performance of Large Language
Models (LLMs). While earlier NLP research primarily focused on semantic
similarity, often within the English language context, we instead investigate
the broader phenomenon of semantic relatedness. In this paper, we present
SemRel, a new semantic relatedness dataset collection annotated by native
speakers across 14 languages:Afrikaans, Algerian Arabic, Amharic, English,
Hausa, Hindi, Indonesian, Kinyarwanda, Marathi, Moroccan Arabic, Modern
Standard Arabic, Punjabi, Spanish, and Telugu. These languages originate from
five distinct language families and are predominantly spoken in Africa and Asia
-- regions characterised by a relatively limited availability of NLP resources.
Each instance in the SemRel datasets is a sentence pair associated with a score
that represents the degree of semantic textual relatedness between the two
sentences. The scores are obtained using a comparative annotation framework. We
describe the data collection and annotation processes, related challenges when
building the datasets, and their impact and utility in NLP. We further report
experiments for each language and across the different languages.",http://arxiv.org/abs/2402.08638v1,90,18,18,18,18,18
CroissantLLM: A Truly Bilingual French-English Language Model,"We introduce CroissantLLM, a 1.3B language model pretrained on a set of 3T
English and French tokens, to bring to the research and industrial community a
high-performance, fully open-sourced bilingual model that runs swiftly on
consumer-grade local hardware. To that end, we pioneer the approach of training
an intrinsically bilingual model with a 1:1 English-to-French pretraining data
ratio, a custom tokenizer, and bilingual finetuning datasets. We release the
training dataset, notably containing a French split with manually curated,
high-quality, and varied data sources. To assess performance outside of
English, we craft a novel benchmark, FrenchBench, consisting of an array of
classification and generation tasks, covering various orthogonal aspects of
model performance in the French Language. Additionally, rooted in transparency
and to foster further Large Language Model research, we release codebases, and
dozens of checkpoints across various model sizes, training data distributions,
and training steps, as well as fine-tuned Chat models, and strong translation
models. We evaluate our model through the FMTI framework, and validate 81 % of
the transparency criteria, far beyond the scores of even most open initiatives.
This work enriches the NLP landscape, breaking away from previous
English-centric work in order to strengthen our understanding of
multilinguality in language models.",http://arxiv.org/abs/2402.00786v3,90,18,20,18,20,14
Forecasting high-impact research topics via machine learning on evolving knowledge graphs,"The exponential growth in scientific publications poses a severe challenge
for human researchers. It forces attention to more narrow sub-fields, which
makes it challenging to discover new impactful research ideas and
collaborations outside one's own field. While there are ways to predict a
scientific paper's future citation counts, they need the research to be
finished and the paper written, usually assessing impact long after the idea
was conceived. Here we show how to predict the impact of onsets of ideas that
have never been published by researchers. For that, we developed a large
evolving knowledge graph built from more than 21 million scientific papers. It
combines a semantic network created from the content of the papers and an
impact network created from the historic citations of papers. Using machine
learning, we can predict the dynamic of the evolving network into the future
with high accuracy, and thereby the impact of new research directions. We
envision that the ability to predict the impact of new ideas will be a crucial
component of future artificial muses that can inspire new impactful and
interesting scientific ideas.",http://arxiv.org/abs/2402.08640v1,90,18,20,18,18,16
Computation and Parameter Efficient Multi-Modal Fusion Transformer for Cued Speech Recognition,"Cued Speech (CS) is a pure visual coding method used by hearing-impaired
people that combines lip reading with several specific hand shapes to make the
spoken language visible. Automatic CS recognition (ACSR) seeks to transcribe
visual cues of speech into text, which can help hearing-impaired people to
communicate effectively. The visual information of CS contains lip reading and
hand cueing, thus the fusion of them plays an important role in ACSR. However,
most previous fusion methods struggle to capture the global dependency present
in long sequence inputs of multi-modal CS data. As a result, these methods
generally fail to learn the effective cross-modal relationships that contribute
to the fusion. Recently, attention-based transformers have been a prevalent
idea for capturing the global dependency over the long sequence in multi-modal
fusion, but existing multi-modal fusion transformers suffer from both poor
recognition accuracy and inefficient computation for the ACSR task. To address
these problems, we develop a novel computation and parameter efficient
multi-modal fusion transformer by proposing a novel Token-Importance-Aware
Attention mechanism (TIAA), where a token utilization rate (TUR) is formulated
to select the important tokens from the multi-modal streams. More precisely,
TIAA firstly models the modality-specific fine-grained temporal dependencies
over all tokens of each modality, and then learns the efficient cross-modal
interaction for the modality-shared coarse-grained temporal dependencies over
the important tokens of different modalities. Besides, a light-weight gated
hidden projection is designed to control the feature flows of TIAA. The
resulting model, named Economical Cued Speech Fusion Transformer (EcoCued),
achieves state-of-the-art performance on all existing CS datasets, compared
with existing transformer-based fusion methods and ACSR fusion methods.",http://arxiv.org/abs/2401.17604v2,90,18,18,20,18,16
Time Series Supplier Allocation via Deep Black-Litterman Model,"Time Series Supplier Allocation (TSSA) poses a complex NP-hard challenge,
aimed at refining future order dispatching strategies to satisfy order demands
with maximum supply efficiency fully. Traditionally derived from financial
portfolio management, the Black-Litterman (BL) model offers a new perspective
for the TSSA scenario by balancing expected returns against insufficient supply
risks. However, its application within TSSA is constrained by the reliance on
manually constructed perspective matrices and spatio-temporal market dynamics,
coupled with the absence of supervisory signals and data unreliability inherent
to supplier information. To solve these limitations, we introduce the
pioneering Deep Black-Litterman Model (DBLM), which innovatively adapts the BL
model from financial roots to supply chain context. Leveraging the
Spatio-Temporal Graph Neural Networks (STGNNS), DBLM automatically generates
future perspective matrices for TSSA, by integrating spatio-temporal
dependency. Moreover, a novel Spearman rank correlation distinctively
supervises our approach to address the lack of supervisory signals,
specifically designed to navigate through the complexities of supplier risks
and interactions. This is further enhanced by a masking mechanism aimed at
counteracting the biases from unreliable data, thereby improving the model's
precision and reliability. Extensive experimentation on two datasets
unequivocally demonstrates DBLM's enhanced performance in TSSA, setting new
standards for the field. Our findings and methodology are made available for
community access and further development.",http://arxiv.org/abs/2401.17350v2,90,20,18,18,18,16
NeuroBench: An Open-Source Benchmark Framework for the Standardization of Methodology in Brainwave-based Authentication Research,"Biometric systems based on brain activity have been proposed as an
alternative to passwords or to complement current authentication techniques. By
leveraging the unique brainwave patterns of individuals, these systems offer
the possibility of creating authentication solutions that are resistant to
theft, hands-free, accessible, and potentially even revocable. However, despite
the growing stream of research in this area, faster advance is hindered by
reproducibility problems. Issues such as the lack of standard reporting schemes
for performance results and system configuration, or the absence of common
evaluation benchmarks, make comparability and proper assessment of different
biometric solutions challenging. Further, barriers are erected to future work
when, as so often, source code is not published open access. To bridge this
gap, we introduce NeuroBench, a flexible open source tool to benchmark
brainwave-based authentication models. It incorporates nine diverse datasets,
implements a comprehensive set of pre-processing parameters and machine
learning algorithms, enables testing under two common adversary models (known
vs unknown attacker), and allows researchers to generate full performance
reports and visualizations. We use NeuroBench to investigate the shallow
classifiers and deep learning-based approaches proposed in the literature, and
to test robustness across multiple sessions. We observe a 37.6\% reduction in
Equal Error Rate (EER) for unknown attacker scenarios (typically not tested in
the literature), and we highlight the importance of session variability to
brainwave authentication. All in all, our results demonstrate the viability and
relevance of NeuroBench in streamlining fair comparisons of algorithms, thereby
furthering the advancement of brainwave-based authentication through robust
methodological practices.",http://arxiv.org/abs/2402.08656v1,90,18,20,18,18,16
SERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning,"In recent years, significant progress has been made in the field of robotic
reinforcement learning (RL), enabling methods that handle complex image
observations, train in the real world, and incorporate auxiliary data, such as
demonstrations and prior experience. However, despite these advances, robotic
RL remains hard to use. It is acknowledged among practitioners that the
particular implementation details of these algorithms are often just as
important (if not more so) for performance as the choice of algorithm. We posit
that a significant challenge to widespread adoption of robotic RL, as well as
further development of robotic RL methods, is the comparative inaccessibility
of such methods. To address this challenge, we developed a carefully
implemented library containing a sample efficient off-policy deep RL method,
together with methods for computing rewards and resetting the environment, a
high-quality controller for a widely-adopted robot, and a number of challenging
example tasks. We provide this library as a resource for the community,
describe its design choices, and present experimental results. Perhaps
surprisingly, we find that our implementation can achieve very efficient
learning, acquiring policies for PCB board assembly, cable routing, and object
relocation between 25 to 50 minutes of training per policy on average,
improving over state-of-the-art results reported for similar tasks in the
literature. These policies achieve perfect or near-perfect success rates,
extreme robustness even under perturbations, and exhibit emergent recovery and
correction behaviors. We hope that these promising results and our high-quality
open-source implementation will provide a tool for the robotics community to
facilitate further developments in robotic RL. Our code, documentation, and
videos can be found at https://serl-robot.github.io/",http://arxiv.org/abs/2401.16013v3,90,18,19,19,18,16
Five ethical principles for generative AI in scientific research,"Generative artificial intelligence tools like large language models are
rapidly transforming academic research and real world applications. However,
discussions on ethical guidelines for generative AI in science remain
fragmented, underscoring the urgent need for consensus based standards. This
paper offers an initial framework by developing analyses and mitigation
strategies across five key themes: understanding model limitations regarding
truthfulness and bias; respecting privacy, confidentiality, and copyright;
avoiding plagiarism and policy violations when incorporating model output;
ensuring applications provide overall benefit; and using AI transparently and
reproducibly. Common scenarios are outlined to demonstrate potential ethical
violations. We argue that global consensus coupled with professional training
and reasonable enforcement are critical to promoting the benefits of AI while
safeguarding research integrity.",http://arxiv.org/abs/2401.15284v2,90,18,19,19,17,17
Cross-Space Adaptive Filter: Integrating Graph Topology and Node Attributes for Alleviating the Over-smoothing Problem,"The vanilla Graph Convolutional Network (GCN) uses a low-pass filter to
extract low-frequency signals from graph topology, which may lead to the
over-smoothing problem when GCN goes deep. To this end, various methods have
been proposed to create an adaptive filter by incorporating an extra filter
(e.g., a high-pass filter) extracted from the graph topology. However, these
methods heavily rely on topological information and ignore the node attribute
space, which severely sacrifices the expressive power of the deep GCNs,
especially when dealing with disassortative graphs. In this paper, we propose a
cross-space adaptive filter, called CSF, to produce the adaptive-frequency
information extracted from both the topology and attribute spaces.
Specifically, we first derive a tailored attribute-based high-pass filter that
can be interpreted theoretically as a minimizer for semi-supervised kernel
ridge regression. Then, we cast the topology-based low-pass filter as a
Mercer's kernel within the context of GCNs. This serves as a foundation for
combining it with the attribute-based filter to capture the adaptive-frequency
information. Finally, we derive the cross-space filter via an effective
multiple-kernel learning strategy, which unifies the attribute-based high-pass
filter and the topology-based low-pass filter. This helps to address the
over-smoothing problem while maintaining effectiveness. Extensive experiments
demonstrate that CSF not only successfully alleviates the over-smoothing
problem but also promotes the effectiveness of the node classification task.",http://arxiv.org/abs/2401.14876v2,90,20,19,18,17,16
Enhancing Zero-shot Counting via Language-guided Exemplar Learning,"Recently, Class-Agnostic Counting (CAC) problem has garnered increasing
attention owing to its intriguing generality and superior efficiency compared
to Category-Specific Counting (CSC). This paper proposes a novel ExpressCount
to enhance zero-shot object counting by delving deeply into language-guided
exemplar learning. Specifically, the ExpressCount is comprised of an innovative
Language-oriented Exemplar Perceptron and a downstream visual Zero-shot
Counting pipeline. Thereinto, the perceptron hammers at exploiting accurate
exemplar cues from collaborative language-vision signals by inheriting rich
semantic priors from the prevailing pre-trained Large Language Models (LLMs),
whereas the counting pipeline excels in mining fine-grained features through
dual-branch and cross-attention schemes, contributing to the high-quality
similarity learning. Apart from building a bridge between the LLM in vogue and
the visual counting tasks, expression-guided exemplar estimation significantly
advances zero-shot learning capabilities for counting instances with arbitrary
classes. Moreover, devising a FSC-147-Express with annotations of meticulous
linguistic expressions pioneers a new venue for developing and validating
language-based counting models. Extensive experiments demonstrate the
state-of-the-art performance of our ExpressCount, even showcasing the accuracy
on par with partial CSC models.",http://arxiv.org/abs/2402.05394v1,90,18,19,19,19,15
Do Large Code Models Understand Programming Concepts? A Black-box Approach,"Large Language Models' success on text generation has also made them better
at code generation and coding tasks. While a lot of work has demonstrated their
remarkable performance on tasks such as code completion and editing, it is
still unclear as to why. We help bridge this gap by exploring to what degree
auto-regressive models understand the logical constructs of the underlying
programs. We propose Counterfactual Analysis for Programming Concept Predicates
(CACP) as a counterfactual testing framework to evaluate whether Large Code
Models understand programming concepts. With only black-box access to the
model, we use CACP to evaluate ten popular Large Code Models for four different
programming concepts. Our findings suggest that current models lack
understanding of concepts such as data flow and control flow.",http://arxiv.org/abs/2402.05980v1,90,19,18,19,17,17
Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering,"Advances towards more faithful and traceable answers of Large Language Models
(LLMs) are crucial for various research and practical endeavors. One avenue in
reaching this goal is basing the answers on reliable sources. However, this
Evidence-Based QA has proven to work insufficiently with LLMs in terms of
citing the correct sources (source quality) and truthfully representing the
information within sources (answer attributability). In this work, we
systematically investigate how to robustly fine-tune LLMs for better source
quality and answer attributability. Specifically, we introduce a data
generation pipeline with automated data quality filters, which can synthesize
diversified high-quality training and testing data at scale. We further
introduce four test sets to benchmark the robustness of fine-tuned specialist
models. Extensive evaluation shows that fine-tuning on synthetic data improves
performance on both in- and out-of-distribution. %Evidence-Based QA cases.
Furthermore, we show that data quality, which can be drastically improved by
proposed quality filters, matters more than quantity in improving
Evidence-Based QA.",http://arxiv.org/abs/2402.08277v1,90,19,18,19,17,17
Leveraging AI for Enhanced Software Effort Estimation: A Comprehensive Study and Framework Proposal,"This paper presents an extensive study on the application of AI techniques
for software effort estimation in the past five years from 2017 to 2023. By
overcoming the limitations of traditional methods, the study aims to improve
accuracy and reliability. Through performance evaluation and comparison with
diverse Machine Learning models, including Artificial Neural Network (ANN),
Support Vector Machine (SVM), Linear Regression, Random Forest and other
techniques, the most effective method is identified. The proposed AI-based
framework holds the potential to enhance project planning and resource
allocation, contributing to the research area of software project effort
estimation.",http://arxiv.org/abs/2402.05484v1,90,18,20,18,18,16
Pix2Code: Learning to Compose Neural Visual Concepts as Programs,"The challenge in learning abstract concepts from images in an unsupervised
fashion lies in the required integration of visual perception and generalizable
relational reasoning. Moreover, the unsupervised nature of this task makes it
necessary for human users to be able to understand a model's learnt concepts
and potentially revise false behaviours. To tackle both the generalizability
and interpretability constraints of visual concept learning, we propose
Pix2Code, a framework that extends program synthesis to visual relational
reasoning by utilizing the abilities of both explicit, compositional symbolic
and implicit neural representations. This is achieved by retrieving object
representations from images and synthesizing relational concepts as
lambda-calculus programs. We evaluate the diverse properties of Pix2Code on the
challenging reasoning domains, Kandinsky Patterns and CURI, thereby testing its
ability to identify compositional visual concepts that generalize to novel data
and concept configurations. Particularly, in stark contrast to neural
approaches, we show that Pix2Code's representations remain human interpretable
and can be easily revised for improved performance.",http://arxiv.org/abs/2402.08280v1,90,18,19,19,18,16
Contrastive Approach to Prior Free Positive Unlabeled Learning,"Positive Unlabeled (PU) learning refers to the task of learning a binary
classifier given a few labeled positive samples, and a set of unlabeled samples
(which could be positive or negative). In this paper, we propose a novel PU
learning framework, that starts by learning a feature space through
pretext-invariant representation learning and then applies pseudo-labeling to
the unlabeled examples, leveraging the concentration property of the
embeddings. Overall, our proposed approach handily outperforms state-of-the-art
PU learning methods across several standard PU benchmark datasets, while not
requiring a-priori knowledge or estimate of class prior. Remarkably, our method
remains effective even when labeled data is scant, where most PU learning
algorithms falter. We also provide simple theoretical analysis motivating our
proposed algorithms and establish generalization guarantee for our approach.",http://arxiv.org/abs/2402.06038v1,90,18,18,20,16,18
An operator learning perspective on parameter-to-observable maps,"Computationally efficient surrogates for parametrized physical models play a
crucial role in science and engineering. Operator learning provides data-driven
surrogates that map between function spaces. However, instead of full-field
measurements, often the available data are only finite-dimensional
parametrizations of model inputs or finite observables of model outputs.
Building off of Fourier Neural Operators, this paper introduces the Fourier
Neural Mappings (FNMs) framework that is able to accommodate such
finite-dimensional inputs and outputs. The paper develops universal
approximation theorems for the method. Moreover, in many applications the
underlying parameter-to-observable (PtO) map is defined implicitly through an
infinite-dimensional operator, such as the solution operator of a partial
differential equation. A natural question is whether it is more data-efficient
to learn the PtO map end-to-end or first learn the solution operator and
subsequently compute the observable from the full-field solution. A theoretical
analysis of Bayesian nonparametric regression of linear functionals, which is
of independent interest, suggests that the end-to-end approach can actually
have worse sample complexity. Extending beyond the theory, numerical results
for the FNM approximation of three nonlinear PtO maps demonstrate the benefits
of the operator learning perspective that this paper adopts.",http://arxiv.org/abs/2402.06031v1,90,18,20,18,18,16
NPSVC++: Nonparallel Classifiers Encounter Representation Learning,"This paper focuses on a specific family of classifiers called nonparallel
support vector classifiers (NPSVCs). Different from typical classifiers, the
training of an NPSVC involves the minimization of multiple objectives,
resulting in the potential concerns of feature suboptimality and class
dependency. Consequently, no effective learning scheme has been established to
improve NPSVCs' performance through representation learning, especially deep
learning. To break this bottleneck, we develop NPSVC++ based on multi-objective
optimization, enabling the end-to-end learning of NPSVC and its features. By
pursuing Pareto optimality, NPSVC++ theoretically ensures feature optimality
across classes, hence effectively overcoming the two issues above. A general
learning procedure via duality optimization is proposed, based on which we
provide two applicable instances, K-NPSVC++ and D-NPSVC++. The experiments show
their superiority over the existing methods and verify the efficacy of NPSVC++.",http://arxiv.org/abs/2402.06010v1,90,18,19,19,17,17
Large Language Model Meets Graph Neural Network in Knowledge Distillation,"Despite recent community revelations about the advancements and potential
applications of Large Language Models (LLMs) in understanding Text-Attributed
Graph (TAG), the deployment of LLMs for production is hindered by its high
computational and storage requirements, as well as long latencies during model
inference. Simultaneously, although traditional Graph Neural Networks (GNNs)
are light weight and adept at learning structural features of graphs, their
ability to grasp the complex semantics in TAG is somewhat constrained for real
applications. To address these limitations, we concentrate on the downstream
task of node classification in TAG and propose a novel graph knowledge
distillation framework, termed Linguistic Graph Knowledge Distillation
(LinguGKD), using LLMs as teacher models and GNNs as student models for
knowledge distillation. It involves TAG-oriented instruction tuning of LLM on
designed tailored prompts, followed by propagating knowledge and aligning the
hierarchically learned node features from the teacher LLM to the student GNN in
latent space, employing a layer-adaptive contrastive learning strategy. Through
extensive experiments on a variety of LLM and GNN models and multiple benchmark
datasets, the proposed LinguGKD significantly boosts the student GNN's
predictive accuracy and convergence rate, without the need of extra data or
model parameters. Compared to teacher LLM, distilled GNN achieves superior
inference speed equipped with much fewer computing and storage demands, when
surpassing the teacher LLM's classification accuracy on some of benchmark
datasets.",http://arxiv.org/abs/2402.05894v2,90,18,20,18,16,18
Adaptive Surface Normal Constraint for Geometric Estimation from Monocular Images,"We introduce a novel approach to learn geometries such as depth and surface
normal from images while incorporating geometric context. The difficulty of
reliably capturing geometric context in existing methods impedes their ability
to accurately enforce the consistency between the different geometric
properties, thereby leading to a bottleneck of geometric estimation quality. We
therefore propose the Adaptive Surface Normal (ASN) constraint, a simple yet
efficient method. Our approach extracts geometric context that encodes the
geometric variations present in the input image and correlates depth estimation
with geometric constraints. By dynamically determining reliable local geometry
from randomly sampled candidates, we establish a surface normal constraint,
where the validity of these candidates is evaluated using the geometric
context. Furthermore, our normal estimation leverages the geometric context to
prioritize regions that exhibit significant geometric variations, which makes
the predicted normals accurately capture intricate and detailed geometric
information. Through the integration of geometric context, our method unifies
depth and surface normal estimations within a cohesive framework, which enables
the generation of high-quality 3D geometry from images. We validate the
superiority of our approach over state-of-the-art methods through extensive
evaluations and comparisons on diverse indoor and outdoor datasets, showcasing
its efficiency and robustness.",http://arxiv.org/abs/2402.05869v1,90,18,18,18,18,18
EmojiCrypt: Prompt Encryption for Secure Communication with Large Language Models,"Cloud-based large language models (LLMs) such as ChatGPT have increasingly
become integral to daily operations, serving as vital tools across various
applications. While these models offer substantial benefits in terms of
accessibility and functionality, they also introduce significant privacy
concerns: the transmission and storage of user data in cloud infrastructures
pose substantial risks of data breaches and unauthorized access to sensitive
information; even if the transmission and storage of data is encrypted, the LLM
service provider itself still knows the real contents of the data, preventing
individuals or entities from confidently using such LLM services. To address
these concerns, this paper proposes a simple yet effective mechanism EmojiCrypt
to protect user privacy. It uses Emoji to encrypt the user inputs before
sending them to LLM, effectively rendering them indecipherable to human or
LLM's examination while retaining the original intent of the prompt, thus
ensuring the model's performance remains unaffected. We conduct experiments on
three tasks, personalized recommendation, sentiment analysis, and tabular data
analysis. Experiment results reveal that EmojiCrypt can encrypt personal
information within prompts in such a manner that not only prevents the
discernment of sensitive data by humans or LLM itself, but also maintains or
even improves the precision without further tuning, achieving comparable or
even better task accuracy than directly prompting the LLM without prompt
encryption. These results highlight the practicality of adopting encryption
measures that safeguard user privacy without compromising the functional
integrity and performance of LLMs. Code and dataset are available at
https://github.com/agiresearch/EmojiCrypt.",http://arxiv.org/abs/2402.05868v2,90,18,18,18,18,18
Permute-and-Flip: An optimally robust and watermarkable decoder for LLMs,"In this paper, we propose a new decoding method called Permute-and-Flip (PF)
decoder. It enjoys robustness properties similar to the standard sampling
decoder, but is provably up to 2x better in its quality-robustness tradeoff
than sampling and never worse than any other decoder. We also design a
cryptographic watermarking scheme analogous to Aaronson's Gumbel watermark, but
naturally tailored for PF decoder. The watermarking scheme does not change the
distribution to sample, while allowing arbitrarily low false positive rate and
high recall whenever the generated text has high entropy. Our experiments show
that the PF decoder (and its watermarked counterpart) significantly
outperform(s) naive sampling (and it's Gumbel watermarked counterpart) in terms
of perplexity, while retaining the same robustness (and detectability), hence
making it a promising new approach for LLM decoding. The code is available at
https://github.com/XuandongZhao/pf-decoding",http://arxiv.org/abs/2402.05864v1,90,18,19,18,17,18
Let Your Graph Do the Talking: Encoding Structured Data for LLMs,"How can we best encode structured data into sequential form for use in large
language models (LLMs)? In this work, we introduce a parameter-efficient method
to explicitly represent structured data for LLMs. Our method, GraphToken,
learns an encoding function to extend prompts with explicit structured
information. Unlike other work which focuses on limited domains (e.g. knowledge
graph representation), our work is the first effort focused on the general
encoding of structured data to be used for various reasoning tasks. We show
that explicitly representing the graph structure allows significant
improvements to graph reasoning tasks. Specifically, we see across the board
improvements - up to 73% points - on node, edge and, graph-level tasks from the
GraphQA benchmark.",http://arxiv.org/abs/2402.05862v1,90,18,19,19,18,16
Uncertainty Quantification via Stable Distribution Propagation,"We propose a new approach for propagating stable probability distributions
through neural networks. Our method is based on local linearization, which we
show to be an optimal approximation in terms of total variation distance for
the ReLU non-linearity. This allows propagating Gaussian and Cauchy input
uncertainties through neural networks to quantify their output uncertainties.
To demonstrate the utility of propagating distributions, we apply the proposed
method to predicting calibrated confidence intervals and selective prediction
on out-of-distribution data. The results demonstrate a broad applicability of
propagating distributions and show the advantages of our method over other
approaches such as moment matching.",http://arxiv.org/abs/2402.08324v1,90,18,20,18,18,16
Neural Models for Source Code Synthesis and Completion,"Natural language (NL) to code suggestion systems assist developers in
Integrated Development Environments (IDEs) by translating NL utterances into
compilable code snippet. The current approaches mainly involve hard-coded,
rule-based systems based on semantic parsing. These systems make heavy use of
hand-crafted rules that map patterns in NL or elements in its syntax parse tree
to various query constructs and can only work on a limited subset of NL with a
restricted NL syntax. These systems are unable to extract semantic information
from the coding intents of the developer, and often fail to infer types, names,
and the context of the source code to get accurate system-level code
suggestions. In this master thesis, we present sequence-to-sequence deep
learning models and training paradigms to map NL to general-purpose programming
languages that can assist users with suggestions of source code snippets, given
a NL intent, and also extend auto-completion functionality of the source code
to users while they are writing source code. The developed architecture
incorporates contextual awareness into neural models which generate source code
tokens directly instead of generating parse trees/abstract meaning
representations from the source code and converting them back to source code.
The proposed pretraining strategy and the data augmentation techniques improve
the performance of the proposed architecture. The proposed architecture has
been found to exceed the performance of a neural semantic parser, TranX, based
on the BLEU-4 metric by 10.82%. Thereafter, a finer analysis for the parsable
code translations from the NL intent for CoNaLA challenge was introduced. The
proposed system is bidirectional as it can be also used to generate NL code
documentation given source code. Lastly, a RoBERTa masked language model for
Python was proposed to extend the developed system for code completion.",http://arxiv.org/abs/2402.06690v1,90,18,18,18,18,18
RBF-PINN: Non-Fourier Positional Embedding in Physics-Informed Neural Networks,"While many recent Physics-Informed Neural Networks (PINNs) variants have had
considerable success in solving Partial Differential Equations, the empirical
benefits of feature mapping drawn from the broader Neural Representations
research have been largely overlooked. We highlight the limitations of widely
used Fourier-based feature mapping in certain situations and suggest the use of
the conditionally positive definite Radial Basis Function. The empirical
findings demonstrate the effectiveness of our approach across a variety of
forward and inverse problem cases. Our method can be seamlessly integrated into
coordinate-based input neural networks and contribute to the wider field of
PINNs research.",http://arxiv.org/abs/2402.08367v1,90,20,18,20,16,16
Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning,"In this paper, we propose R$^3$: Learning Reasoning through Reverse
Curriculum Reinforcement Learning (RL), a novel method that employs only
outcome supervision to achieve the benefits of process supervision for large
language models. The core challenge in applying RL to complex reasoning is to
identify a sequence of actions that result in positive rewards and provide
appropriate supervision for optimization. Outcome supervision provides sparse
rewards for final results without identifying error locations, whereas process
supervision offers step-wise rewards but requires extensive manual annotation.
R$^3$ overcomes these limitations by learning from correct demonstrations.
Specifically, R$^3$ progressively slides the start state of reasoning from a
demonstration's end to its beginning, facilitating easier model exploration at
all stages. Thus, R$^3$ establishes a step-wise curriculum, allowing outcome
supervision to offer step-level signals and precisely pinpoint errors. Using
Llama2-7B, our method surpasses RL baseline on eight reasoning tasks by $4.1$
points on average. Notebaly, in program-based reasoning on GSM8K, it exceeds
the baseline by $4.2$ points across three backbone models, and without any
extra data, Codellama-7B + R$^3$ performs comparable to larger models or
closed-source models.",http://arxiv.org/abs/2402.05808v1,90,18,19,19,17,17
InkSight: Offline-to-Online Handwriting Conversion by Learning to Read and Write,"Digital note-taking is gaining popularity, offering a durable, editable, and
easily indexable way of storing notes in the vectorized form, known as digital
ink. However, a substantial gap remains between this way of note-taking and
traditional pen-and-paper note-taking, a practice still favored by a vast
majority. Our work, InkSight, aims to bridge the gap by empowering physical
note-takers to effortlessly convert their work (offline handwriting) to digital
ink (online handwriting), a process we refer to as Derendering. Prior research
on the topic has focused on the geometric properties of images, resulting in
limited generalization beyond their training domains. Our approach combines
reading and writing priors, allowing training a model in the absence of large
amounts of paired samples, which are difficult to obtain. To our knowledge,
this is the first work that effectively derenders handwritten text in arbitrary
photos with diverse visual characteristics and backgrounds. Furthermore, it
generalizes beyond its training domain into simple sketches. Our human
evaluation reveals that 87% of the samples produced by our model on the
challenging HierText dataset are considered as a valid tracing of the input
image and 67% look like a pen trajectory traced by a human.",http://arxiv.org/abs/2402.05804v1,90,18,19,19,18,16
One-shot Imitation in a Non-Stationary Environment via Multi-Modal Skill,"One-shot imitation is to learn a new task from a single demonstration, yet it
is a challenging problem to adopt it for complex tasks with the high domain
diversity inherent in a non-stationary environment. To tackle the problem, we
explore the compositionality of complex tasks, and present a novel skill-based
imitation learning framework enabling one-shot imitation and zero-shot
adaptation; from a single demonstration for a complex unseen task, a semantic
skill sequence is inferred and then each skill in the sequence is converted
into an action sequence optimized for environmental hidden dynamics that can
vary over time. Specifically, we leverage a vision-language model to learn a
semantic skill set from offline video datasets, where each skill is represented
on the vision-language embedding space, and adapt meta-learning with dynamics
inference to enable zero-shot skill adaptation. We evaluate our framework with
various one-shot imitation scenarios for extended multi-stage Meta-world tasks,
showing its superiority in learning complex tasks, generalizing to dynamics
changes, and extending to different demonstration conditions and modalities,
compared to other baselines.",http://arxiv.org/abs/2402.08369v1,90,19,18,18,16,19
Prompting Fairness: Artificial Intelligence as Game Players,"Utilitarian games such as dictator games to measure fairness have been
studied in the social sciences for decades. These games have given us insight
into not only how humans view fairness but also in what conditions the
frequency of fairness, altruism and greed increase or decrease. While these
games have traditionally been focused on humans, the rise of AI gives us the
ability to study how these models play these games. AI is becoming a constant
in human interaction and examining how these models portray fairness in game
play can give us some insight into how AI makes decisions. Over 101 rounds of
the dictator game, I conclude that AI has a strong sense of fairness that is
dependant of it it deems the person it is playing with as trustworthy, framing
has a strong effect on how much AI gives a recipient when designated the
trustee, and there may be evidence that AI experiences inequality aversion just
as humans.",http://arxiv.org/abs/2402.05786v2,90,18,20,18,16,18
Should I try multiple optimizers when fine-tuning pre-trained Transformers for NLP tasks? Should I tune their hyperparameters?,"NLP research has explored different neural model architectures and sizes,
datasets, training objectives, and transfer learning techniques. However, the
choice of optimizer during training has not been explored as extensively.
Typically, some variant of Stochastic Gradient Descent (SGD) is employed,
selected among numerous variants, using unclear criteria, often with minimal or
no tuning of the optimizer's hyperparameters. Experimenting with five GLUE
datasets, two models (DistilBERT and DistilRoBERTa), and seven popular
optimizers (SGD, SGD with Momentum, Adam, AdaMax, Nadam, AdamW, and AdaBound),
we find that when the hyperparameters of the optimizers are tuned, there is no
substantial difference in test performance across the five more elaborate
(adaptive) optimizers, despite differences in training loss. Furthermore,
tuning just the learning rate is in most cases as good as tuning all the
hyperparameters. Hence, we recommend picking any of the best-behaved adaptive
optimizers (e.g., Adam) and tuning only its learning rate. When no
hyperparameter can be tuned, SGD with Momentum is the best choice.",http://arxiv.org/abs/2402.06948v1,90,16,18,18,20,18
Uncertainty Quantification for Forward and Inverse Problems of PDEs via Latent Global Evolution,"Deep learning-based surrogate models have demonstrated remarkable advantages
over classical solvers in terms of speed, often achieving speedups of 10 to
1000 times over traditional partial differential equation (PDE) solvers.
However, a significant challenge hindering their widespread adoption in both
scientific and industrial domains is the lack of understanding about their
prediction uncertainties, particularly in scenarios that involve critical
decision making. To address this limitation, we propose a method that
integrates efficient and precise uncertainty quantification into a deep
learning-based surrogate model. Our method, termed Latent Evolution of PDEs
with Uncertainty Quantification (LE-PDE-UQ), endows deep learning-based
surrogate models with robust and efficient uncertainty quantification
capabilities for both forward and inverse problems. LE-PDE-UQ leverages latent
vectors within a latent space to evolve both the system's state and its
corresponding uncertainty estimation. The latent vectors are decoded to provide
predictions for the system's state as well as estimates of its uncertainty. In
extensive experiments, we demonstrate the accurate uncertainty quantification
performance of our approach, surpassing that of strong baselines including deep
ensembles, Bayesian neural network layers, and dropout. Our method excels at
propagating uncertainty over extended auto-regressive rollouts, making it
suitable for scenarios involving long-term predictions. Our code is available
at: https://github.com/AI4Science-WestlakeU/le-pde-uq.",http://arxiv.org/abs/2402.08383v1,90,20,18,18,18,16
Dual-modal Tactile E-skin: Enabling Bidirectional Human-Robot Interaction via Integrated Tactile Perception and Feedback,"To foster an immersive and natural human-robot interaction, the
implementation of tactile perception and feedback becomes imperative,
effectively bridging the conventional sensory gap. In this paper, we propose a
dual-modal electronic skin (e-skin) that integrates magnetic tactile sensing
and vibration feedback for enhanced human-robot interaction. The dual-modal
tactile e-skin offers multi-functional tactile sensing and programmable haptic
feedback, underpinned by a layered structure comprised of flexible magnetic
films, soft silicone, a Hall sensor and actuator array, and a microcontroller
unit. The e-skin captures the magnetic field changes caused by subtle
deformations through Hall sensors, employing deep learning for accurate tactile
perception. Simultaneously, the actuator array generates mechanical vibrations
to facilitate haptic feedback, delivering diverse mechanical stimuli. Notably,
the dual-modal e-skin is capable of transmitting tactile information
bidirectionally, enabling object recognition and fine-weighing operations. This
bidirectional tactile interaction framework will enhance the immersion and
efficiency of interactions between humans and robots.",http://arxiv.org/abs/2402.05725v1,90,18,20,18,18,16
Model-Based RL for Mean-Field Games is not Statistically Harder than Single-Agent RL,"We study the sample complexity of reinforcement learning (RL) in Mean-Field
Games (MFGs) with model-based function approximation that requires strategic
exploration to find a Nash Equilibrium policy. We introduce the Partial
Model-Based Eluder Dimension (P-MBED), a more effective notion to characterize
the model class complexity. Notably, P-MBED measures the complexity of the
single-agent model class converted from the given mean-field model class, and
potentially, can be exponentially lower than the MBED proposed by
\citet{huang2023statistical}. We contribute a model elimination algorithm
featuring a novel exploration strategy and establish sample complexity results
polynomial w.r.t.~P-MBED. Crucially, our results reveal that, under the basic
realizability and Lipschitz continuity assumptions, \emph{learning Nash
Equilibrium in MFGs is no more statistically challenging than solving a
logarithmic number of single-agent RL problems}. We further extend our results
to Multi-Type MFGs, generalizing from conventional MFGs and involving multiple
types of agents. This extension implies statistical tractability of a broader
class of Markov Games through the efficacy of mean-field approximation.
Finally, inspired by our theoretical algorithm, we present a heuristic approach
with improved computational efficiency and empirically demonstrate its
effectiveness.",http://arxiv.org/abs/2402.05724v1,90,18,19,18,17,18
NfgTransformer: Equivariant Representation Learning for Normal-form Games,"Normal-form games (NFGs) are the fundamental model of strategic interaction.
We study their representation using neural networks. We describe the inherent
equivariance of NFGs -- any permutation of strategies describes an equivalent
game -- as well as the challenges this poses for representation learning. We
then propose the NfgTransformer architecture that leverages this equivariance,
leading to state-of-the-art performance in a range of game-theoretic tasks
including equilibrium-solving, deviation gain estimation and ranking, with a
common approach to NFG representation. We show that the resulting model is
interpretable and versatile, paving the way towards deep learning systems
capable of game-theoretic reasoning when interacting with humans and with each
other.",http://arxiv.org/abs/2402.08393v1,90,19,18,18,19,16
Comprehensive Assessment of Jailbreak Attacks Against LLMs,"Misuse of the Large Language Models (LLMs) has raised widespread concern. To
address this issue, safeguards have been taken to ensure that LLMs align with
social ethics. However, recent findings have revealed an unsettling
vulnerability bypassing the safeguards of LLMs, known as jailbreak attacks. By
applying techniques, such as employing role-playing scenarios, adversarial
examples, or subtle subversion of safety objectives as a prompt, LLMs can
produce an inappropriate or even harmful response. While researchers have
studied several categories of jailbreak attacks, they have done so in
isolation. To fill this gap, we present the first large-scale measurement of
various jailbreak attack methods. We concentrate on 13 cutting-edge jailbreak
methods from four categories, 160 questions from 16 violation categories, and
six popular LLMs. Our extensive experimental results demonstrate that the
optimized jailbreak prompts consistently achieve the highest attack success
rates, as well as exhibit robustness across different LLMs. Some jailbreak
prompt datasets, available from the Internet, can also achieve high attack
success rates on many LLMs, such as ChatGLM3, GPT-3.5, and PaLM2. Despite the
claims from many organizations regarding the coverage of violation categories
in their policies, the attack success rates from these categories remain high,
indicating the challenges of effectively aligning LLM policies and the ability
to counter jailbreak attacks. We also discuss the trade-off between the attack
performance and efficiency, as well as show that the transferability of the
jailbreak prompts is still viable, becoming an option for black-box models.
Overall, our research highlights the necessity of evaluating different
jailbreak methods. We hope our study can provide insights for future research
on jailbreak attacks and serve as a benchmark tool for evaluating them for
practitioners.",http://arxiv.org/abs/2402.05668v1,90,18,18,20,16,18
Optimizing Delegation in Collaborative Human-AI Hybrid Teams,"When humans and autonomous systems operate together as what we refer to as a
hybrid team, we of course wish to ensure the team operates successfully and
effectively. We refer to team members as agents. In our proposed framework, we
address the case of hybrid teams in which, at any time, only one team member
(the control agent) is authorized to act as control for the team. To determine
the best selection of a control agent, we propose the addition of an AI manager
(via Reinforcement Learning) which learns as an outside observer of the team.
The manager learns a model of behavior linking observations of agent
performance and the environment/world the team is operating in, and from these
observations makes the most desirable selection of a control agent. We restrict
the manager task by introducing a set of constraints. The manager constraints
indicate acceptable team operation, so a violation occurs if the team enters a
condition which is unacceptable and requires manager intervention. To ensure
minimal added complexity or potential inefficiency for the team, the manager
should attempt to minimize the number of times the team reaches a constraint
violation and requires subsequent manager intervention. Therefore our manager
is optimizing its selection of authorized agents to boost overall team
performance while minimizing the frequency of manager intervention. We
demonstrate our manager performance in a simulated driving scenario
representing the case of a hybrid team of agents composed of a human driver and
autonomous driving system. We perform experiments for our driving scenario with
interfering vehicles, indicating the need for collision avoidance and proper
speed control. Our results indicate a positive impact of our manager, with some
cases resulting in increased team performance up to ~187% that of the best solo
agent performance.",http://arxiv.org/abs/2402.05605v1,90,19,18,19,18,16
Offline Actor-Critic Reinforcement Learning Scales to Large Models,"We show that offline actor-critic reinforcement learning can scale to large
models - such as transformers - and follows similar scaling laws as supervised
learning. We find that offline actor-critic algorithms can outperform strong,
supervised, behavioral cloning baselines for multi-task training on a large
dataset containing both sub-optimal and expert behavior on 132 continuous
control tasks. We introduce a Perceiver-based actor-critic model and elucidate
the key model features needed to make offline RL work with self- and
cross-attention modules. Overall, we find that: i) simple offline actor critic
algorithms are a natural choice for gradually moving away from the currently
predominant paradigm of behavioral cloning, and ii) via offline RL it is
possible to learn multi-task policies that master many domains simultaneously,
including real robotics tasks, from sub-optimal demonstrations or
self-generated data.",http://arxiv.org/abs/2402.05546v1,90,20,18,18,18,16
Generating Java Methods: An Empirical Assessment of Four AI-Based Code Assistants,"AI-based code assistants are promising tools that can facilitate and speed up
code development. They exploit machine learning algorithms and natural language
processing to interact with developers, suggesting code snippets (e.g., method
implementations) that can be incorporated into projects. Recent studies
empirically investigated the effectiveness of code assistants using simple
exemplary problems (e.g., the re-implementation of well-known algorithms),
which fail to capture the spectrum and nature of the tasks actually faced by
developers. In this paper, we expand the knowledge in the area by comparatively
assessing four popular AI-based code assistants, namely GitHub Copilot,
Tabnine, ChatGPT, and Google Bard, with a dataset of 100 methods that we
constructed from real-life open-source Java projects, considering a variety of
cases for complexity and dependency from contextual elements. Results show that
Copilot is often more accurate than other techniques, yet none of the
assistants is completely subsumed by the rest of the approaches. Interestingly,
the effectiveness of these solutions dramatically decreases when dealing with
dependencies outside the boundaries of single classes.",http://dx.doi.org/10.1145/3643916.3644402,90,18,16,18,20,18
Camera Calibration through Geometric Constraints from Rotation and Projection Matrices,"The process of camera calibration involves estimating the intrinsic and
extrinsic parameters, which are essential for accurately performing tasks such
as 3D reconstruction, object tracking and augmented reality. In this work, we
propose a novel constraints-based loss for measuring the intrinsic (focal
length: $(f_x, f_y)$ and principal point: $(p_x, p_y)$) and extrinsic
(baseline: ($b$), disparity: ($d$), translation: $(t_x, t_y, t_z)$, and
rotation specifically pitch: $(\theta_p)$) camera parameters. Our novel
constraints are based on geometric properties inherent in the camera model,
including the anatomy of the projection matrix (vanishing points, image of
world origin, axis planes) and the orthonormality of the rotation matrix. Thus
we proposed a novel Unsupervised Geometric Constraint Loss (UGCL) via a
multitask learning framework. Our methodology is a hybrid approach that employs
the learning power of a neural network to estimate the desired parameters along
with the underlying mathematical properties inherent in the camera projection
matrix. This distinctive approach not only enhances the interpretability of the
model but also facilitates a more informed learning process. Additionally, we
introduce a new CVGL Camera Calibration dataset, featuring over 900
configurations of camera parameters, incorporating 63,600 image pairs that
closely mirror real-world conditions. By training and testing on both synthetic
and real-world datasets, our proposed approach demonstrates improvements across
all parameters when compared to the state-of-the-art (SOTA) benchmarks. The
code and the updated dataset can be found here:
https://github.com/CVLABLUMS/CVGL-Camera-Calibration",http://arxiv.org/abs/2402.08437v1,90,20,18,18,18,16
Linearizing Models for Efficient yet Robust Private Inference,"The growing concern about data privacy has led to the development of private
inference (PI) frameworks in client-server applications which protects both
data privacy and model IP. However, the cryptographic primitives required yield
significant latency overhead which limits its wide-spread application. At the
same time, changing environments demand the PI service to be robust against
various naturally occurring and gradient-based perturbations. Despite several
works focused on the development of latency-efficient models suitable for PI,
the impact of these models on robustness has remained unexplored. Towards this
goal, this paper presents RLNet, a class of robust linearized networks that can
yield latency improvement via reduction of high-latency ReLU operations while
improving the model performance on both clean and corrupted images. In
particular, RLNet models provide a ""triple win ticket"" of improved
classification accuracy on clean, naturally perturbed, and gradient-based
perturbed images using a shared-mask shared-weight architecture with over an
order of magnitude fewer ReLUs than baseline models. To demonstrate the
efficacy of RLNet, we perform extensive experiments with ResNet and WRN model
variants on CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets. Our experimental
evaluations show that RLNet can yield models with up to 11.14x fewer ReLUs,
with accuracy close to the all-ReLU models, on clean, naturally perturbed, and
gradient-based perturbed images. Compared with the SoTA non-robust linearized
models at similar ReLU budgets, RLNet achieves an improvement in adversarial
accuracy of up to ~47%, naturally perturbed accuracy up to ~16.4%, while
improving clean image accuracy up to ~1.5%.",http://arxiv.org/abs/2402.05521v1,90,20,18,20,16,16
Assessing Uncertainty Estimation Methods for 3D Image Segmentation under Distribution Shifts,"In recent years, machine learning has witnessed extensive adoption across
various sectors, yet its application in medical image-based disease detection
and diagnosis remains challenging due to distribution shifts in real-world
data. In practical settings, deployed models encounter samples that differ
significantly from the training dataset, especially in the health domain,
leading to potential performance issues. This limitation hinders the
expressiveness and reliability of deep learning models in health applications.
Thus, it becomes crucial to identify methods capable of producing reliable
uncertainty estimation in the context of distribution shifts in the health
sector. In this paper, we explore the feasibility of using cutting-edge
Bayesian and non-Bayesian methods to detect distributionally shifted samples,
aiming to achieve reliable and trustworthy diagnostic predictions in
segmentation task. Specifically, we compare three distinct uncertainty
estimation methods, each designed to capture either unimodal or multimodal
aspects in the posterior distribution. Our findings demonstrate that methods
capable of addressing multimodal characteristics in the posterior distribution,
offer more dependable uncertainty estimates. This research contributes to
enhancing the utility of deep learning in healthcare, making diagnostic
predictions more robust and trustworthy.",http://arxiv.org/abs/2402.06937v1,90,19,18,18,17,18
Anomaly Unveiled: Securing Image Classification against Adversarial Patch Attacks,"Adversarial patch attacks pose a significant threat to the practical
deployment of deep learning systems. However, existing research primarily
focuses on image pre-processing defenses, which often result in reduced
classification accuracy for clean images and fail to effectively counter
physically feasible attacks. In this paper, we investigate the behavior of
adversarial patches as anomalies within the distribution of image information
and leverage this insight to develop a robust defense strategy. Our proposed
defense mechanism utilizes a clustering-based technique called DBSCAN to
isolate anomalous image segments, which is carried out by a three-stage
pipeline consisting of Segmenting, Isolating, and Blocking phases to identify
and mitigate adversarial noise. Upon identifying adversarial components, we
neutralize them by replacing them with the mean pixel value, surpassing
alternative replacement options. Our model-agnostic defense mechanism is
evaluated across multiple models and datasets, demonstrating its effectiveness
in countering various adversarial patch attacks in image classification tasks.
Our proposed approach significantly improves accuracy, increasing from 38.8\%
without the defense to 67.1\% with the defense against LaVAN and GoogleAp
attacks, surpassing prominent state-of-the-art methods such as LGS (53.86\%)
and Jujutsu (60\%)",http://arxiv.org/abs/2402.06249v1,90,19,19,18,17,17
A Benchmark Grocery Dataset of Realworld Point Clouds From Single View,"Fine-grained grocery object recognition is an important computer vision
problem with broad applications in automatic checkout, in-store robotic
navigation, and assistive technologies for the visually impaired. Existing
datasets on groceries are mainly 2D images. Models trained on these datasets
are limited to learning features from the regular 2D grids. While portable 3D
sensors such as Kinect were commonly available for mobile phones, sensors such
as LiDAR and TrueDepth, have recently been integrated into mobile phones.
Despite the availability of mobile 3D sensors, there are currently no dedicated
real-world large-scale benchmark 3D datasets for grocery. In addition, existing
3D datasets lack fine-grained grocery categories and have limited training
samples. Furthermore, collecting data by going around the object versus the
traditional photo capture makes data collection cumbersome. Thus, we introduce
a large-scale grocery dataset called 3DGrocery100. It constitutes 100 classes,
with a total of 87,898 3D point clouds created from 10,755 RGB-D single-view
images. We benchmark our dataset on six recent state-of-the-art 3D point cloud
classification models. Additionally, we also benchmark the dataset on few-shot
and continual learning point cloud classification tasks. Project Page:
https://bigdatavision.org/3DGrocery100/.",http://arxiv.org/abs/2402.07819v1,90,18,20,18,18,16
ML Framework for Wireless MAC Protocol Design,"Adaptivity, reconfigurability and intelligence are key features of the
next-generation wireless networks to meet the increasingly diverse quality of
service (QoS) requirements of the future applications. Conventional protocol
designs, however, struggle to provide flexibility and agility to changing radio
environments, traffic types and different user service requirements. In this
paper, we explore the potential of deep reinforcement learning (DRL), in
particular Proximal Policy Optimization (PPO), to design and configure
intelligent and application-specific medium access control (MAC) protocols. We
propose a framework that enables the addition, removal, or modification of
protocol features to meet individual application needs. The DRL channel access
policy design empowers the protocol to adapt and optimize in accordance with
the network and radio environment. Through extensive simulations, we
demonstrate the superior performance of the learned protocols over legacy IEEE
802.11ac in terms of throughput and latency.",http://arxiv.org/abs/2402.07208v1,90,20,18,18,18,16
Structured Satellite-UAV-Terrestrial Networks for 6G Internet of Things,"The upcoming sixth generation (6G) wireless communication network is
envisioned to cover space, air, and maritime areas, in addition to
urban-centered terrestrial coverage by the fifth generation (5G) network, to
support intelligent Internet of Things (IoT). Towards this end, we investigate
structured integration of satellites, unmanned aerial vehicles (UAVs), and
terrestrial networks, aiming to serve future universal IoT possibly with a
massive number of devices in the coverage holes of current 5G. The hybrid
satellite-UAV-terrestrial network usually leads to high system complexity, due
to the heterogeneity and dynamics of space/air/ground links. With a systematic
thinking, we propose to create and exploit hierarchies for the integrated
network. Four basic structures are discussed by learning from the synergies in
our human body. To orchestrate multiple heterogeneous basic structures, we
further propose a process-oriented on-demand coverage method, which
characterizes the system behavior as a series of events over time and is able
to tackle the system complexity elaborately. We also outline open issues for
promoting the agility and intelligence of structured satellite-UAV-terrestrial
networks in the making.",http://arxiv.org/abs/2402.07359v1,90,18,19,19,17,17
Data Distribution-based Curriculum Learning,"The order of training samples can have a significant impact on the
performance of a classifier. Curriculum learning is a method of ordering
training samples from easy to hard. This paper proposes the novel idea of a
curriculum learning approach called Data Distribution-based Curriculum Learning
(DDCL). DDCL uses the data distribution of a dataset to build a curriculum
based on the order of samples. Two types of scoring methods known as DDCL
(Density) and DDCL (Point) are used to score training samples thus determining
their training order. DDCL (Density) uses the sample density to assign scores
while DDCL (Point) utilises the Euclidean distance for scoring. We evaluate the
proposed DDCL approach by conducting experiments on multiple datasets using a
neural network, support vector machine and random forest classifier. Evaluation
results show that the application of DDCL improves the average classification
accuracy for all datasets compared to standard evaluation without any
curriculum. Moreover, analysis of the error losses for a single training epoch
reveals that convergence is faster when using DDCL over the no curriculum
method.",http://arxiv.org/abs/2402.07352v1,90,18,19,19,18,16
HYPO: Hyperspherical Out-of-Distribution Generalization,"Out-of-distribution (OOD) generalization is critical for machine learning
models deployed in the real world. However, achieving this can be fundamentally
challenging, as it requires the ability to learn invariant features across
different domains or environments. In this paper, we propose a novel framework
HYPO (HYPerspherical OOD generalization) that provably learns domain-invariant
representations in a hyperspherical space. In particular, our hyperspherical
learning algorithm is guided by intra-class variation and inter-class
separation principles -- ensuring that features from the same class (across
different training domains) are closely aligned with their class prototypes,
while different class prototypes are maximally separated. We further provide
theoretical justifications on how our prototypical learning objective improves
the OOD generalization bound. Through extensive experiments on challenging OOD
benchmarks, we demonstrate that our approach outperforms competitive baselines
and achieves superior performance. Code is available at
https://github.com/deeplearning-wisc/hypo.",http://arxiv.org/abs/2402.07785v1,90,18,20,18,16,18
Differentially Private Training of Mixture of Experts Models,"This position paper investigates the integration of Differential Privacy (DP)
in the training of Mixture of Experts (MoE) models within the field of natural
language processing. As Large Language Models (LLMs) scale to billions of
parameters, leveraging expansive datasets, they exhibit enhanced linguistic
capabilities and emergent abilities. However, this growth raises significant
computational and privacy concerns. Our study addresses these issues by
exploring the potential of MoE models, known for their computational
efficiency, and the application of DP, a standard for privacy preservation. We
present the first known attempt to train MoE models under the constraints of
DP, addressing the unique challenges posed by their architecture and the
complexities of DP integration. Our initial experimental studies demonstrate
that MoE models can be effectively trained with DP, achieving performance that
is competitive with their non-private counterparts. This initial study aims to
provide valuable insights and ignite further research in the domain of
privacy-preserving MoE models, softly laying the groundwork for prospective
developments in this evolving field.",http://arxiv.org/abs/2402.07334v1,90,18,18,20,18,16
Sourcerer: Sample-based Maximum Entropy Source Distribution Estimation,"Scientific modeling applications often require estimating a distribution of
parameters consistent with a dataset of observations - an inference task also
known as source distribution estimation. This problem can be ill-posed,
however, since many different source distributions might produce the same
distribution of data-consistent simulations. To make a principled choice among
many equally valid sources, we propose an approach which targets the maximum
entropy distribution, i.e., prioritizes retaining as much uncertainty as
possible. Our method is purely sample-based - leveraging the Sliced-Wasserstein
distance to measure the discrepancy between the dataset and simulations - and
thus suitable for simulators with intractable likelihoods. We benchmark our
method on several tasks, and show that it can recover source distributions with
substantially higher entropy without sacrificing the fidelity of the
simulations. Finally, to demonstrate the utility of our approach, we infer
source distributions for parameters of the Hodgkin-Huxley neuron model from
experimental datasets with thousands of measurements. In summary, we propose a
principled framework for inferring unique source distributions of scientific
simulator parameters while retaining as much uncertainty as possible.",http://arxiv.org/abs/2402.07808v1,90,18,18,18,18,18
Retrieval-Augmented Thought Process as Sequential Decision Making,"Large Language Models (LLMs) have demonstrated their strong ability to assist
people and show ""sparks of intelligence"". However, several open challenges
hinder their wider application: such as concerns over privacy, tendencies to
produce hallucinations, and difficulties in handling long contexts. In this
work, we address those challenges by introducing the Retrieval-Augmented
Thought Process (RATP). Given access to external knowledge, RATP formulates the
thought generation of LLMs as a multiple-step decision process. To optimize
such a thought process, RATP leverages Monte-Carlo Tree Search, and learns a
Q-value estimator that permits cost-efficient inference. In addressing the task
of question-answering with private data, where ethical and security concerns
limit LLM training methods, RATP achieves a 50% improvement over existing
in-context retrieval-augmented language models.",http://arxiv.org/abs/2402.07812v1,90,18,20,18,18,16
Can Tree Based Approaches Surpass Deep Learning in Anomaly Detection? A Benchmarking Study,"Detection of anomalous situations for complex mission-critical systems holds
paramount importance when their service continuity needs to be ensured. A major
challenge in detecting anomalies from the operational data arises due to the
imbalanced class distribution problem since the anomalies are supposed to be
rare events. This paper evaluates a diverse array of machine learning-based
anomaly detection algorithms through a comprehensive benchmark study. The paper
contributes significantly by conducting an unbiased comparison of various
anomaly detection algorithms, spanning classical machine learning including
various tree-based approaches to deep learning and outlier detection methods.
The inclusion of 104 publicly available and a few proprietary industrial
systems datasets enhances the diversity of the study, allowing for a more
realistic evaluation of algorithm performance and emphasizing the importance of
adaptability to real-world scenarios. The paper dispels the deep learning myth,
demonstrating that though powerful, deep learning is not a universal solution
in this case. We observed that recently proposed tree-based evolutionary
algorithms outperform in many scenarios. We noticed that tree-based approaches
catch a singleton anomaly in a dataset where deep learning methods fail. On the
other hand, classical SVM performs the best on datasets with more than 10%
anomalies, implying that such scenarios can be best modeled as a classification
problem rather than anomaly detection. To our knowledge, such a study on a
large number of state-of-the-art algorithms using diverse data sets, with the
objective of guiding researchers and practitioners in making informed
algorithmic choices, has not been attempted earlier.",http://arxiv.org/abs/2402.07281v1,90,19,19,18,17,17
An Investigation into Using Unsupervised Metrics to Optimise GNNs for Node Clustering,"Graph Neural Networks (GNNs) can be trained to detect communities within a
graph by learning from the duality of feature and connectivity information.
Currently, the common approach for optimisation of GNNs is to use comparisons
to ground-truth for hyperparameter tuning and model selection. In this work, we
show that nodes can be clustered into communities with GNNs by solely
optimising for modularity, without any comparison to ground-truth. Although
modularity is a graph partitioning quality metric, we show that this can be
used to optimise GNNs that also encode features without a drop in performance.
We take it a step further and also study whether the unsupervised metric
performance can predict ground-truth performance. To investigate why modularity
can be used to optimise GNNs, we design synthetic experiments that show the
limitations of this approach. The synthetic graphs are created to highlight
current capabilities in distinct, random and zero information space partitions
in attributed graphs. We conclude that modularity can be used for
hyperparameter optimisation and model selection on real-world datasets as well
as being a suitable proxy for predicting ground-truth performance, however,
GNNs fail to balance the information duality when the spaces contain
conflicting signals.",http://arxiv.org/abs/2402.07845v1,90,18,20,17,18,17
Informativeness of Reward Functions in Reinforcement Learning,"Reward functions are central in specifying the task we want a reinforcement
learning agent to perform. Given a task and desired optimal behavior, we study
the problem of designing informative reward functions so that the designed
rewards speed up the agent's convergence. In particular, we consider
expert-driven reward design settings where an expert or teacher seeks to
provide informative and interpretable rewards to a learning agent. Existing
works have considered several different reward design formulations; however,
the key challenge is formulating a reward informativeness criterion that adapts
w.r.t. the agent's current policy and can be optimized under specified
structural constraints to obtain interpretable rewards. In this paper, we
propose a novel reward informativeness criterion, a quantitative measure that
captures how the agent's current policy will improve if it receives rewards
from a specific reward function. We theoretically showcase the utility of the
proposed informativeness criterion for adaptively designing rewards for an
agent. Experimental results on two navigation tasks demonstrate the
effectiveness of our adaptive reward informativeness criterion.",http://arxiv.org/abs/2402.07019v1,90,19,18,18,17,18
Bayesian Optimization with Adaptive Kernels for Robot Control,"Active policy search combines the trial-and-error methodology from policy
search with Bayesian optimization to actively find the optimal policy. First,
policy search is a type of reinforcement learning which has become very popular
for robot control, for its ability to deal with complex continuous state and
action spaces. Second, Bayesian optimization is a sample efficient global
optimization method that uses a surrogate model, like a Gaussian process, and
optimal decision making to carefully select each sample during the optimization
process. Sample efficiency is of paramount importance when each trial involves
the real robot, expensive Monte Carlo runs, or a complex simulator. Black-box
Bayesian optimization generally assumes a cost function from a stationary
process, because nonstationary modeling is usually based on prior knowledge.
However, many control problems are inherently nonstationary due to their
failure conditions, terminal states and other abrupt effects. In this paper, we
present a kernel function specially designed for Bayesian optimization, that
allows nonstationary modeling without prior knowledge, using an adaptive local
region. The new kernel results in an improved local search (exploitation),
without penalizing the global search (exploration), as shown experimentally in
well-known optimization benchmarks and robot control scenarios. We finally show
its potential for the design of the wing shape of a UAV.",http://dx.doi.org/10.1109/ICRA.2017.7989380,90,19,17,19,18,17
Large Language Models are Few-shot Generators: Proposing Hybrid Prompt Algorithm To Generate Webshell Escape Samples,"The frequent occurrence of cyber-attacks has made webshell attacks and
defense gradually become a research hotspot in the field of network security.
However, the lack of publicly available benchmark datasets and the
over-reliance on manually defined rules for webshell escape sample generation
have slowed down the progress of research related to webshell escape sample
generation strategies and artificial intelligence-based webshell detection
algorithms. To address the drawbacks of weak webshell sample escape
capabilities, the lack of webshell datasets with complex malicious features,
and to promote the development of webshell detection technology, we propose the
Hybrid Prompt algorithm for webshell escape sample generation with the help of
large language models. As a prompt algorithm specifically developed for
webshell sample generation, the Hybrid Prompt algorithm not only combines
various prompt ideas including Chain of Thought, Tree of Thought, but also
incorporates various components such as webshell hierarchical module and
few-shot example to facilitate the LLM in learning and reasoning webshell
escape strategies. Experimental results show that the Hybrid Prompt algorithm
can work with multiple LLMs with excellent code reasoning ability to generate
high-quality webshell samples with high Escape Rate (88.61% with GPT-4 model on
VIRUSTOTAL detection engine) and Survival Rate (54.98% with GPT-4 model).",http://arxiv.org/abs/2402.07408v1,90,18,19,19,18,16
Divide and Conquer: Provably Unveiling the Pareto Front with Multi-Objective Reinforcement Learning,"A significant challenge in multi-objective reinforcement learning is
obtaining a Pareto front of policies that attain optimal performance under
different preferences. We introduce Iterated Pareto Referent Optimisation
(IPRO), a principled algorithm that decomposes the task of finding the Pareto
front into a sequence of single-objective problems for which various solution
methods exist. This enables us to establish convergence guarantees while
providing an upper bound on the distance to undiscovered Pareto optimal
solutions at each step. Empirical evaluations demonstrate that IPRO matches or
outperforms methods that require additional domain knowledge. By leveraging
problem-specific single-objective solvers, our approach also holds promise for
applications beyond multi-objective reinforcement learning, such as in
pathfinding and optimisation.",http://arxiv.org/abs/2402.07182v1,90,18,19,19,18,16
Instance-Level Safety-Aware Fidelity of Synthetic Data and Its Calibration,"Modeling and calibrating the fidelity of synthetic data is paramount in
shaping the future of safe and reliable self-driving technology by offering a
cost-effective and scalable alternative to real-world data collection. We focus
on its role in safety-critical applications, introducing four types of
instance-level fidelity that go beyond mere visual input characteristics. The
aim is to align synthetic data with real-world safety issues. We suggest an
optimization method to refine the synthetic data generator, reducing fidelity
gaps identified by the DNN-based component. Our findings show this tuning
enhances the correlation between safety-critical errors in synthetic and real
images.",http://arxiv.org/abs/2402.07031v1,90,20,18,18,18,16
Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models,"Large Language Models (LLMs) based on Mixture-of-Experts (MoE) architecture
are showing promising performance on various tasks. However, running them on
resource-constrained settings, where GPU memory resources are not abundant, is
challenging due to huge model sizes. Existing systems that offload model
weights to CPU memory suffer from the significant overhead of frequently moving
data between CPU and GPU. In this paper, we propose Fiddler, a
resource-efficient inference engine with CPU-GPU orchestration for MoE models.
The key idea of Fiddler is to use the computation ability of the CPU to
minimize the data movement between the CPU and GPU. Our evaluation shows that
Fiddler can run the uncompressed Mixtral-8x7B model, which exceeds 90GB in
parameters, to generate over $3$ tokens per second on a single GPU with 24GB
memory, showing an order of magnitude improvement over existing methods. The
code of Fiddler is publicly available at
\url{https://github.com/efeslab/fiddler}",http://arxiv.org/abs/2402.07033v1,90,18,20,18,18,16
MODIPHY: Multimodal Obscured Detection for IoT using PHantom Convolution-Enabled Faster YOLO,"Low-light conditions and occluded scenarios impede object detection in
real-world Internet of Things (IoT) applications like autonomous vehicles and
security systems. While advanced machine learning models strive for accuracy,
their computational demands clash with the limitations of resource-constrained
devices, hampering real-time performance. In our current research, we tackle
this challenge, by introducing ""YOLO Phantom"", one of the smallest YOLO models
ever conceived. YOLO Phantom utilizes the novel Phantom Convolution block,
achieving comparable accuracy to the latest YOLOv8n model while simultaneously
reducing both parameters and model size by 43%, resulting in a significant 19%
reduction in Giga Floating Point Operations (GFLOPs). YOLO Phantom leverages
transfer learning on our multimodal RGB-infrared dataset to address low-light
and occlusion issues, equipping it with robust vision under adverse conditions.
Its real-world efficacy is demonstrated on an IoT platform with advanced
low-light and RGB cameras, seamlessly connecting to an AWS-based notification
endpoint for efficient real-time object detection. Benchmarks reveal a
substantial boost of 17% and 14% in frames per second (FPS) for thermal and RGB
detection, respectively, compared to the baseline YOLOv8n model. For community
contribution, both the code and the multimodal dataset are available on GitHub.",http://arxiv.org/abs/2402.07894v1,90,18,20,20,16,16
PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs,"Vision language models (VLMs) have shown impressive capabilities across a
variety of tasks, from logical reasoning to visual understanding. This opens
the door to richer interaction with the world, for example robotic control.
However, VLMs produce only textual outputs, while robotic control and other
spatial tasks require outputting continuous coordinates, actions, or
trajectories. How can we enable VLMs to handle such settings without
fine-tuning on task-specific data?
  In this paper, we propose a novel visual prompting approach for VLMs that we
call Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as
iterative visual question answering. In each iteration, the image is annotated
with a visual representation of proposals that the VLM can refer to (e.g.,
candidate robot actions, localizations, or trajectories). The VLM then selects
the best ones for the task. These proposals are iteratively refined, allowing
the VLM to eventually zero in on the best available answer. We investigate
PIVOT on real-world robotic navigation, real-world manipulation from images,
instruction following in simulation, and additional spatial inference tasks
such as localization. We find, perhaps surprisingly, that our approach enables
zero-shot control of robotic systems without any robot training data,
navigation in a variety of environments, and other capabilities. Although
current performance is far from perfect, our work highlights potentials and
limitations of this new regime and shows a promising approach for
Internet-Scale VLMs in robotic and spatial reasoning domains. Website:
pivot-prompt.github.io and HuggingFace:
https://huggingface.co/spaces/pivot-prompt/pivot-prompt-demo.",http://arxiv.org/abs/2402.07872v1,90,20,18,18,16,18
"Towards Robust Car Following Dynamics Modeling via Blackbox Models: Methodology, Analysis, and Recommendations","The selection of the target variable is important while learning parameters
of the classical car following models like GIPPS, IDM, etc. There is a vast
body of literature on which target variable is optimal for classical car
following models, but there is no study that empirically evaluates the
selection of optimal target variables for black-box models, such as LSTM, etc.
The black-box models, like LSTM and Gaussian Process (GP) are increasingly
being used to model car following behavior without wise selection of target
variables. The current work tests different target variables, like
acceleration, velocity, and headway, for three black-box models, i.e., GP,
LSTM, and Kernel Ridge Regression. These models have different objective
functions and work in different vector spaces, e.g., GP works in function
space, and LSTM works in parameter space. The experiments show that the optimal
target variable recommendations for black-box models differ from classical car
following models depending on the objective function and the vector space. It
is worth mentioning that models and datasets used during evaluation are diverse
in nature: the datasets contained both automated and human-driven vehicle
trajectories; the black-box models belong to both parametric and non-parametric
classes of models. This diversity is important during the analysis of variance,
wherein we try to find the interaction between datasets, models, and target
variables. It is shown that the models and target variables interact and
recommended target variables don't depend on the dataset under consideration.",http://arxiv.org/abs/2402.07139v1,90,18,20,18,18,16
Label-Efficient Model Selection for Text Generation,"Model selection for a given target task can be costly, as it may entail
extensive annotation of the quality of outputs of different models. We
introduce DiffUse, an efficient method to make an informed decision between
candidate text generation models. DiffUse reduces the required amount of
preference annotations, thus saving valuable time and resources in performing
evaluation. DiffUse intelligently selects instances by clustering embeddings
that represent the semantic differences between model outputs. Thus, it is able
to identify a subset of examples that are more informative for preference
decisions. Our method is model-agnostic, and can be applied to any text
generation model. Moreover, we propose a practical iterative approach for
dynamically determining how many instances to annotate. In a series of
experiments over hundreds of model pairs, we demonstrate that DiffUse can
dramatically reduce the required number of annotations -- by up to 75% -- while
maintaining high evaluation reliability.",http://arxiv.org/abs/2402.07891v1,90,18,17,20,18,17
Echoes of Socratic Doubt: Embracing Uncertainty in Calibrated Evidential Reinforcement Learning,"We present a novel statistical approach to incorporating uncertainty
awareness in model-free distributional reinforcement learning involving
quantile regression-based deep Q networks. The proposed algorithm,
$\textit{Calibrated Evidential Quantile Regression in Deep Q Networks
(CEQR-DQN)}$, aims to address key challenges associated with separately
estimating aleatoric and epistemic uncertainty in stochastic environments. It
combines deep evidential learning with quantile calibration based on principles
of conformal inference to provide explicit, sample-free computations of
$\textit{global}$ uncertainty as opposed to $\textit{local}$ estimates based on
simple variance, overcoming limitations of traditional methods in computational
and statistical efficiency and handling of out-of-distribution (OOD)
observations. Tested on a suite of miniaturized Atari games (i.e., MinAtar),
CEQR-DQN is shown to surpass similar existing frameworks in scores and learning
speed. Its ability to rigorously evaluate uncertainty improves exploration
strategies and can serve as a blueprint for other algorithms requiring
uncertainty awareness.",http://arxiv.org/abs/2402.07107v2,90,19,18,19,18,16
The Relevance Feature and Vector Machine for health applications,"This paper presents the Relevance Feature and Vector Machine (RFVM), a novel
model that addresses the challenges of the fat-data problem when dealing with
clinical prospective studies. The fat-data problem refers to the limitations of
Machine Learning (ML) algorithms when working with databases in which the
number of features is much larger than the number of samples (a common scenario
in certain medical fields). To overcome such limitations, the RFVM incorporates
different characteristics: (1) A Bayesian formulation which enables the model
to infer its parameters without overfitting thanks to the Bayesian model
averaging. (2) A joint optimisation that overcomes the limitations arising from
the fat-data characteristic by simultaneously including the variables that
define the primal space (features) and those that define the dual space
(observations). (3) An integrated prunning that removes the irrelevant features
and samples during the training iterative optimization. Also, this last point
turns out crucial when performing medical prospective studies, enabling
researchers to exclude unnecessary medical tests, reducing costs and
inconvenience for patients, and identifying the critical patients/subjects that
characterize the disorder and, subsequently, optimize the patient recruitment
process that leads to a balanced cohort. The model capabilities are tested
against state-of-the-art models in several medical datasets with fat-data
problems. These experimental works show that RFVM is capable of achieving
competitive classification accuracies while providing the most compact subset
of data (in both terms of features and samples). Moreover, the selected
features (medical tests) seem to be aligned with the existing medical
literature.",http://arxiv.org/abs/2402.07079v1,90,18,18,18,18,18
BDIQA: A New Dataset for Video Question Answering to Explore Cognitive Reasoning through Theory of Mind,"As a foundational component of cognitive intelligence, theory of mind (ToM)
can make AI more closely resemble human thought processes, thereby enhancing
their interaction and collaboration with human. In particular, it can
significantly improve a model's comprehension of videos in complex scenes.
However, current video question answer (VideoQA) datasets focus on studying
causal reasoning within events few of them genuinely incorporating human ToM.
Consequently, there is a lack of development in ToM reasoning tasks within the
area of VideoQA. This paper presents BDIQA, the first benchmark to explore the
cognitive reasoning capabilities of VideoQA models in the context of ToM. BDIQA
is inspired by the cognitive development of children's ToM and addresses the
current deficiencies in machine ToM within datasets and tasks. Specifically, it
offers tasks at two difficulty levels, assessing Belief, Desire and Intention
(BDI) reasoning in both simple and complex scenarios. We conduct evaluations on
several mainstream methods of VideoQA and diagnose their capabilities with zero
shot, few shot and supervised learning. We find that the performance of
pre-trained models on cognitive reasoning tasks remains unsatisfactory. To
counter this challenge, we undertake thorough analysis and experimentation,
ultimately presenting two guidelines to enhance cognitive reasoning derived
from ablation analysis.",http://arxiv.org/abs/2402.07402v1,90,18,20,18,16,18
A Theoretical Analysis of Nash Learning from Human Feedback under General KL-Regularized Preference,"Reinforcement Learning from Human Feedback (RLHF) learns from the preference
signal provided by a probabilistic preference model, which takes a prompt and
two responses as input, and produces a score indicating the preference of one
response against another. So far, the most popular RLHF paradigm is
reward-based, which starts with an initial step of reward modeling, and the
constructed reward is then used to provide a reward signal for the subsequent
reward optimization stage. However, the existence of a reward function is a
strong assumption and the reward-based RLHF is limited in expressivity and
cannot capture the real-world complicated human preference.
  In this work, we provide theoretical insights for a recently proposed
learning paradigm, Nash learning from human feedback (NLHF), which considered a
general preference model and formulated the alignment process as a game between
two competitive LLMs. The learning objective is to find a policy that
consistently generates responses preferred over any competing policy while
staying close to the initial model. The objective is defined as the Nash
equilibrium (NE) of the KL-regularized preference model. We aim to make the
first attempt to study the theoretical learnability of the KL-regularized NLHF
by considering both offline and online settings. For the offline learning from
a pre-collected dataset, we propose algorithms that are efficient under
suitable coverage conditions of the dataset. For batch online learning from
iterative interactions with a preference oracle, our proposed algorithm enjoys
a finite sample guarantee under the structural condition of the underlying
preference model. Our results connect the new NLHF paradigm with traditional RL
theory, and validate the potential of reward-model-free learning under general
preference.",http://arxiv.org/abs/2402.07314v1,90,18,16,18,20,18
Refined Direct Preference Optimization with Synthetic Data for Behavioral Alignment of LLMs,"In this paper, we introduce \emph{refined Direct Preference Optimization}
(rDPO), a method for improving the behavioral alignment of Large Language
Models (LLMs) without the need for human-annotated data. The method involves
creating synthetic data using self-critique prompting by a teacher LLM and then
utilising a generalized DPO loss function to distil to a student LLM. The loss
function incorporates an additional external reward model to improve the
quality of synthetic data, making rDPO robust to potential noise in the
synthetic dataset. rDPO is shown to be effective in a diverse set of
behavioural alignment tasks, such as improved safety, robustness against
role-playing, and reduced sycophancy. Code to be released at
https://github.com/vicgalle/refined-dpo.",http://arxiv.org/abs/2402.08005v1,90,19,18,19,17,17
Bandit-Feedback Online Multiclass Classification: Variants and Tradeoffs,"Consider the domain of multiclass classification within the adversarial
online setting. What is the price of relying on bandit feedback as opposed to
full information? To what extent can an adaptive adversary amplify the loss
compared to an oblivious one? To what extent can a randomized learner reduce
the loss compared to a deterministic one? We study these questions in the
mistake bound model and provide nearly tight answers.
  We demonstrate that the optimal mistake bound under bandit feedback is at
most $O(k)$ times higher than the optimal mistake bound in the full information
case, where $k$ represents the number of labels. This bound is tight and
provides an answer to an open question previously posed and studied by Daniely
and Helbertal ['13] and by Long ['17, '20], who focused on deterministic
learners.
  Moreover, we present nearly optimal bounds of $\tilde{\Theta}(k)$ on the gap
between randomized and deterministic learners, as well as between adaptive and
oblivious adversaries in the bandit feedback setting. This stands in contrast
to the full information scenario, where adaptive and oblivious adversaries are
equivalent, and the gap in mistake bounds between randomized and deterministic
learners is a constant multiplicative factor of $2$.
  In addition, our results imply that in some cases the optimal randomized
mistake bound is approximately the square-root of its deterministic parallel.
Previous results show that this is essentially the smallest it can get.",http://arxiv.org/abs/2402.07453v1,90,18,19,19,17,17
Food Recommendation as Language Processing (F-RLP): A Personalized and Contextual Paradigm,"State-of-the-art rule-based and classification-based food recommendation
systems face significant challenges in becoming practical and useful. This
difficulty arises primarily because most machine learning models struggle with
problems characterized by an almost infinite number of classes and a limited
number of samples within an unbalanced dataset. Conversely, the emergence of
Large Language Models (LLMs) as recommendation engines offers a promising
avenue. However, a general-purpose Recommendation as Language Processing (RLP)
approach lacks the critical components necessary for effective food
recommendations. To address this gap, we introduce Food Recommendation as
Language Processing (F-RLP), a novel framework that offers a food-specific,
tailored infrastructure. F-RLP leverages the capabilities of LLMs to maximize
their potential, thereby paving the way for more accurate, personalized food
recommendations.",http://arxiv.org/abs/2402.07477v1,90,18,18,20,18,16
Online Sequential Decision-Making with Unknown Delays,"In the field of online sequential decision-making, we address the problem
with delays utilizing the framework of online convex optimization (OCO), where
the feedback of a decision can arrive with an unknown delay. Unlike previous
research that is limited to Euclidean norm and gradient information, we propose
three families of delayed algorithms based on approximate solutions to handle
different types of received feedback. Our proposed algorithms are versatile and
applicable to universal norms. Specifically, we introduce a family of Follow
the Delayed Regularized Leader algorithms for feedback with full information on
the loss function, a family of Delayed Mirror Descent algorithms for feedback
with gradient information on the loss function and a family of Simplified
Delayed Mirror Descent algorithms for feedback with the value information of
the loss function's gradients at corresponding decision points. For each type
of algorithm, we provide corresponding regret bounds under cases of general
convexity and relative strong convexity, respectively. We also demonstrate the
efficiency of each algorithm under different norms through concrete examples.
Furthermore, our theoretical results are consistent with the current best
bounds when degenerated to standard settings.",http://arxiv.org/abs/2402.07703v1,90,18,18,18,18,18
NeuralSentinel: Safeguarding Neural Network Reliability and Trustworthiness,"The usage of Artificial Intelligence (AI) systems has increased
exponentially, thanks to their ability to reduce the amount of data to be
analyzed, the user efforts and preserving a high rate of accuracy. However,
introducing this new element in the loop has converted them into attacked
points that can compromise the reliability of the systems. This new scenario
has raised crucial challenges regarding the reliability and trustworthiness of
the AI models, as well as about the uncertainties in their response decisions,
becoming even more crucial when applied in critical domains such as healthcare,
chemical, electrical plants, etc. To contain these issues, in this paper, we
present NeuralSentinel (NS), a tool able to validate the reliability and
trustworthiness of AI models. This tool combines attack and defence strategies
and explainability concepts to stress an AI model and help non-expert staff
increase their confidence in this new system by understanding the model
decisions. NS provide a simple and easy-to-use interface for helping humans in
the loop dealing with all the needed information. This tool was deployed and
used in a Hackathon event to evaluate the reliability of a skin cancer image
detector. During the event, experts and non-experts attacked and defended the
detector, learning which factors were the most important for model
misclassification and which techniques were the most efficient. The event was
also used to detect NS's limitations and gather feedback for further
improvements.",http://arxiv.org/abs/2402.07506v1,90,18,18,18,18,18
Secret Collusion Among Generative AI Agents,"Recent capability increases in large language models (LLMs) open up
applications in which teams of communicating generative AI agents solve joint
tasks. This poses privacy and security challenges concerning the unauthorised
sharing of information, or other unwanted forms of agent coordination. Modern
steganographic techniques could render such dynamics hard to detect. In this
paper, we comprehensively formalise the problem of secret collusion in systems
of generative AI agents by drawing on relevant concepts from both the AI and
security literature. We study incentives for the use of steganography, and
propose a variety of mitigation measures. Our investigations result in a model
evaluation framework that systematically tests capabilities required for
various forms of secret collusion. We provide extensive empirical results
across a range of contemporary LLMs. While the steganographic capabilities of
current models remain limited, GPT-4 displays a capability jump suggesting the
need for continuous monitoring of steganographic frontier model capabilities.
We conclude by laying out a comprehensive research program to mitigate future
risks of collusion between generative AI models.",http://arxiv.org/abs/2402.07510v1,90,18,19,19,18,16
MAFIA: Multi-Adapter Fused Inclusive LanguAge Models,"Pretrained Language Models (PLMs) are widely used in NLP for various tasks.
Recent studies have identified various biases that such models exhibit and have
proposed methods to correct these biases. However, most of the works address a
limited set of bias dimensions independently such as gender, race, or religion.
Moreover, the methods typically involve finetuning the full model to maintain
the performance on the downstream task. In this work, we aim to modularly
debias a pretrained language model across multiple dimensions. Previous works
extensively explored debiasing PLMs using limited US-centric counterfactual
data augmentation (CDA). We use structured knowledge and a large generative
model to build a diverse CDA across multiple bias dimensions in a
semi-automated way. We highlight how existing debiasing methods do not consider
interactions between multiple societal biases and propose a debiasing model
that exploits the synergy amongst various societal biases and enables
multi-bias debiasing simultaneously. An extensive evaluation on multiple tasks
and languages demonstrates the efficacy of our approach.",http://arxiv.org/abs/2402.07519v1,90,18,18,20,18,16
ClusterTabNet: Supervised clustering method for table detection and table structure recognition,"We present a novel deep-learning-based method to cluster words in documents
which we apply to detect and recognize tables given the OCR output. We
interpret table structure bottom-up as a graph of relations between pairs of
words (belonging to the same row, column, header, as well as to the same table)
and use a transformer encoder model to predict its adjacency matrix. We
demonstrate the performance of our method on the PubTables-1M dataset as well
as PubTabNet and FinTabNet datasets. Compared to the current state-of-the-art
detection methods such as DETR and Faster R-CNN, our method achieves similar or
better accuracy, while requiring a significantly smaller model.",http://arxiv.org/abs/2402.07502v1,90,18,19,18,17,18
G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering,"Given a graph with textual attributes, we enable users to `chat with their
graph': that is, to ask questions about the graph using a conversational
interface. In response to a user's questions, our method provides textual
replies and highlights the relevant parts of the graph. While existing works
integrate large language models (LLMs) and graph neural networks (GNNs) in
various ways, they mostly focus on either conventional graph tasks (such as
node, edge, and graph classification), or on answering simple graph queries on
small or synthetic graphs. In contrast, we develop a flexible
question-answering framework targeting real-world textual graphs, applicable to
multiple applications including scene graph understanding, common sense
reasoning, and knowledge graph reasoning. Toward this goal, we first develop
our Graph Question Answering (GraphQA) benchmark with data collected from
different tasks. Then, we propose our G-Retriever approach, which integrates
the strengths of GNNs, LLMs, and Retrieval-Augmented Generation (RAG), and can
be fine-tuned to enhance graph understanding via soft prompting. To resist
hallucination and to allow for textual graphs that greatly exceed the LLM's
context window size, G-Retriever performs RAG over a graph by formulating this
task as a Prize-Collecting Steiner Tree optimization problem. Empirical
evaluations show that our method outperforms baselines on textual graph tasks
from multiple domains, scales well with larger graph sizes, and resists
hallucination. (Our codes and datasets are available at:
https://github.com/XiaoxinHe/G-Retriever.)",http://arxiv.org/abs/2402.07630v1,90,20,19,18,16,17
A Flow-based Credibility Metric for Safety-critical Pedestrian Detection,"Safety is of utmost importance for perception in automated driving (AD).
However, a prime safety concern in state-of-the art object detection is that
standard evaluation schemes utilize safety-agnostic metrics to argue sufficient
detection performance. Hence, it is imperative to leverage supplementary domain
knowledge to accentuate safety-critical misdetections during evaluation tasks.
To tackle the underspecification, this paper introduces a novel credibility
metric, called c-flow, for pedestrian bounding boxes. To this end, c-flow
relies on a complementary optical flow signal from image sequences and enhances
the analyses of safety-critical misdetections without requiring additional
labels. We implement and evaluate c-flow with a state-of-the-art pedestrian
detector on a large AD dataset. Our analysis demonstrates that c-flow allows
developers to identify safety-critical misdetections.",http://arxiv.org/abs/2402.07642v1,90,18,18,18,18,18
Unveiling Group-Specific Distributed Concept Drift: A Fairness Imperative in Federated Learning,"In the evolving field of machine learning, ensuring fairness has become a
critical concern, prompting the development of algorithms designed to mitigate
discriminatory outcomes in decision-making processes. However, achieving
fairness in the presence of group-specific concept drift remains an unexplored
frontier, and our research represents pioneering efforts in this regard.
Group-specific concept drift refers to situations where one group experiences
concept drift over time while another does not, leading to a decrease in
fairness even if accuracy remains fairly stable. Within the framework of
federated learning, where clients collaboratively train models, its distributed
nature further amplifies these challenges since each client can experience
group-specific concept drift independently while still sharing the same
underlying concept, creating a complex and dynamic environment for maintaining
fairness. One of the significant contributions of our research is the
formalization and introduction of the problem of group-specific concept drift
and its distributed counterpart, shedding light on its critical importance in
the realm of fairness. In addition, leveraging insights from prior research, we
adapt an existing distributed concept drift adaptation algorithm to tackle
group-specific distributed concept drift which utilizes a multi-model approach,
a local group-specific drift detection mechanism, and continuous clustering of
models over time. The findings from our experiments highlight the importance of
addressing group-specific concept drift and its distributed counterpart to
advance fairness in machine learning.",http://arxiv.org/abs/2402.07586v1,90,18,20,18,18,16
CAHSOR: Competence-Aware High-Speed Off-Road Ground Navigation in SE(3),"While the workspace of traditional ground vehicles is usually assumed to be
in a 2D plane, i.e., SE(2), such an assumption may not hold when they drive at
high speeds on unstructured off-road terrain: High-speed sharp turns on
high-friction surfaces may lead to vehicle rollover; Turning aggressively on
loose gravel or grass may violate the non-holonomic constraint and cause
significant lateral sliding; Driving quickly on rugged terrain will produce
extensive vibration along the vertical axis. Therefore, most offroad vehicles
are currently limited to drive only at low speeds to assure vehicle stability
and safety. In this work, we aim at empowering high-speed off-road vehicles
with competence awareness in SE(3) so that they can reason about the
consequences of taking aggressive maneuvers on different terrain with a 6-DoF
forward kinodynamic model. The model is learned from visual and inertial
Terrain Representation for Off-road Navigation (TRON) using multimodal,
self-supervised vehicle-terrain interactions. We demonstrate the efficacy of
our Competence-Aware High-Speed Off-Road (CAHSOR) navigation approach on a
physical ground robot in both an autonomous navigation and a human
shared-control setup and show that CAHSOR can efficiently reduce vehicle
instability by 62% while only compromising 8.6% average speed with the help of
TRON.",http://arxiv.org/abs/2402.07065v1,89,18,19,18,17,17
Differentially Private Model-Based Offline Reinforcement Learning,"We address offline reinforcement learning with privacy guarantees, where the
goal is to train a policy that is differentially private with respect to
individual trajectories in the dataset. To achieve this, we introduce DP-MORL,
an MBRL algorithm coming with differential privacy guarantees. A private model
of the environment is first learned from offline data using DP-FedAvg, a
training method for neural networks that provides differential privacy
guarantees at the trajectory level. Then, we use model-based policy
optimization to derive a policy from the (penalized) private model, without any
further interaction with the system or access to the input data. We empirically
show that DP-MORL enables the training of private RL agents from offline data
and we furthermore outline the price of privacy in this setting.",http://arxiv.org/abs/2402.05525v1,88,17,18,19,17,17
Investigating Out-of-Distribution Generalization of GNNs: An Architecture Perspective,"Graph neural networks (GNNs) have exhibited remarkable performance under the
assumption that test data comes from the same distribution of training data.
However, in real-world scenarios, this assumption may not always be valid.
Consequently, there is a growing focus on exploring the Out-of-Distribution
(OOD) problem in the context of graphs. Most existing efforts have primarily
concentrated on improving graph OOD generalization from two
\textbf{model-agnostic} perspectives: data-driven methods and strategy-based
learning. However, there has been limited attention dedicated to investigating
the impact of well-known \textbf{GNN model architectures} on graph OOD
generalization, which is orthogonal to existing research. In this work, we
provide the first comprehensive investigation of OOD generalization on graphs
from an architecture perspective, by examining the common building blocks of
modern GNNs. Through extensive experiments, we reveal that both the graph
self-attention mechanism and the decoupled architecture contribute positively
to graph OOD generalization. In contrast, we observe that the linear
classification layer tends to compromise graph OOD generalization capability.
Furthermore, we provide in-depth theoretical insights and discussions to
underpin these discoveries. These insights have empowered us to develop a novel
GNN backbone model, DGAT, designed to harness the robust properties of both
graph self-attention mechanism and the decoupled architecture. Extensive
experimental results demonstrate the effectiveness of our model under graph
OOD, exhibiting substantial and consistent enhancements across various training
strategies.",http://arxiv.org/abs/2402.08228v1,88,19,18,20,17,14
Assessing Generalization for Subpopulation Representative Modeling via In-Context Learning,"This study evaluates the ability of Large Language Model (LLM)-based
Subpopulation Representative Models (SRMs) to generalize from empirical data,
utilizing in-context learning with data from the 2016 and 2020 American
National Election Studies. We explore generalization across response variables
and demographic subgroups. While conditioning with empirical data improves
performance on the whole, the benefit of in-context learning varies
considerably across demographics, sometimes hurting performance for one
demographic while helping performance for others. The inequitable benefits of
in-context learning for SRM present a challenge for practitioners implementing
SRMs, and for decision-makers who might come to rely on them. Our work
highlights a need for fine-grained benchmarks captured from diverse
subpopulations that test not only fidelity but generalization.",http://arxiv.org/abs/2402.07368v1,88,18,16,18,18,18
Asynchronous Distributed Coordinated Hybrid Precoding in Multi-cell mmWave Wireless Networks,"Asynchronous distributed hybrid beamformers (ADBF) are conceived for
minimizing the total transmit power subject to
signal-to-interference-plus-noise ratio (SINR) constraints at the users. Our
design requires only limited information exchange between the base stations
(BSs) of the mmWave multi-cell coordinated (MCC) networks considered. To begin
with, a semidefinite relaxation (SDR)-based fully-digital (FD) beamformer is
designed for a centralized MCC system. Subsequently, a Bayesian learning (BL)
technique is harnessed for decomposing the FD beamformer into its analog and
baseband components and construct a hybrid transmit precoder (TPC). However,
the centralized TPC design requires global channel state information (CSI),
hence it results in a high signaling overhead. An alternating direction based
method of multipliers (ADMM) technique is developed for a synchronous
distributed beamformer (SDBF) design, which relies only on limited information
exchange among the BSs, thus reducing the signaling overheads required by the
centralized TPC design procedure.
  However, the SDBF design is challenging, since it requires the updates from
the BSs to be strictly synchronized. As a remedy, an ADBF framework is
developed that mitigates the inter-cell interference (ICI) and also control the
asynchrony in the system.
  Furthermore, the above ADBF framework is also extended to the robust ADBF
(R-ADBF) algorithm that incorporates the CSI uncertainty into the design
procedure for minimizing the the worst-case transmit power. Our simulation
results illustrate both the enhanced performance and the improved convergence
properties of the ADMM-based ADBF and R-ADBF schemes.",http://dx.doi.org/10.1109/OJVT.2023.3349151,88,18,16,18,20,16
SliceGPT: Compress Large Language Models by Deleting Rows and Columns,"Large language models have become the cornerstone of natural language
processing, but their use comes with substantial costs in terms of compute and
memory resources. Sparsification provides a solution to alleviate these
resource constraints, and recent works have shown that trained models can be
sparsified post-hoc. Existing sparsification techniques face challenges as they
need additional data structures and offer constrained speedup with current
hardware. In this paper we present SliceGPT, a new post-training sparsification
scheme which replaces each weight matrix with a smaller (dense) matrix,
reducing the embedding dimension of the network. Through extensive
experimentation, we show that SliceGPT can remove up to 25% of the model
parameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models
while maintaining 99%, 99% and 90% zero-shot task performance of the dense
model respectively. Our sliced models run on fewer GPUs and run faster without
any additional code optimization: on 24GB consumer GPUs we reduce the total
compute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GB
A100 GPUs we reduce it to 66%. We offer a new insight, computational invariance
in transformer networks, which enables SliceGPT and we hope it will inspire and
enable future avenues to reduce memory and computation demands for pre-trained
models. Code is available at:
https://github.com/microsoft/TransformerCompression",http://arxiv.org/abs/2401.15024v2,87,18,18,17,17,17
Active Preference Learning for Large Language Models,"As large language models (LLMs) become more capable, fine-tuning techniques
for aligning with human intent are increasingly important. A key consideration
for aligning these models is how to most effectively use human resources, or
model resources in the case where LLMs themselves are used as oracles.
Reinforcement learning from Human or AI preferences (RLHF/RLAIF) is the most
prominent example of such a technique, but is complex and often unstable.
Direct Preference Optimization (DPO) has recently been proposed as a simpler
and more stable alternative. In this work, we develop an active learning
strategy for DPO to make better use of preference labels. We propose a
practical acquisition function for prompt/completion pairs based on the
predictive entropy of the language model and a measure of certainty of the
implicit preference model optimized by DPO. We demonstrate how our approach
improves both the rate of learning and final performance of fine-tuning on
pairwise preference data.",http://arxiv.org/abs/2402.08114v1,87,18,17,18,17,17
Learning time-dependent PDE via graph neural networks and deep operator network for robust accuracy on irregular grids,"Scientific computing using deep learning has seen significant advancements in
recent years. There has been growing interest in models that learn the operator
from the parameters of a partial differential equation (PDE) to the
corresponding solutions. Deep Operator Network (DeepONet) and Fourier Neural
operator, among other models, have been designed with structures suitable for
handling functions as inputs and outputs, enabling real-time predictions as
surrogate models for solution operators. There has also been significant
progress in the research on surrogate models based on graph neural networks
(GNNs), specifically targeting the dynamics in time-dependent PDEs. In this
paper, we propose GraphDeepONet, an autoregressive model based on GNNs, to
effectively adapt DeepONet, which is well-known for successful operator
learning. GraphDeepONet exhibits robust accuracy in predicting solutions
compared to existing GNN-based PDE solver models. It maintains consistent
performance even on irregular grids, leveraging the advantages inherited from
DeepONet and enabling predictions on arbitrary grids. Additionally, unlike
traditional DeepONet and its variants, GraphDeepONet enables time extrapolation
for time-dependent PDE solutions. We also provide theoretical analysis of the
universal approximation capability of GraphDeepONet in approximating continuous
operators across arbitrary time intervals.",http://arxiv.org/abs/2402.08187v1,87,18,19,18,16,16
pFedMoE: Data-Level Personalization with Mixture of Experts for Model-Heterogeneous Personalized Federated Learning,"Federated learning (FL) has been widely adopted for collaborative training on
decentralized data. However, it faces the challenges of data, system, and model
heterogeneity. This has inspired the emergence of model-heterogeneous
personalized federated learning (MHPFL). Nevertheless, the problem of ensuring
data and model privacy, while achieving good model performance and keeping
communication and computation costs low remains open in MHPFL. To address this
problem, we propose a model-heterogeneous personalized Federated learning with
Mixture of Experts (pFedMoE) method. It assigns a shared homogeneous small
feature extractor and a local gating network for each client's local
heterogeneous large model. Firstly, during local training, the local
heterogeneous model's feature extractor acts as a local expert for personalized
feature (representation) extraction, while the shared homogeneous small feature
extractor serves as a global expert for generalized feature extraction. The
local gating network produces personalized weights for extracted
representations from both experts on each data sample. The three models form a
local heterogeneous MoE. The weighted mixed representation fuses generalized
and personalized features and is processed by the local heterogeneous large
model's header with personalized prediction information. The MoE and prediction
header are updated simultaneously. Secondly, the trained local homogeneous
small feature extractors are sent to the server for cross-client information
fusion via aggregation. Overall, pFedMoE enhances local model personalization
at a fine-grained data level, while supporting model heterogeneity.",http://arxiv.org/abs/2402.01350v3,86,19,18,18,16,15
Body Schema Acquisition through Active Learning,"We present an active learning algorithm for the problem of body schema
learning, i.e. estimating a kinematic model of a serial robot. The learning
process is done online using Recursive Least Squares (RLS) estimation, which
outperforms gradient methods usually applied in the literature. In addiction,
the method provides the required information to apply an active learning
algorithm to find the optimal set of robot configurations and observations to
improve the learning process. By selecting the most informative observations,
the proposed method minimizes the required amount of data. We have developed an
efficient version of the active learning algorithm to select the points in
real-time. The algorithms have been tested and compared using both simulated
environments and a real humanoid robot.",http://dx.doi.org/10.1109/ROBOT.2010.5509406,86,18,16,18,18,16
Quality and Trust in LLM-generated Code,"Machine learning models are widely used but can also often be wrong. Users
would benefit from a reliable indication of whether a given output from a given
model should be trusted, so a rational decision can be made whether to use the
output or not. For example, outputs can be associated with a confidence
measure; if this confidence measure is strongly associated with likelihood of
correctness, then the model is said to be well-calibrated. In this case, for
example, high-confidence outputs could be safely accepted, and low-confidence
outputs rejected.
  Calibration has so far been studied in non-generative (e.g., classification)
settings, especially in Software Engineering. However, generated code can quite
often be wrong: Developers need to know when they should e.g., directly use,
use after careful review, or discard model-generated code; thus Calibration is
vital in generative settings. However, the notion of correctness of generated
code is non-trivial, and thus so is Calibration. In this paper we make several
contributions. We develop a framework for evaluating the Calibration of
code-generating models. We consider several tasks, correctness criteria,
datasets, and approaches, and find that by and large generative code models are
not well-calibrated out of the box. We then show how Calibration can be
improved, using standard methods such as Platt scaling. Our contributions will
lead to better-calibrated decision-making in the current use of code generated
by language models, and offers a framework for future research to further
improve calibration methods for generative models in Software Engineering.",http://arxiv.org/abs/2402.02047v2,86,18,18,18,16,16
Multi-Level GNN Preconditioner for Solving Large Scale Problems,"Large-scale numerical simulations often come at the expense of daunting
computations. High-Performance Computing has enhanced the process, but adapting
legacy codes to leverage parallel GPU computations remains challenging.
Meanwhile, Machine Learning models can harness GPU computations effectively but
often struggle with generalization and accuracy. Graph Neural Networks (GNNs),
in particular, are great for learning from unstructured data like meshes but
are often limited to small-scale problems. Moreover, the capabilities of the
trained model usually restrict the accuracy of the data-driven solution. To
benefit from both worlds, this paper introduces a novel preconditioner
integrating a GNN model within a multi-level Domain Decomposition framework.
The proposed GNN-based preconditioner is used to enhance the efficiency of a
Krylov method, resulting in a hybrid solver that can converge with any desired
level of accuracy. The efficiency of the Krylov method greatly benefits from
the GNN preconditioner, which is adaptable to meshes of any size and shape, is
executed on GPUs, and features a multi-level approach to enforce the
scalability of the entire process. Several experiments are conducted to
validate the numerical behavior of the hybrid solver, and an in-depth analysis
of its performance is proposed to assess its competitiveness against a C++
legacy solver.",http://arxiv.org/abs/2402.08296v1,86,18,16,18,18,16
Integrating LLMs for Explainable Fault Diagnosis in Complex Systems,"This paper introduces an integrated system designed to enhance the
explainability of fault diagnostics in complex systems, such as nuclear power
plants, where operator understanding is critical for informed decision-making.
By combining a physics-based diagnostic tool with a Large Language Model, we
offer a novel solution that not only identifies faults but also provides clear,
understandable explanations of their causes and implications. The system's
efficacy is demonstrated through application to a molten salt facility,
showcasing its ability to elucidate the connections between diagnosed faults
and sensor data, answer operator queries, and evaluate historical sensor
anomalies. Our approach underscores the importance of merging model-based
diagnostics with advanced AI to improve the reliability and transparency of
autonomous systems.",http://arxiv.org/abs/2402.06695v1,86,18,18,16,18,16
Utilizing Low-Dimensional Molecular Embeddings for Rapid Chemical Similarity Search,"Nearest neighbor-based similarity searching is a common task in chemistry,
with notable use cases in drug discovery. Yet, some of the most commonly used
approaches for this task still leverage a brute-force approach. In practice
this can be computationally costly and overly time-consuming, due in part to
the sheer size of modern chemical databases. Previous computational
advancements for this task have generally relied on improvements to hardware or
dataset-specific tricks that lack generalizability. Approaches that leverage
lower-complexity searching algorithms remain relatively underexplored. However,
many of these algorithms are approximate solutions and/or struggle with typical
high-dimensional chemical embeddings. Here we evaluate whether a combination of
low-dimensional chemical embeddings and a k-d tree data structure can achieve
fast nearest neighbor queries while maintaining performance on standard
chemical similarity search benchmarks. We examine different dimensionality
reductions of standard chemical embeddings as well as a learned,
structurally-aware embedding -- SmallSA -- for this task. With this framework,
searches on over one billion chemicals execute in less than a second on a
single CPU core, five orders of magnitude faster than the brute-force approach.
We also demonstrate that SmallSA achieves competitive performance on chemical
similarity benchmarks.",http://arxiv.org/abs/2402.07970v1,86,16,18,18,20,14
ContPhy: Continuum Physical Concept Learning and Reasoning from Videos,"We introduce the Continuum Physical Dataset (ContPhy), a novel benchmark for
assessing machine physical commonsense. ContPhy complements existing physical
reasoning benchmarks by encompassing the inference of diverse physical
properties, such as mass and density, across various scenarios and predicting
corresponding dynamics. We evaluated a range of AI models and found that they
still struggle to achieve satisfactory performance on ContPhy, which shows that
the current AI models still lack physical commonsense for the continuum,
especially soft-bodies, and illustrates the value of the proposed dataset. We
also introduce an oracle model (ContPRO) that marries the particle-based
physical dynamic models with the recent large language models, which enjoy the
advantages of both models, precise dynamic predictions, and interpretable
reasoning. ContPhy aims to spur progress in perception and reasoning within
diverse physical settings, narrowing the divide between human and machine
intelligence in understanding the physical world. Project page:
https://physical-reasoning-project.github.io.",http://arxiv.org/abs/2402.06119v1,86,17,19,17,16,17
Intelligent Diagnosis of Alzheimer's Disease Based on Machine Learning,"This study is based on the Alzheimer's Disease Neuroimaging Initiative (ADNI)
dataset and aims to explore early detection and disease progression in
Alzheimer's disease (AD). We employ innovative data preprocessing strategies,
including the use of the random forest algorithm to fill missing data and the
handling of outliers and invalid data, thereby fully mining and utilizing these
limited data resources. Through Spearman correlation coefficient analysis, we
identify some features strongly correlated with AD diagnosis. We build and test
three machine learning models using these features: random forest, XGBoost, and
support vector machine (SVM). Among them, the XGBoost model performs the best
in terms of diagnostic performance, achieving an accuracy of 91%. Overall, this
study successfully overcomes the challenge of missing data and provides
valuable insights into early detection of Alzheimer's disease, demonstrating
its unique research value and practical significance.",http://arxiv.org/abs/2402.08539v1,86,18,16,18,18,16
BERT4FCA: A Method for Bipartite Link Prediction using Formal Concept Analysis and BERT,"We propose BERT4FCA, a novel method for link prediction in bipartite
networks, using formal concept analysis (FCA) and BERT. Link prediction in
bipartite networks is an important task that can solve various practical
problems like friend recommendation in social networks and co-authorship
prediction in author-paper networks. Recent research has found that in
bipartite networks, maximal bi-cliques provide important information for link
prediction, and they can be extracted by FCA. Some FCA-based bipartite link
prediction methods have achieved good performance. However, we figured out that
their performance could be further improved because these methods did not fully
capture the rich information of the extracted maximal bi-cliques. To address
this limitation, we propose an approach using BERT, which can learn more
information from the maximal bi-cliques extracted by FCA and use them to make
link prediction. We conduct experiments on three real-world bipartite networks
and demonstrate that our method outperforms previous FCA-based methods, and
some classic methods such as matrix-factorization and node2vec.",http://arxiv.org/abs/2402.08236v1,86,18,18,18,16,16
Reinforcement Learning for Blind Stair Climbing with Legged and Wheeled-Legged Robots,"In recent years, legged and wheeled-legged robots have gained prominence for
tasks in environments predominantly created for humans across various domains.
One significant challenge faced by many of these robots is their limited
capability to navigate stairs, which hampers their functionality in multi-story
environments. This study proposes a method aimed at addressing this limitation,
employing reinforcement learning to develop a versatile controller applicable
to a wide range of robots. In contrast to the conventional velocity-based
controllers, our approach builds upon a position-based formulation of the RL
task, which we show to be vital for stair climbing. Furthermore, the
methodology leverages an asymmetric actor-critic structure, enabling the
utilization of privileged information from simulated environments during
training while eliminating the reliance on exteroceptive sensors during
real-world deployment. Another key feature of the proposed approach is the
incorporation of a boolean observation within the controller, enabling the
activation or deactivation of a stair-climbing mode. We present our results on
different quadrupeds and bipedal robots in simulation and showcase how our
method allows the balancing robot Ascento to climb 15cm stairs in the real
world, a task that was previously impossible for this robot.",http://arxiv.org/abs/2402.06143v1,86,18,17,18,16,17
Domain Generalization with Small Data,"In this work, we propose to tackle the problem of domain generalization in
the context of \textit{insufficient samples}. Instead of extracting latent
feature embeddings based on deterministic models, we propose to learn a
domain-invariant representation based on the probabilistic framework by mapping
each data point into probabilistic embeddings. Specifically, we first extend
empirical maximum mean discrepancy (MMD) to a novel probabilistic MMD that can
measure the discrepancy between mixture distributions (i.e., source domains)
consisting of a series of latent distributions rather than latent points.
Moreover, instead of imposing the contrastive semantic alignment (CSA) loss
based on pairs of latent points, a novel probabilistic CSA loss encourages
positive probabilistic embedding pairs to be closer while pulling other
negative ones apart. Benefiting from the learned representation captured by
probabilistic models, our proposed method can marriage the measurement on the
\textit{distribution over distributions} (i.e., the global perspective
alignment) and the distribution-based contrastive semantic alignment (i.e., the
local perspective alignment). Extensive experimental results on three
challenging medical datasets show the effectiveness of our proposed method in
the context of insufficient data compared with state-of-the-art methods.",http://arxiv.org/abs/2402.06150v1,86,18,20,17,18,13
Multiscale Neuroimaging Features for the Identification of Medication Class and Non-Responders in Mood Disorder Treatment,"In the clinical treatment of mood disorders, the complex behavioral symptoms
presented by patients and variability of patient response to particular
medication classes can create difficulties in providing fast and reliable
treatment when standard diagnostic and prescription methods are used.
Increasingly, the incorporation of physiological information such as
neuroimaging scans and derivatives into the clinical process promises to
alleviate some of the uncertainty surrounding this process. Particularly, if
neural features can help to identify patients who may not respond to standard
courses of anti-depressants or mood stabilizers, clinicians may elect to avoid
lengthy and side-effect-laden treatments and seek out a different, more
effective course that might otherwise not have been under consideration.
Previously, approaches for the derivation of relevant neuroimaging features
work at only one scale in the data, potentially limiting the depth of
information available for clinical decision support. In this work, we show that
the utilization of multi spatial scale neuroimaging features - particularly
resting state functional networks and functional network connectivity measures
- provide a rich and robust basis for the identification of relevant medication
class and non-responders in the treatment of mood disorders. We demonstrate
that the generated features, along with a novel approach for fast and automated
feature selection, can support high accuracy rates in the identification of
medication class and non-responders as well as the identification of novel,
multi-scale biomarkers.",http://arxiv.org/abs/2402.07858v1,86,18,16,18,18,16
Learning by Watching: A Review of Video-based Learning Approaches for Robot Manipulation,"Robot learning of manipulation skills is hindered by the scarcity of diverse,
unbiased datasets. While curated datasets can help, challenges remain in
generalizability and real-world transfer. Meanwhile, large-scale ""in-the-wild""
video datasets have driven progress in computer vision through self-supervised
techniques. Translating this to robotics, recent works have explored learning
manipulation skills by passively watching abundant videos sourced online.
Showing promising results, such video-based learning paradigms provide scalable
supervision while reducing dataset bias. This survey reviews foundations such
as video feature representation learning techniques, object affordance
understanding, 3D hand/body modeling, and large-scale robot resources, as well
as emerging techniques for acquiring robot manipulation skills from
uncontrolled video demonstrations. We discuss how learning only from observing
large-scale human videos can enhance generalization and sample efficiency for
robotic manipulation. The survey summarizes video-based learning approaches,
analyses their benefits over standard datasets, survey metrics, and benchmarks,
and discusses open challenges and future directions in this nascent domain at
the intersection of computer vision, natural language processing, and robot
learning.",http://arxiv.org/abs/2402.07127v1,86,16,18,18,20,14
Topic Modeling as Multi-Objective Contrastive Optimization,"Recent representation learning approaches enhance neural topic models by
optimizing the weighted linear combination of the evidence lower bound (ELBO)
of the log-likelihood and the contrastive learning objective that contrasts
pairs of input documents. However, document-level contrastive learning might
capture low-level mutual information, such as word ratio, which disturbs topic
modeling. Moreover, there is a potential conflict between the ELBO loss that
memorizes input details for better reconstruction quality, and the contrastive
loss which attempts to learn topic representations that generalize among input
documents. To address these issues, we first introduce a novel contrastive
learning method oriented towards sets of topic vectors to capture useful
semantics that are shared among a set of input documents. Secondly, we
explicitly cast contrastive topic modeling as a gradient-based multi-objective
optimization problem, with the goal of achieving a Pareto stationary solution
that balances the trade-off between the ELBO and the contrastive objective.
Extensive experiments demonstrate that our framework consistently produces
higher-performing neural topic models in terms of topic coherence, topic
diversity, and downstream performance.",http://arxiv.org/abs/2402.07577v1,86,17,18,18,17,16
TimEHR: Image-based Time Series Generation for Electronic Health Records,"Time series in Electronic Health Records (EHRs) present unique challenges for
generative models, such as irregular sampling, missing values, and high
dimensionality. In this paper, we propose a novel generative adversarial
network (GAN) model, TimEHR, to generate time series data from EHRs. In
particular, TimEHR treats time series as images and is based on two conditional
GANs. The first GAN generates missingness patterns, and the second GAN
generates time series values based on the missingness pattern. Experimental
results on three real-world EHR datasets show that TimEHR outperforms
state-of-the-art methods in terms of fidelity, utility, and privacy metrics.",http://arxiv.org/abs/2402.06318v1,86,18,16,18,18,16
Causal Discovery under Off-Target Interventions,"Causal graph discovery is a significant problem with applications across
various disciplines. However, with observational data alone, the underlying
causal graph can only be recovered up to its Markov equivalence class, and
further assumptions or interventions are necessary to narrow down the true
graph. This work addresses the causal discovery problem under the setting of
stochastic interventions with the natural goal of minimizing the number of
interventions performed. We propose the following stochastic intervention model
which subsumes existing adaptive noiseless interventions in the literature
while capturing scenarios such as fat-hand interventions and CRISPR gene
knockouts: any intervention attempt results in an actual intervention on a
random subset of vertices, drawn from a distribution dependent on attempted
action. Under this model, we study the two fundamental problems in causal
discovery of verification and search and provide approximation algorithms with
polylogarithmic competitive ratios and provide some preliminary experimental
results.",http://arxiv.org/abs/2402.08229v1,86,17,18,19,16,16
Approximating Families of Sharp Solutions to Fisher's Equation with Physics-Informed Neural Networks,"This paper employs physics-informed neural networks (PINNs) to solve Fisher's
equation, a fundamental representation of a reaction-diffusion system with both
simplicity and significance. The focus lies specifically in investigating
Fisher's equation under conditions of large reaction rate coefficients, wherein
solutions manifest as traveling waves, posing a challenge for numerical methods
due to the occurring steepness of the wave front. To address optimization
challenges associated with the standard PINN approach, a residual weighting
scheme is introduced. This scheme is designed to enhance the tracking of
propagating wave fronts by considering the reaction term in the
reaction-diffusion equation. Furthermore, a specific network architecture is
studied which is tailored for solutions in the form of traveling waves. Lastly,
the capacity of PINNs to approximate an entire family of solutions is assessed
by incorporating the reaction rate coefficient as an additional input to the
network architecture. This modification enables the approximation of the
solution across a broad and continuous range of reaction rate coefficients,
thus solving a class of reaction-diffusion systems using a single PINN
instance.",http://arxiv.org/abs/2402.08313v1,86,18,18,16,18,16
WebLINX: Real-World Website Navigation with Multi-Turn Dialogue,"We propose the problem of conversational web navigation, where a digital
agent controls a web browser and follows user instructions to solve real-world
tasks in a multi-turn dialogue fashion. To support this problem, we introduce
WEBLINX - a large-scale benchmark of 100K interactions across 2300 expert
demonstrations of conversational web navigation. Our benchmark covers a broad
range of patterns on over 150 real-world websites and can be used to train and
evaluate agents in diverse scenarios. Due to the magnitude of information
present, Large Language Models (LLMs) cannot process entire web pages in
real-time. To solve this bottleneck, we design a retrieval-inspired model that
efficiently prunes HTML pages by ranking relevant elements. We use the selected
elements, along with screenshots and action history, to assess a variety of
models for their ability to replicate human behavior when navigating the web.
Our experiments span from small text-only to proprietary multimodal LLMs. We
find that smaller finetuned decoders surpass the best zero-shot LLMs (including
GPT-4V), but also larger finetuned multimodal models which were explicitly
pretrained on screenshots. However, all finetuned models struggle to generalize
to unseen websites. Our findings highlight the need for large multimodal models
that can generalize to novel settings. Our code, data and models are available
for research: https://mcgill-nlp.github.io/weblinx",http://arxiv.org/abs/2402.05930v1,86,18,19,16,18,15
ODIN: Disentangled Reward Mitigates Hacking in RLHF,"In this work, we study the issue of reward hacking on the response length, a
challenge emerging in Reinforcement Learning from Human Feedback (RLHF) on
LLMs. A well-formatted, verbose but less helpful response from the LLMs can
often deceive LLMs or even human evaluators to achieve high scores. The same
issue also holds for some reward models in RL. To address the challenges in
both training and evaluation, we establish a more reliable evaluation protocol
for comparing different training configurations, which inspects the trade-off
between LLM evaluation score and response length obtained by varying training
hyperparameters. Based on this evaluation, we conduct large-scale studies,
where the results shed insights into the efficacy of hyperparameters and tricks
used in RL on mitigating length bias. We further propose to improve the reward
model by jointly training two linear heads on shared feature representations to
predict the rewards, one trained to correlate with length, and the other
trained to decorrelate with length and therefore focus more on the actual
content. We then discard the length head in RL to prevent reward hacking on
length. Experiments demonstrate that our approach almost eliminates the reward
correlation with length, and improves the obtained policy by a significant
margin.",http://arxiv.org/abs/2402.07319v1,86,18,16,18,18,16
BIKED++: A Multimodal Dataset of 1.4 Million Bicycle Image and Parametric CAD Designs,"This paper introduces a public dataset of 1.4 million procedurally-generated
bicycle designs represented parametrically, as JSON files, and as rasterized
images. The dataset is created through the use of a rendering engine which
harnesses the BikeCAD software to generate vector graphics from parametric
designs. This rendering engine is discussed in the paper and also released
publicly alongside the dataset. Though this dataset has numerous applications,
a principal motivation is the need to train cross-modal predictive models
between parametric and image-based design representations. For example, we
demonstrate that a predictive model can be trained to accurately estimate
Contrastive Language-Image Pretraining (CLIP) embeddings from a parametric
representation directly. This allows similarity relations to be established
between parametric bicycle designs and text strings or reference images.
Trained predictive models are also made public. The dataset joins the BIKED
dataset family which includes thousands of mixed-representation human-designed
bicycle models and several datasets quantifying design performance. The code
and dataset can be found at:
https://github.com/Lyleregenwetter/BIKED_multimodal/tree/main",http://arxiv.org/abs/2402.05301v2,86,18,17,17,18,16
KIX: A Metacognitive Generalization Framework,"Humans and other animals aptly exhibit general intelligence behaviors in
solving a variety of tasks with flexibility and ability to adapt to novel
situations by reusing and applying high level knowledge acquired over time. But
artificial agents are more of a specialist, lacking such generalist behaviors.
Artificial agents will require understanding and exploiting critical structured
knowledge representations. We present a metacognitive generalization framework,
Knowledge-Interaction-eXecution (KIX), and argue that interactions with objects
leveraging type space facilitate the learning of transferable interaction
concepts and generalization. It is a natural way of integrating knowledge into
reinforcement learning and promising to act as an enabler for autonomous and
generalist behaviors in artificial intelligence systems.",http://arxiv.org/abs/2402.05346v1,86,18,16,18,18,16
"SpirDet: Towards Efficient, Accurate and Lightweight Infrared Small Target Detector","In recent years, the detection of infrared small targets using deep learning
methods has garnered substantial attention due to notable advancements. To
improve the detection capability of small targets, these methods commonly
maintain a pathway that preserves high-resolution features of sparse and tiny
targets. However, it can result in redundant and expensive computations. To
tackle this challenge, we propose SpirDet, a novel approach for efficient
detection of infrared small targets. Specifically, to cope with the
computational redundancy issue, we employ a new dual-branch sparse decoder to
restore the feature map. Firstly, the fast branch directly predicts a sparse
map indicating potential small target locations (occupying only 0.5\% area of
the map). Secondly, the slow branch conducts fine-grained adjustments at the
positions indicated by the sparse map. Additionally, we design an lightweight
DO-RepEncoder based on reparameterization with the Downsampling Orthogonality,
which can effectively reduce memory consumption and inference latency.
Extensive experiments show that the proposed SpirDet significantly outperforms
state-of-the-art models while achieving faster inference speed and fewer
parameters. For example, on the IRSTD-1K dataset, SpirDet improves $MIoU$ by
4.7 and has a $7\times$ $FPS$ acceleration compared to the previous
state-of-the-art model. The code will be open to the public.",http://arxiv.org/abs/2402.05410v1,86,18,18,16,18,16
MTSA-SNN: A Multi-modal Time Series Analysis Model Based on Spiking Neural Network,"Time series analysis and modelling constitute a crucial research area.
Traditional artificial neural networks struggle with complex, non-stationary
time series data due to high computational complexity, limited ability to
capture temporal information, and difficulty in handling event-driven data. To
address these challenges, we propose a Multi-modal Time Series Analysis Model
Based on Spiking Neural Network (MTSA-SNN). The Pulse Encoder unifies the
encoding of temporal images and sequential information in a common pulse-based
representation. The Joint Learning Module employs a joint learning function and
weight allocation mechanism to fuse information from multi-modal pulse signals
complementary. Additionally, we incorporate wavelet transform operations to
enhance the model's ability to analyze and evaluate temporal information.
Experimental results demonstrate that our method achieved superior performance
on three complex time-series tasks. This work provides an effective
event-driven approach to overcome the challenges associated with analyzing
intricate temporal information. Access to the source code is available at
https://github.com/Chenngzz/MTSA-SNN}{https://github.com/Chenngzz/MTSA-SNN",http://arxiv.org/abs/2402.05423v1,86,18,18,18,16,16
Generative VS non-Generative Models in Engineering Shape Optimization,"In this work, we perform a systematic comparison of the effectiveness and
efficiency of generative and non-generative models in constructing design
spaces for novel and efficient design exploration and shape optimization. We
apply these models in the case of airfoil/hydrofoil design and conduct the
comparison on the resulting design spaces. A conventional Generative
Adversarial Network (GAN) and a state-of-the-art generative model, the
Performance-Augmented Diverse Generative Adversarial Network (PaDGAN), are
juxtaposed with a linear non-generative model based on the coupling of the
Karhunen-Lo\`eve Expansion and a physics-informed Shape Signature Vector
(SSV-KLE). The comparison demonstrates that, with an appropriate shape encoding
and a physics-augmented design space, non-generative models have the potential
to cost-effectively generate high-performing valid designs with enhanced
coverage of the design space. In this work, both approaches are applied to two
large foil profile datasets comprising real-world and artificial designs
generated through either a profile-generating parametric model or deep-learning
approach. These datasets are further enriched with integral properties of their
members' shapes as well as physics-informed parameters. Our results illustrate
that the design spaces constructed by the non-generative model outperform the
generative model in terms of design validity, generating robust latent spaces
with none or significantly fewer invalid designs when compared to generative
models. We aspire that these findings will aid the engineering design community
in making informed decisions when constructing designs spaces for shape
optimization, as we have show that under certain conditions computationally
inexpensive approaches can closely match or even outperform state-of-the art
generative models.",http://arxiv.org/abs/2402.08540v1,86,16,18,18,20,14
Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction,"Developing a generalist agent is a longstanding objective in artificial
intelligence. Previous efforts utilizing extensive offline datasets from
various tasks demonstrate remarkable performance in multitasking scenarios
within Reinforcement Learning. However, these works encounter challenges in
extending their capabilities to new tasks. Recent approaches integrate textual
guidance or visual trajectory into decision networks to provide task-specific
contextual cues, representing a promising direction. However, it is observed
that relying solely on textual guidance or visual trajectory is insufficient
for accurately conveying the contextual information of tasks. This paper
explores enhanced forms of task guidance for agents, enabling them to
comprehend gameplay instructions, thereby facilitating a ""read-to-play""
capability. Drawing inspiration from the success of multimodal instruction
tuning in visual tasks, we treat the visual-based RL task as a long-horizon
vision task and construct a set of multimodal game instructions to incorporate
instruction tuning into a decision transformer. Experimental results
demonstrate that incorporating multimodal game instructions significantly
enhances the decision transformer's multitasking and generalization
capabilities.",http://arxiv.org/abs/2402.04154v2,86,18,18,18,16,16
NoisyICL: A Little Noise in Model Parameters Calibrates In-context Learning,"In-Context Learning (ICL) is suffering from unsatisfactory performance and
under-calibration due to high prior bias and unfaithful confidence. Some
previous works fine-tuned language models for better ICL performance with
enormous datasets and computing costs. In this paper, we propose NoisyICL,
simply perturbing the model parameters by random noises to strive for better
performance and calibration. Our experiments on 2 models and 12 downstream
datasets show that NoisyICL can help ICL produce more accurate predictions. Our
further analysis indicates that NoisyICL enables the model to provide more fair
predictions, and also with less unfaithful confidence. Therefore, we believe
that NoisyICL is an effective calibration of ICL. Our experimental code is
uploaded to Github.",http://arxiv.org/abs/2402.05515v1,86,18,16,16,18,18
Signed Distance Field based Segmentation and Statistical Shape Modelling of the Left Atrial Appendage,"Patients with atrial fibrillation have a 5-7 fold increased risk of having an
ischemic stroke. In these cases, the most common site of thrombus localization
is inside the left atrial appendage (LAA) and studies have shown a correlation
between the LAA shape and the risk of ischemic stroke. These studies make use
of manual measurement and qualitative assessment of shape and are therefore
prone to large inter-observer discrepancies, which may explain the
contradictions between the conclusions in different studies. We argue that
quantitative shape descriptors are necessary to robustly characterize LAA
morphology and relate to other functional parameters and stroke risk.
  Deep Learning methods are becoming standardly available for segmenting
cardiovascular structures from high resolution images such as computed
tomography (CT), but only few have been tested for LAA segmentation.
Furthermore, the majority of segmentation algorithms produces non-smooth 3D
models that are not ideal for further processing, such as statistical shape
analysis or computational fluid modelling. In this paper we present a fully
automatic pipeline for image segmentation, mesh model creation and statistical
shape modelling of the LAA. The LAA anatomy is implicitly represented as a
signed distance field (SDF), which is directly regressed from the CT image
using Deep Learning. The SDF is further used for registering the LAA shapes to
a common template and build a statistical shape model (SSM). Based on 106
automatically segmented LAAs, the built SSM reveals that the LAA shape can be
quantified using approximately 5 PCA modes and allows the identification of two
distinct shape clusters corresponding to the so-called chicken-wing and
non-chicken-wing morphologies.",http://arxiv.org/abs/2402.07708v1,86,16,18,18,18,16
Succint Interaction-Aware Explanations,"SHAP is a popular approach to explain black-box models by revealing the
importance of individual features. As it ignores feature interactions, SHAP
explanations can be confusing up to misleading. NSHAP, on the other hand,
reports the additive importance for all subsets of features. While this does
include all interacting sets of features, it also leads to an exponentially
sized, difficult to interpret explanation. In this paper, we propose to combine
the best of these two worlds, by partitioning the features into parts that
significantly interact, and use these parts to compose a succinct,
interpretable, additive explanation. We derive a criterion by which to measure
the representativeness of such a partition for a models behavior, traded off
against the complexity of the resulting explanation. To efficiently find the
best partition out of super-exponentially many, we show how to prune
sub-optimal solutions using a statistical test, which not only improves runtime
but also helps to detect spurious interactions. Experiments on synthetic and
real world data show that our explanations are both more accurate resp. more
easily interpretable than those of SHAP and NSHAP.",http://arxiv.org/abs/2402.05566v1,86,18,16,18,18,16
A Novel Approach to Regularising 1NN classifier for Improved Generalization,"In this paper, we propose a class of non-parametric classifiers, that learn
arbitrary boundaries and generalize well.
  Our approach is based on a novel way to regularize 1NN classifiers using a
greedy approach. We refer to this class of classifiers as Watershed
Classifiers. 1NN classifiers are known to trivially over-fit but have very
large VC dimension, hence do not generalize well. We show that watershed
classifiers can find arbitrary boundaries on any dense enough dataset, and, at
the same time, have very small VC dimension; hence a watershed classifier leads
to good generalization.
  Traditional approaches to regularize 1NN classifiers are to consider $K$
nearest neighbours. Neighbourhood component analysis (NCA) proposes a way to
learn representations consistent with ($n-1$) nearest neighbour classifier,
where $n$ denotes the size of the dataset. In this article, we propose a loss
function which can learn representations consistent with watershed classifiers,
and show that it outperforms the NCA baseline.",http://arxiv.org/abs/2402.08405v1,86,18,18,16,20,14
"Towards Explainable, Safe Autonomous Driving with Language Embeddings for Novelty Identification and Active Learning: Framework and Experimental Analysis with Real-World Data Sets","This research explores the integration of language embeddings for active
learning in autonomous driving datasets, with a focus on novelty detection.
Novelty arises from unexpected scenarios that autonomous vehicles struggle to
navigate, necessitating higher-level reasoning abilities. Our proposed method
employs language-based representations to identify novel scenes, emphasizing
the dual purpose of safety takeover responses and active learning. The research
presents a clustering experiment using Contrastive Language-Image Pretrained
(CLIP) embeddings to organize datasets and detect novelties. We find that the
proposed algorithm effectively isolates novel scenes from a collection of
subsets derived from two real-world driving datasets, one vehicle-mounted and
one infrastructure-mounted. From the generated clusters, we further present
methods for generating textual explanations of elements which differentiate
scenes classified as novel from other scenes in the data pool, presenting
qualitative examples from the clustered results. Our results demonstrate the
effectiveness of language-driven embeddings in identifying novel elements and
generating explanations of data, and we further discuss potential applications
in safe takeovers, data curation, and multi-task active learning.",http://arxiv.org/abs/2402.07320v1,86,16,20,18,18,14
Unichain and Aperiodicity are Sufficient for Asymptotic Optimality of Average-Reward Restless Bandits,"We consider the infinite-horizon, average-reward restless bandit problem in
discrete time. We propose a new class of policies that are designed to drive a
progressively larger subset of arms toward the optimal distribution. We show
that our policies are asymptotically optimal with an $O(1/\sqrt{N})$ optimality
gap for an $N$-armed problem, provided that the single-armed relaxed problem is
unichain and aperiodic. Our approach departs from most existing work that
focuses on index or priority policies, which rely on the Uniform Global
Attractor Property (UGAP) to guarantee convergence to the optimum, or a
recently developed simulation-based policy, which requires a Synchronization
Assumption (SA).",http://arxiv.org/abs/2402.05689v1,86,17,18,18,17,16
On the Convergence of Zeroth-Order Federated Tuning in Large Language Models,"The confluence of Federated Learning (FL) and Large Language Models (LLMs) is
ushering in a new era in privacy-preserving natural language processing.
However, the intensive memory requirements for fine-tuning LLMs pose
significant challenges, especially when deploying on edge devices with limited
computational resources. To circumvent this, we explore the novel integration
of Memory-efficient Zeroth-Order Optimization within a federated setting, a
synergy we denote as FedMeZO. Our study is the first to examine the theoretical
underpinnings of FedMeZO in the context of LLMs, tackling key questions
regarding the influence of large parameter spaces on optimization behavior, the
establishment of convergence properties, and the identification of critical
parameters for convergence to inform personalized federated strategies. Our
extensive empirical evidence supports the theory, showing that FedMeZO not only
converges faster than traditional first-order methods such as SGD but also
significantly reduces GPU memory usage during training to levels comparable to
those during inference. Moreover, the proposed personalized FL strategy that is
built upon the theoretical insights to customize the client-wise learning rate
can effectively accelerate loss reduction. We hope our work can help to bridge
theoretical and practical aspects of federated fine-tuning for LLMs and
facilitate further development and research.",http://arxiv.org/abs/2402.05926v1,86,19,17,18,16,16
Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation,"Aligning large language models (LLMs) with human values is imperative to
mitigate potential adverse effects resulting from their misuse. Drawing from
the sociological insight that acknowledging all parties' concerns is a key
factor in shaping human values, this paper proposes a novel direction to align
LLMs by themselves: social scene simulation. To achieve this, we present
MATRIX, a novel social scene simulator that emulates realistic scenes around a
user's input query, enabling the LLM to take social consequences into account
before responding. MATRIX serves as a virtual rehearsal space, akin to a
Monopolylogue, where the LLM performs diverse roles related to the query and
practice by itself. To inject this alignment, we fine-tune the LLM with
MATRIX-simulated data, ensuring adherence to human values without compromising
inference speed. We theoretically show that the LLM with MATRIX outperforms
Constitutional AI under mild assumptions. Finally, extensive experiments
validate that our method outperforms over 10 baselines across 4 benchmarks. As
evidenced by 875 user ratings, our tuned 13B-size LLM exceeds GPT-4 in aligning
with human values. Code is available at https://github.com/pangxianghe/MATRIX.",http://arxiv.org/abs/2402.05699v1,86,18,18,18,16,16
When is Mean-Field Reinforcement Learning Tractable and Relevant?,"Mean-field reinforcement learning has become a popular theoretical framework
for efficiently approximating large-scale multi-agent reinforcement learning
(MARL) problems exhibiting symmetry. However, questions remain regarding the
applicability of mean-field approximations: in particular, their approximation
accuracy of real-world systems and conditions under which they become
computationally tractable. We establish explicit finite-agent bounds for how
well the MFG solution approximates the true $N$-player game for two popular
mean-field solution concepts. Furthermore, for the first time, we establish
explicit lower bounds indicating that MFGs are poor or uninformative at
approximating $N$-player games assuming only Lipschitz dynamics and rewards.
Finally, we analyze the computational complexity of solving MFGs with only
Lipschitz properties and prove that they are in the class of
\textsc{PPAD}-complete problems conjectured to be intractable, similar to
general sum $N$ player games. Our theoretical results underscore the
limitations of MFGs and complement and justify existing work by proving
difficulty in the absence of common theoretical assumptions.",http://arxiv.org/abs/2402.05757v1,86,18,16,18,20,14
Latent variable model for high-dimensional point process with structured missingness,"Longitudinal data are important in numerous fields, such as healthcare,
sociology and seismology, but real-world datasets present notable challenges
for practitioners because they can be high-dimensional, contain structured
missingness patterns, and measurement time points can be governed by an unknown
stochastic process. While various solutions have been suggested, the majority
of them have been designed to account for only one of these challenges. In this
work, we propose a flexible and efficient latent-variable model that is capable
of addressing all these limitations. Our approach utilizes Gaussian processes
to capture temporal correlations between samples and their associated
missingness masks as well as to model the underlying point process. We
construct our model as a variational autoencoder together with deep neural
network parameterised encoder and decoder models, and develop a scalable
amortised variational inference approach for efficient model training. We
demonstrate competitive performance using both simulated and real datasets.",http://arxiv.org/abs/2402.05758v1,86,16,20,18,16,16
Off-policy Distributional Q($$): Distributional RL without Importance Sampling,"We introduce off-policy distributional Q($\lambda$), a new addition to the
family of off-policy distributional evaluation algorithms. Off-policy
distributional Q($\lambda$) does not apply importance sampling for off-policy
learning, which introduces intriguing interactions with signed measures. Such
unique properties distributional Q($\lambda$) from other existing alternatives
such as distributional Retrace. We characterize the algorithmic properties of
distributional Q($\lambda$) and validate theoretical insights with tabular
experiments. We show how distributional Q($\lambda$)-C51, a combination of
Q($\lambda$) with the C51 agent, exhibits promising results on deep RL
benchmarks.",http://arxiv.org/abs/2402.05766v1,86,18,16,18,18,16
Topological safeguard for evasion attack interpreting the neural networks' behavior,"In the last years, Deep Learning technology has been proposed in different
fields, bringing many advances in each of them, but identifying new threats in
these solutions regarding cybersecurity. Those implemented models have brought
several vulnerabilities associated with Deep Learning technology. Moreover,
those allow taking advantage of the implemented model, obtaining private
information, and even modifying the model's decision-making. Therefore,
interest in studying those vulnerabilities/attacks and designing defenses to
avoid or fight them is gaining prominence among researchers. In particular, the
widely known evasion attack is being analyzed by researchers; thus, several
defenses to avoid such a threat can be found in the literature. Since the
presentation of the L-BFG algorithm, this threat concerns the research
community. However, it continues developing new and ingenious countermeasures
since there is no perfect defense for all the known evasion algorithms. In this
work, a novel detector of evasion attacks is developed. It focuses on the
information of the activations of the neurons given by the model when an input
sample is injected. Moreover, it puts attention to the topology of the targeted
deep learning model to analyze the activations according to which neurons are
connecting. This approach has been decided because the literature shows that
the targeted model's topology contains essential information about if the
evasion attack occurs. For this purpose, a huge data preprocessing is required
to introduce all this information in the detector, which uses the Graph
Convolutional Neural Network (GCN) technology. Thus, it understands the
topology of the target model, obtaining promising results and improving the
outcomes presented in the literature related to similar defenses.",http://dx.doi.org/10.1016/j.patcog.2023.110130,86,18,16,18,18,16
Data Quality Aware Approaches for Addressing Model Drift of Semantic Segmentation Models,"In the midst of the rapid integration of artificial intelligence (AI) into
real world applications, one pressing challenge we confront is the phenomenon
of model drift, wherein the performance of AI models gradually degrades over
time, compromising their effectiveness in real-world, dynamic environments.
Once identified, we need techniques for handling this drift to preserve the
model performance and prevent further degradation. This study investigates two
prominent quality aware strategies to combat model drift: data quality
assessment and data conditioning based on prior model knowledge. The former
leverages image quality assessment metrics to meticulously select high-quality
training data, improving the model robustness, while the latter makes use of
learned feature vectors from existing models to guide the selection of future
data, aligning it with the model's prior knowledge. Through comprehensive
experimentation, this research aims to shed light on the efficacy of these
approaches in enhancing the performance and reliability of semantic
segmentation models, thereby contributing to the advancement of computer vision
capabilities in real-world scenarios.",http://arxiv.org/abs/2402.07258v1,86,18,18,18,16,16
Conditional Information Gain Trellis,"Conditional computing processes an input using only part of the neural
network's computational units. Learning to execute parts of a deep
convolutional network by routing individual samples has several advantages:
Reducing the computational burden is an obvious advantage. Furthermore, if
similar classes are routed to the same path, that part of the network learns to
discriminate between finer differences and better classification accuracies can
be attained with fewer parameters. Recently, several papers have exploited this
idea to take a particular child of a node in a tree-shaped network or to skip
parts of a network. In this work, we follow a Trellis-based approach for
generating specific execution paths in a deep convolutional neural network. We
have designed routing mechanisms that use differentiable information gain-based
cost functions to determine which subset of features in a convolutional layer
will be executed. We call our method Conditional Information Gain Trellis
(CIGT). We show that our conditional execution mechanism achieves comparable or
better model performance compared to unconditional baselines, using only a
fraction of the computational resources.",http://arxiv.org/abs/2402.08345v1,86,17,18,18,16,17
SLIT: Boosting Audio-Text Pre-Training via Multi-Stage Learning and Instruction Tuning,"Audio-text pre-training (ATP) has witnessed remarkable strides across a
variety of downstream tasks. Yet, most existing pretrained audio models only
specialize in either discriminative tasks or generative tasks. In this study,
we develop SLIT, a novel ATP framework which transfers flexibly to both
audio-text understanding and generation tasks, bootstrapping audio-text
pre-training from frozen pretrained audio encoders and large language models.
To bridge the modality gap during pre-training, we leverage Q-Former, which
undergoes a multi-stage pre-training process. The first stage enhances
audio-text representation learning from a frozen audio encoder, while the
second stage boosts audio-to-text generative learning with a frozen language
model. Furthermore, we introduce an ATP instruction tuning strategy, which
enables flexible and informative feature extraction tailered to the given
instructions for different tasks. Experiments show that SLIT achieves superior
performances on a variety of audio-text understanding and generation tasks, and
even demonstrates strong generalization capabilities when directly applied to
zero-shot scenarios.",http://arxiv.org/abs/2402.07485v1,86,18,18,18,16,16
Generative Echo Chamber? Effects of LLM-Powered Search Systems on Diverse Information Seeking,"Large language models (LLMs) powered conversational search systems have
already been used by hundreds of millions of people, and are believed to bring
many benefits over conventional search. However, while decades of research and
public discourse interrogated the risk of search systems in increasing
selective exposure and creating echo chambers -- limiting exposure to diverse
opinions and leading to opinion polarization, little is known about such a risk
of LLM-powered conversational search. We conduct two experiments to
investigate: 1) whether and how LLM-powered conversational search increases
selective exposure compared to conventional search; 2) whether and how LLMs
with opinion biases that either reinforce or challenge the user's view change
the effect. Overall, we found that participants engaged in more biased
information querying with LLM-powered conversational search, and an opinionated
LLM reinforcing their views exacerbated this bias. These results present
critical implications for the development of LLMs and conversational search
systems, and the policy governing these technologies.",http://arxiv.org/abs/2402.05880v2,86,18,16,18,18,16
Personalizing Driver Safety Interfaces via Driver Cognitive Factors Inference,"Recent advances in AI and intelligent vehicle technology hold promise to
revolutionize mobility and transportation, in the form of advanced driving
assistance (ADAS) interfaces. Although it is widely recognized that certain
cognitive factors, such as impulsivity and inhibitory control, are related to
risky driving behavior, play a significant role in on-road risk-taking,
existing systems fail to leverage such factors. Varying levels of these
cognitive factors could influence the effectiveness and acceptance of driver
safety interfaces.
  We demonstrate an approach for personalizing driver interaction via driver
safety interfaces that are triggered based on a learned recurrent neural
network. The network is trained from a population of human drivers to infer
impulsivity and inhibitory control from recent driving behavior. Using a
high-fidelity vehicle motion simulator, we demonstrate the ability to deduce
these factors from driver behavior. We then use these inferred factors to make
instantaneous determinations on whether or not to engage a driver safety
interface. This interface aims to decrease a driver's speed during yellow
lights and reduce their inclination to run through them.",http://arxiv.org/abs/2402.05893v1,86,18,16,18,18,16
Knowledge Distillation for Road Detection based on cross-model Semi-Supervised Learning,"The advancement of knowledge distillation has played a crucial role in
enabling the transfer of knowledge from larger teacher models to smaller and
more efficient student models, and is particularly beneficial for online and
resource-constrained applications. The effectiveness of the student model
heavily relies on the quality of the distilled knowledge received from the
teacher. Given the accessibility of unlabelled remote sensing data,
semi-supervised learning has become a prevalent strategy for enhancing model
performance. However, relying solely on semi-supervised learning with smaller
models may be insufficient due to their limited capacity for feature
extraction. This limitation restricts their ability to exploit training data.
To address this issue, we propose an integrated approach that combines
knowledge distillation and semi-supervised learning methods. This hybrid
approach leverages the robust capabilities of large models to effectively
utilise large unlabelled data whilst subsequently providing the small student
model with rich and informative features for enhancement. The proposed
semi-supervised learning-based knowledge distillation (SSLKD) approach
demonstrates a notable improvement in the performance of the student model, in
the application of road segmentation, surpassing the effectiveness of
traditional semi-supervised learning methods.",http://arxiv.org/abs/2402.05305v1,86,18,17,17,17,17
Deep Learning-Based Auto-Segmentation of Planning Target Volume for Total Marrow and Lymph Node Irradiation,"In order to optimize the radiotherapy delivery for cancer treatment,
especially when dealing with complex treatments such as Total Marrow and Lymph
Node Irradiation (TMLI), the accurate contouring of the Planning Target Volume
(PTV) is crucial. Unfortunately, relying on manual contouring for such
treatments is time-consuming and prone to errors. In this paper, we investigate
the application of Deep Learning (DL) to automate the segmentation of the PTV
in TMLI treatment, building upon previous work that introduced a solution to
this problem based on a 2D U-Net model. We extend the previous research (i) by
employing the nnU-Net framework to develop both 2D and 3D U-Net models and (ii)
by evaluating the trained models on the PTV with the exclusion of bones, which
consist mainly of lymp-nodes and represent the most challenging region of the
target volume to segment. Our result show that the introduction of nnU-NET
framework led to statistically significant improvement in the segmentation
performance. In addition, the analysis on the PTV after the exclusion of bones
showed that the models are quite robust also on the most challenging areas of
the target volume. Overall, our study is a significant step forward in the
application of DL in a complex radiotherapy treatment such as TMLI, offering a
viable and scalable solution to increase the number of patients who can benefit
from this treatment.",http://arxiv.org/abs/2402.06494v1,86,18,16,18,16,18
DimVis: Interpreting Visual Clusters in Dimensionality Reduction With Explainable Boosting Machine,"Dimensionality Reduction (DR) techniques such as t-SNE and UMAP are popular
for transforming complex datasets into simpler visual representations. However,
while effective in uncovering general dataset patterns, these methods may
introduce artifacts and suffer from interpretability issues. This paper
presents DimVis, a visualization tool that employs supervised Explainable
Boosting Machine (EBM) models (trained on user-selected data of interest) as an
interpretation assistant for DR projections. Our tool facilitates
high-dimensional data analysis by providing an interpretation of feature
relevance in visual clusters through interactive exploration of UMAP
projections. Specifically, DimVis uses a contrastive EBM model that is trained
in real time to differentiate between the data inside and outside a cluster of
interest. Taking advantage of the inherent explainable nature of the EBM, we
then use this model to interpret the cluster itself via single and pairwise
feature comparisons in a ranking based on the EBM model's feature importance.
The applicability and effectiveness of DimVis are demonstrated through two use
cases involving real-world datasets, and we also discuss the limitations and
potential directions for future research.",http://arxiv.org/abs/2402.06885v1,86,16,18,18,18,16
Forecasting Events in Soccer Matches Through Language,"This paper introduces an approach to predicting the next event in a soccer
match, a challenge bearing remarkable similarities to the problem faced by
Large Language Models (LLMs). Unlike other methods that severely limit event
dynamics in soccer, often abstracting from many variables or relying on a mix
of sequential models, our research proposes a novel technique inspired by the
methodologies used in LLMs. These models predict a complete chain of variables
that compose an event, significantly simplifying the construction of Large
Event Models (LEMs) for soccer. Utilizing deep learning on the publicly
available WyScout dataset, the proposed approach notably surpasses the
performance of previous LEM proposals in critical areas, such as the prediction
accuracy of the next event type. This paper highlights the utility of LEMs in
various applications, including betting and match analytics. Moreover, we show
that LEMs provide a simulation backbone on which many analytics pipelines can
be built, an approach opposite to the current specialized single-purpose
models. LEMs represent a pivotal advancement in soccer analytics, establishing
a foundational framework for multifaceted analytics pipelines through a
singular machine-learning model.",http://arxiv.org/abs/2402.06820v1,86,17,19,19,17,14
A systematic investigation of learnability from single child linguistic input,"Language models (LMs) have demonstrated remarkable proficiency in generating
linguistically coherent text, sparking discussions about their relevance to
understanding human language learnability. However, a significant gap exists
between the training data for these models and the linguistic input a child
receives. LMs are typically trained on data that is orders of magnitude larger
and fundamentally different from child-directed speech (Warstadt and Bowman,
2022; Warstadt et al., 2023; Frank, 2023a). Addressing this discrepancy, our
research focuses on training LMs on subsets of a single child's linguistic
input. Previously, Wang, Vong, Kim, and Lake (2023) found that LMs trained in
this setting can form syntactic and semantic word clusters and develop
sensitivity to certain linguistic phenomena, but they only considered LSTMs and
simpler neural networks trained from just one single-child dataset. Here, to
examine the robustness of learnability from single-child input, we
systematically train six different model architectures on five datasets (3
single-child and 2 baselines). We find that the models trained on single-child
datasets showed consistent results that matched with previous work,
underscoring the robustness of forming meaningful syntactic and semantic
representations from a subset of a child's linguistic input.",http://arxiv.org/abs/2402.07899v1,86,18,16,18,18,16
Multiple Random Masking Autoencoder Ensembles for Robust Multimodal Semi-supervised Learning,"There is an increasing number of real-world problems in computer vision and
machine learning requiring to take into consideration multiple interpretation
layers (modalities or views) of the world and learn how they relate to each
other. For example, in the case of Earth Observations from satellite data, it
is important to be able to predict one observation layer (e.g. vegetation
index) from other layers (e.g. water vapor, snow cover, temperature etc), in
order to best understand how the Earth System functions and also be able to
reliably predict information for one layer when the data is missing (e.g. due
to measurement failure or error).",http://arxiv.org/abs/2402.08035v1,86,18,18,16,18,16
LiRank: Industrial Large Scale Ranking Models at LinkedIn,"We present LiRank, a large-scale ranking framework at LinkedIn that brings to
production state-of-the-art modeling architectures and optimization methods. We
unveil several modeling improvements, including Residual DCN, which adds
attention and residual connections to the famous DCNv2 architecture. We share
insights into combining and tuning SOTA architectures to create a unified
model, including Dense Gating, Transformers and Residual DCN. We also propose
novel techniques for calibration and describe how we productionalized deep
learning based explore/exploit methods. To enable effective, production-grade
serving of large ranking models, we detail how to train and compress models
using quantization and vocabulary compression. We provide details about the
deployment setup for large-scale use cases of Feed ranking, Jobs
Recommendations, and Ads click-through rate (CTR) prediction. We summarize our
learnings from various A/B tests by elucidating the most effective technical
approaches. These ideas have contributed to relative metrics improvements
across the board at LinkedIn: +0.5% member sessions in the Feed, +1.76%
qualified job applications for Jobs search and recommendations, and +4.3% for
Ads CTR. We hope this work can provide practical insights and solutions for
practitioners interested in leveraging large-scale deep ranking systems.",http://arxiv.org/abs/2402.06859v1,86,18,19,17,16,16
Tree Ensembles for Contextual Bandits,"We propose a novel framework for contextual multi-armed bandits based on tree
ensembles. Our framework integrates two widely used bandit methods, Upper
Confidence Bound and Thompson Sampling, for both standard and combinatorial
settings. We demonstrate the effectiveness of our framework via several
experimental studies, employing XGBoost, a popular tree ensemble method.
Compared to state-of-the-art methods based on neural networks, our methods
exhibit superior performance in terms of both regret minimization and
computational runtime, when applied to benchmark datasets and the real-world
application of navigation over road networks.",http://arxiv.org/abs/2402.06963v1,86,18,16,16,18,18
Neural Rendering based Urban Scene Reconstruction for Autonomous Driving,"Dense 3D reconstruction has many applications in automated driving including
automated annotation validation, multimodal data augmentation, providing ground
truth annotations for systems lacking LiDAR, as well as enhancing auto-labeling
accuracy. LiDAR provides highly accurate but sparse depth, whereas camera
images enable estimation of dense depth but noisy particularly at long ranges.
In this paper, we harness the strengths of both sensors and propose a
multimodal 3D scene reconstruction using a framework combining neural implicit
surfaces and radiance fields. In particular, our method estimates dense and
accurate 3D structures and creates an implicit map representation based on
signed distance fields, which can be further rendered into RGB images, and
depth maps. A mesh can be extracted from the learned signed distance field and
culled based on occlusion. Dynamic objects are efficiently filtered on the fly
during sampling using 3D object detection models. We demonstrate qualitative
and quantitative results on challenging automotive scenes.",http://arxiv.org/abs/2402.06826v1,86,18,16,18,20,14
Contextual Stochastic Vehicle Routing with Time Windows,"We study the vehicle routing problem with time windows (VRPTW) and stochastic
travel times, in which the decision-maker observes related contextual
information, represented as feature variables, before making routing decisions.
Despite the extensive literature on stochastic VRPs, the integration of feature
variables has received limited attention in this context. We introduce the
contextual stochastic VRPTW, which minimizes the total transportation cost and
expected late arrival penalties conditioned on the observed features. Since the
joint distribution of travel times and features is unknown, we present novel
data-driven prescriptive models that use historical data to provide an
approximate solution to the problem. We distinguish the prescriptive models
between point-based approximation, sample average approximation, and
penalty-based approximation, each taking a different perspective on dealing
with stochastic travel times and features. We develop specialized
branch-price-and-cut algorithms to solve these data-driven prescriptive models.
In our computational experiments, we compare the out-of-sample cost performance
of different methods on instances with up to one hundred customers. Our results
show that, surprisingly, a feature-dependent sample average approximation
outperforms existing and novel methods in most settings.",http://arxiv.org/abs/2402.06968v1,86,19,17,17,16,17
From Data to Decisions: The Transformational Power of Machine Learning in Business Recommendations,"This research aims to explore the impact of Machine Learning (ML) on the
evolution and efficacy of Recommendation Systems (RS), particularly in the
context of their growing significance in commercial business environments.
Methodologically, the study delves into the role of ML in crafting and refining
these systems, focusing on aspects such as data sourcing, feature engineering,
and the importance of evaluation metrics, thereby highlighting the iterative
nature of enhancing recommendation algorithms. The deployment of Recommendation
Engines (RE), driven by advanced algorithms and data analytics, is explored
across various domains, showcasing their significant impact on user experience
and decision-making processes. These engines not only streamline information
discovery and enhance collaboration but also accelerate knowledge acquisition,
proving vital in navigating the digital landscape for businesses. They
contribute significantly to sales, revenue, and the competitive edge of
enterprises by offering improved recommendations that align with individual
customer needs. The research identifies the increasing expectation of users for
a seamless, intuitive online experience, where content is personalized and
dynamically adapted to changing preferences. Future research directions include
exploring advancements in deep learning models, ethical considerations in the
deployment of RS, and addressing scalability challenges. This study emphasizes
the indispensability of comprehending and leveraging ML in RS for researchers
and practitioners, to tap into the full potential of personalized
recommendation in commercial business prospects.",http://arxiv.org/abs/2402.08109v1,86,18,17,18,17,16
NeRCC: Nested-Regression Coded Computing for Resilient Distributed Prediction Serving Systems,"Resilience against stragglers is a critical element of prediction serving
systems, tasked with executing inferences on input data for a pre-trained
machine-learning model. In this paper, we propose NeRCC, as a general
straggler-resistant framework for approximate coded computing. NeRCC includes
three layers: (1) encoding regression and sampling, which generates coded data
points, as a combination of original data points, (2) computing, in which a
cluster of workers run inference on the coded data points, (3) decoding
regression and sampling, which approximately recovers the predictions of the
original data points from the available predictions on the coded data points.
We argue that the overall objective of the framework reveals an underlying
interconnection between two regression models in the encoding and decoding
layers. We propose a solution to the nested regressions problem by summarizing
their dependence on two regularization terms that are jointly optimized. Our
extensive experiments on different datasets and various machine learning
models, including LeNet5, RepVGG, and Vision Transformer (ViT), demonstrate
that NeRCC accurately approximates the original predictions in a wide range of
stragglers, outperforming the state-of-the-art by up to 23%.",http://arxiv.org/abs/2402.04377v2,85,18,17,18,16,16
Suppressing Pink Elephants with Direct Principle Feedback,"Existing methods for controlling language models, such as RLHF and
Constitutional AI, involve determining which LLM behaviors are desirable and
training them into a language model. However, in many cases, it is desirable
for LLMs to be controllable at inference time, so that they can be used in
multiple contexts with diverse needs. We illustrate this with the Pink Elephant
Problem: instructing an LLM to avoid discussing a certain entity (a ``Pink
Elephant''), and instead discuss a preferred entity (``Grey Elephant''). We
apply a novel simplification of Constitutional AI, Direct Principle Feedback,
which skips the ranking of responses and uses DPO directly on critiques and
revisions. Our results show that after DPF fine-tuning on our synthetic Pink
Elephants dataset, our 13B fine-tuned LLaMA 2 model significantly outperforms
Llama-2-13B-Chat and a prompted baseline, and performs as well as GPT-4 in on
our curated test set assessing the Pink Elephant Problem.",http://arxiv.org/abs/2402.07896v2,85,18,17,18,16,16
Intelligent Mode-switching Framework for Teleoperation,"Teleoperation can be very difficult due to limited perception, high
communication latency, and limited degrees of freedom (DoFs) at the operator
side. Autonomous teleoperation is proposed to overcome this difficulty by
predicting user intentions and performing some parts of the task autonomously
to decrease the demand on the operator and increase the task completion rate.
However, decision-making for mode-switching is generally assumed to be done by
the operator, which brings an extra DoF to be controlled by the operator and
introduces extra mental demand. On the other hand, the communication
perspective is not investigated in the current literature, although
communication imperfections and resource limitations are the main bottlenecks
for teleoperation. In this study, we propose an intelligent mode-switching
framework by jointly considering mode-switching and communication systems. User
intention recognition is done at the operator side. Based on user intention
recognition, a deep reinforcement learning (DRL) agent is trained and deployed
at the operator side to seamlessly switch between autonomous and teleoperation
modes. A real-world data set is collected from our teleoperation testbed to
train both user intention recognition and DRL algorithms. Our results show that
the proposed framework can achieve up to 50% communication load reduction with
improved task completion probability.",http://arxiv.org/abs/2402.06047v1,85,16,18,18,18,15
Binding Dynamics in Rotating Features,"In human cognition, the binding problem describes the open question of how
the brain flexibly integrates diverse information into cohesive object
representations. Analogously, in machine learning, there is a pursuit for
models capable of strong generalization and reasoning by learning
object-centric representations in an unsupervised manner. Drawing from
neuroscientific theories, Rotating Features learn such representations by
introducing vector-valued features that encapsulate object characteristics in
their magnitudes and object affiliation in their orientations. The
""$\chi$-binding"" mechanism, embedded in every layer of the architecture, has
been shown to be crucial, but remains poorly understood. In this paper, we
propose an alternative ""cosine binding"" mechanism, which explicitly computes
the alignment between features and adjusts weights accordingly, and we show
that it achieves equivalent performance. This allows us to draw direct
connections to self-attention and biological neural processes, and to shed
light on the fundamental dynamics for object-centric representations to emerge
in Rotating Features.",http://arxiv.org/abs/2402.05627v1,85,18,16,17,18,16
Outlier-Aware Training for Low-Bit Quantization of Structural Re-Parameterized Networks,"Lightweight design of Convolutional Neural Networks (CNNs) requires co-design
efforts in the model architectures and compression techniques. As a novel
design paradigm that separates training and inference, a structural
re-parameterized (SR) network such as the representative RepVGG revitalizes the
simple VGG-like network with a high accuracy comparable to advanced and often
more complicated networks. However, the merging process in SR networks
introduces outliers into weights, making their distribution distinct from
conventional networks and thus heightening difficulties in quantization. To
address this, we propose an operator-level improvement for training called
Outlier Aware Batch Normalization (OABN). Additionally, to meet the demands of
limited bitwidths while upkeeping the inference accuracy, we develop a
clustering-based non-uniform quantization framework for Quantization-Aware
Training (QAT) named ClusterQAT. Integrating OABN with ClusterQAT, the
quantized performance of RepVGG is largely enhanced, particularly when the
bitwidth falls below 8.",http://arxiv.org/abs/2402.07200v1,85,16,18,18,16,17
Bayesian Federated Learning Via Expectation Maximization and Turbo Deep Approximate Message Passing,"Federated learning (FL) is a machine learning paradigm where the clients
possess decentralized training data and the central server handles aggregation
and scheduling. Typically, FL algorithms involve clients training their local
models using stochastic gradient descent (SGD), which carries drawbacks such as
slow convergence and being prone to getting stuck in suboptimal solutions. In
this work, we propose a message passing based Bayesian federated learning (BFL)
framework to avoid these drawbacks.Specifically, we formulate the problem of
deep neural network (DNN) learning and compression and as a sparse Bayesian
inference problem, in which group sparse prior is employed to achieve
structured model compression. Then, we propose an efficient BFL algorithm
called EMTDAMP, where expectation maximization (EM) and turbo deep approximate
message passing (TDAMP) are combined to achieve distributed learning and
compression. The central server aggregates local posterior distributions to
update global posterior distributions and update hyperparameters based on EM to
accelerate convergence. The clients perform TDAMP to achieve efficient
approximate message passing over DNN with joint prior distribution. We detail
the application of EMTDAMP to Boston housing price prediction and handwriting
recognition, and present extensive numerical results to demonstrate the
advantages of EMTDAMP.",http://arxiv.org/abs/2402.07366v1,85,18,17,17,18,15
AraSpider: Democratizing Arabic-to-SQL,"This study presents AraSpider, the first Arabic version of the Spider
dataset, aimed at improving natural language processing (NLP) in the
Arabic-speaking community. Four multilingual translation models were tested for
their effectiveness in translating English to Arabic. Additionally, two models
were assessed for their ability to generate SQL queries from Arabic text. The
results showed that using back translation significantly improved the
performance of both ChatGPT 3.5 and SQLCoder models, which are considered top
performers on the Spider dataset. Notably, ChatGPT 3.5 demonstrated
high-quality translation, while SQLCoder excelled in text-to-SQL tasks. The
study underscores the importance of incorporating contextual schema and
employing back translation strategies to enhance model performance in Arabic
NLP tasks. Moreover, the provision of detailed methodologies for
reproducibility and translation of the dataset into other languages highlights
the research's commitment to promoting transparency and collaborative knowledge
sharing in the field. Overall, these contributions advance NLP research,
empower Arabic-speaking researchers, and enrich the global discourse on
language comprehension and database interrogation.",http://arxiv.org/abs/2402.07448v1,85,17,16,18,16,18
Intriguing Differences Between Zero-Shot and Systematic Evaluations of Vision-Language Transformer Models,"Transformer-based models have dominated natural language processing and other
areas in the last few years due to their superior (zero-shot) performance on
benchmark datasets. However, these models are poorly understood due to their
complexity and size. While probing-based methods are widely used to understand
specific properties, the structures of the representation space are not
systematically characterized; consequently, it is unclear how such models
generalize and overgeneralize to new inputs beyond datasets. In this paper,
based on a new gradient descent optimization method, we are able to explore the
embedding space of a commonly used vision-language model. Using the Imagenette
dataset, we show that while the model achieves over 99\% zero-shot
classification performance, it fails systematic evaluations completely. Using a
linear approximation, we provide a framework to explain the striking
differences. We have also obtained similar results using a different model to
support that our results are applicable to other transformer models with
continuous inputs. We also propose a robust way to detect the modified images.",http://arxiv.org/abs/2402.08473v1,85,18,18,17,16,16
Regional Adaptive Metropolis Light Transport,"The design of the proposal distributions, and most notably the kernel
parameters, are crucial for the performance of Markov chain Monte Carlo (MCMC)
rendering. A poor selection of parameters can increase the correlation of the
Markov chain and result in bad rendering performance. We approach this problem
by a novel path perturbation strategy for online-learning of state-dependent
kernel parameters. We base our approach on the theoretical framework of
regional adaptive MCMC which enables the adaptation of parameters depending on
the region of the state space which contains the current sample, and on
information collected from previous samples. For this, we define a partitioning
of the path space on a low-dimensional canonical space to capture the
characteristics of paths, with a focus on path segments closer to the sensor.
Fast convergence is achieved by adaptive refinement of the partitions.
Exemplarily, we present two novel regional adaptive path perturbation
techniques akin to lens and multi-chain perturbations. Our approach can easily
be used on top of existing path space MLT methods to improve rendering
efficiency, while being agnostic to the initial choice of kernel parameters.",http://arxiv.org/abs/2402.08273v1,85,16,18,18,16,17
NetInfoF Framework: Measuring and Exploiting Network Usable Information,"Given a node-attributed graph, and a graph task (link prediction or node
classification), can we tell if a graph neural network (GNN) will perform well?
More specifically, do the graph structure and the node features carry enough
usable information for the task? Our goals are (1) to develop a fast tool to
measure how much information is in the graph structure and in the node
features, and (2) to exploit the information to solve the task, if there is
enough. We propose NetInfoF, a framework including NetInfoF_Probe and
NetInfoF_Act, for the measurement and the exploitation of network usable
information (NUI), respectively. Given a graph data, NetInfoF_Probe measures
NUI without any model training, and NetInfoF_Act solves link prediction and
node classification, while two modules share the same backbone. In summary,
NetInfoF has following notable advantages: (a) General, handling both link
prediction and node classification; (b) Principled, with theoretical guarantee
and closed-form solution; (c) Effective, thanks to the proposed adjustment to
node similarity; (d) Scalable, scaling linearly with the input size. In our
carefully designed synthetic datasets, NetInfoF correctly identifies the ground
truth of NUI and is the only method being robust to all graph scenarios.
Applied on real-world datasets, NetInfoF wins in 11 out of 12 times on link
prediction compared to general GNN baselines.",http://arxiv.org/abs/2402.07999v1,85,16,18,18,18,15
Real-World Fluid Directed Rigid Body Control via Deep Reinforcement Learning,"Recent advances in real-world applications of reinforcement learning (RL)
have relied on the ability to accurately simulate systems at scale. However,
domains such as fluid dynamical systems exhibit complex dynamic phenomena that
are hard to simulate at high integration rates, limiting the direct application
of modern deep RL algorithms to often expensive or safety critical hardware. In
this work, we introduce ""Box o Flows"", a novel benchtop experimental control
system for systematically evaluating RL algorithms in dynamic real-world
scenarios. We describe the key components of the Box o Flows, and through a
series of experiments demonstrate how state-of-the-art model-free RL algorithms
can synthesize a variety of complex behaviors via simple reward specifications.
Furthermore, we explore the role of offline RL in data-efficient hypothesis
testing by reusing past experiences. We believe that the insights gained from
this preliminary study and the availability of systems like the Box o Flows
support the way forward for developing systematic RL algorithms that can be
generally applied to complex, dynamical systems. Supplementary material and
videos of experiments are available at
https://sites.google.com/view/box-o-flows/home.",http://arxiv.org/abs/2402.06102v1,85,18,16,18,16,17
"Veni, Vidi, Vici: Solving the Myriad of Challenges before Knowledge Graph Learning","Knowledge Graphs (KGs) have become increasingly common for representing
large-scale linked data. However, their immense size has required graph
learning systems to assist humans in analysis, interpretation, and pattern
detection. While there have been promising results for researcher- and
clinician- empowerment through a variety of KG learning systems, we identify
four key deficiencies in state-of-the-art graph learning that simultaneously
limit KG learning performance and diminish the ability of humans to interface
optimally with these learning systems. These deficiencies are: 1) lack of
expert knowledge integration, 2) instability to node degree extremity in the
KG, 3) lack of consideration for uncertainty and relevance while learning, and
4) lack of explainability. Furthermore, we characterise state-of-the-art
attempts to solve each of these problems and note that each attempt has largely
been isolated from attempts to solve the other problems. Through a
formalisation of these problems and a review of the literature that addresses
them, we adopt the position that not only are deficiencies in these four key
areas holding back human-KG empowerment, but that the divide-and-conquer
approach to solving these problems as individual units rather than a whole is a
significant barrier to the interface between humans and KG learning systems. We
propose that it is only through integrated, holistic solutions to the
limitations of KG learning systems that human and KG learning co-empowerment
will be efficiently affected. We finally present our ""Veni, Vidi, Vici""
framework that sets a roadmap for effectively and efficiently shifting to a
holistic co-empowerment model in both the KG learning and the broader machine
learning domain.",http://arxiv.org/abs/2402.06098v1,85,18,16,18,16,17
The Effect of Data Poisoning on Counterfactual Explanations,"Counterfactual explanations provide a popular method for analyzing the
predictions of black-box systems, and they can offer the opportunity for
computational recourse by suggesting actionable changes on how to change the
input to obtain a different (i.e. more favorable) system output. However,
recent work highlighted their vulnerability to different types of
manipulations. This work studies the vulnerability of counterfactual
explanations to data poisoning. We formalize data poisoning in the context of
counterfactual explanations for increasing the cost of recourse on three
different levels: locally for a single instance, or a sub-group of instances,
or globally for all instances. We demonstrate that state-of-the-art
counterfactual generation methods \& toolboxes are vulnerable to such data
poisoning.",http://arxiv.org/abs/2402.08290v1,85,16,18,18,18,15
Synthesizing CTA Image Data for Type-B Aortic Dissection using Stable Diffusion Models,"Stable Diffusion (SD) has gained a lot of attention in recent years in the
field of Generative AI thus helping in synthesizing medical imaging data with
distinct features. The aim is to contribute to the ongoing effort focused on
overcoming the limitations of data scarcity and improving the capabilities of
ML algorithms for cardiovascular image processing. Therefore, in this study,
the possibility of generating synthetic cardiac CTA images was explored by
fine-tuning stable diffusion models based on user defined text prompts, using
only limited number of CTA images as input. A comprehensive evaluation of the
synthetic data was conducted by incorporating both quantitative analysis and
qualitative assessment, where a clinician assessed the quality of the generated
data. It has been shown that Cardiac CTA images can be successfully generated
using using Text to Image (T2I) stable diffusion model. The results demonstrate
that the tuned T2I CTA diffusion model was able to generate images with
features that are typically unique to acute type B aortic dissection (TBAD)
medical conditions.",http://arxiv.org/abs/2402.06969v1,85,16,18,17,18,16
Solving Hierarchical Information-Sharing Dec-POMDPs: An Extensive-Form Game Approach,"A recent theory shows that a multi-player decentralized partially observable
Markov decision process can be transformed into an equivalent single-player
game, enabling the application of \citeauthor{bellman}'s principle of
optimality to solve the single-player game by breaking it down into
single-stage subgames. However, this approach entangles the decision variables
of all players at each single-stage subgame, resulting in backups with a
double-exponential complexity. This paper demonstrates how to disentangle these
decision variables while maintaining optimality under hierarchical information
sharing, a prominent management style in our society. To achieve this, we apply
the principle of optimality to solve any single-stage subgame by breaking it
down further into smaller subgames, enabling us to make single-player decisions
at a time. Our approach reveals that extensive-form games always exist with
solutions to a single-stage subgame, significantly reducing time complexity.
Our experimental results show that the algorithms leveraging these findings can
scale up to much larger multi-player games without compromising optimality.",http://arxiv.org/abs/2402.02954v2,85,17,16,16,17,19
Decentralized Proactive Model Offloading and Resource Allocation for Split and Federated Learning,"In the resource-constrained IoT-edge environment, Split Federated (SplitFed)
learning is implemented to enhance training efficiency. This method involves
each IoT device dividing its full DNN model at a designated layer into a
device-side model and a server-side model, then offloading the latter to the
edge server. However, existing research overlooks four critical issues as
follows: (1) the heterogeneity of IoT devices' resource capacities and the
sizes of their local data samples impact training efficiency; (2) the influence
of the edge server's computation and network resource allocation on training
efficiency; (3) the data leakage risk associated with the offloaded server-side
sub-model; (4) the privacy drawbacks of current centralized algorithms.
Consequently, proactively identifying the optimal cut layer and server resource
requirements for each IoT device to minimize training latency while adhering to
data leakage risk rate constraint remains a challenging issue. To address these
problems, this paper first formulates the latency and data leakage risk of
training DNN models using Split Federated learning. Next, we frame the Split
Federated learning problem as a mixed-integer nonlinear programming challenge.
To tackle this, we propose a decentralized Proactive Model Offloading and
Resource Allocation (DP-MORA) scheme, empowering each IoT device to determine
its cut layer and resource requirements based on its local multidimensional
training configuration, without knowledge of other devices' configurations.
Extensive experiments on two real-world datasets demonstrate that the DP-MORA
scheme effectively reduces DNN model training latency, enhances training
efficiency, and complies with data leakage risk constraints compared to several
baseline algorithms across various experimental settings.",http://arxiv.org/abs/2402.06123v1,85,18,17,18,16,16
Causal Learning for Trustworthy Recommender Systems: A Survey,"Recommender Systems (RS) have significantly advanced online content discovery
and personalized decision-making. However, emerging vulnerabilities in RS have
catalyzed a paradigm shift towards Trustworthy RS (TRS). Despite numerous
progress on TRS, most of them focus on data correlations while overlooking the
fundamental causal nature in recommendation. This drawback hinders TRS from
identifying the cause in addressing trustworthiness issues, leading to limited
fairness, robustness, and explainability. To bridge this gap, causal learning
emerges as a class of promising methods to augment TRS. These methods, grounded
in reliable causality, excel in mitigating various biases and noises while
offering insightful explanations for TRS. However, there lacks a timely survey
in this vibrant area. This paper creates an overview of TRS from the
perspective of causal learning. We begin by presenting the advantages and
common procedures of Causality-oriented TRS (CTRS). Then, we identify potential
trustworthiness challenges at each stage and link them to viable causal
solutions, followed by a classification of CTRS methods. Finally, we discuss
several future directions for advancing this field.",http://arxiv.org/abs/2402.08241v1,85,18,17,17,16,17
TL;DR Progress: Multi-faceted Literature Exploration in Text Summarization,"This paper presents TL;DR Progress, a new tool for exploring the literature
on neural text summarization. It organizes 514~papers based on a comprehensive
annotation scheme for text summarization approaches and enables fine-grained,
faceted search. Each paper was manually annotated to capture aspects such as
evaluation metrics, quality dimensions, learning paradigms, challenges
addressed, datasets, and document domains. In addition, a succinct indicative
summary is provided for each paper, consisting of automatically extracted
contextual factors, issues, and proposed solutions. The tool is available
online at https://www.tldr-progress.de, a demo video at
https://youtu.be/uCVRGFvXUj8",http://arxiv.org/abs/2402.06913v1,85,18,16,16,20,15
Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey,"Large-scale pre-trained vision models (PVMs) have shown great potential for
adaptability across various downstream vision tasks. However, with
state-of-the-art PVMs growing to billions or even trillions of parameters, the
standard full fine-tuning paradigm is becoming unsustainable due to high
computational and storage demands. In response, researchers are exploring
parameter-efficient fine-tuning (PEFT), which seeks to exceed the performance
of full fine-tuning with minimal parameter modifications. This survey provides
a comprehensive overview and future directions for visual PEFT, offering a
systematic review of the latest advancements. First, we provide a formal
definition of PEFT and discuss model pre-training methods. We then categorize
existing methods into three categories: addition-based, partial-based, and
unified-based. Finally, we introduce the commonly used datasets and
applications and suggest potential future research challenges. A comprehensive
collection of resources is available at
https://github.com/synbol/Awesome-Parameter-Efficient-Transfer-Learning.",http://arxiv.org/abs/2402.02242v2,85,18,16,18,16,17
On the Exploitation of DCT-Traces in the Generative-AI Domain,"Deepfakes represent one of the toughest challenges in the world of
Cybersecurity and Digital Forensics, especially considering the high-quality
results obtained with recent generative AI-based solutions. Almost all
generative models leave unique traces in synthetic data that, if analyzed and
identified in detail, can be exploited to improve the generalization
limitations of existing deepfake detectors. In this paper we analyzed deepfake
images in the frequency domain generated by both GAN and Diffusion Model
engines, examining in detail the underlying statistical distribution of
Discrete Cosine Transform (DCT) coefficients. Recognizing that not all
coefficients contribute equally to image detection, we hypothesize the
existence of a unique ""discriminative fingerprint"", embedded in specific
combinations of coefficients. To identify them, Machine Learning classifiers
were trained on various combinations of coefficients. In addition, the
Explainable AI (XAI) LIME algorithm was used to search for intrinsic
discriminative combinations of coefficients. Finally, we performed a robustness
test to analyze the persistence of traces by applying JPEG compression. The
experimental results reveal the existence of traces left by the generative
models that are more discriminative and persistent at JPEG attacks.",http://arxiv.org/abs/2402.02209v2,85,16,18,18,18,15
NLP for Knowledge Discovery and Information Extraction from Energetics Corpora,"We present a demonstration of the utility of NLP for aiding research into
energetic materials and associated systems. The NLP method enables machine
understanding of textual data, offering an automated route to knowledge
discovery and information extraction from energetics text. We apply three
established unsupervised NLP models: Latent Dirichlet Allocation, Word2Vec, and
the Transformer to a large curated dataset of energetics-related scientific
articles. We demonstrate that each NLP algorithm is capable of identifying
energetic topics and concepts, generating a language model which aligns with
Subject Matter Expert knowledge. Furthermore, we present a document
classification pipeline for energetics text. Our classification pipeline
achieves 59-76\% accuracy depending on the NLP model used, with the highest
performing Transformer model rivaling inter-annotator agreement metrics. The
NLP approaches studied in this work can identify concepts germane to energetics
and therefore hold promise as a tool for accelerating energetics research
efforts and energetics material development.",http://arxiv.org/abs/2402.06964v1,85,18,16,17,17,17
Knowledge Editing on Black-box Large Language Models,"Knowledge editing (KE) aims to efficiently and precisely modify the behavior
of large language models (LLMs) to update specific knowledge without negatively
influencing other knowledge. Current research primarily focuses on white-box
LLMs editing, overlooking an important scenario: black-box LLMs editing, where
LLMs are accessed through interfaces and only textual output is available. To
address the limitations of existing evaluations that are not inapplicable to
black-box LLM editing and lack comprehensiveness, we propose a
multi-perspective evaluation framework, incorporating the assessment of style
retention for the first time. To tackle privacy leaks of editing data and style
over-editing in current methods, we introduce a novel postEdit framework,
resolving privacy concerns through downstream post-processing and maintaining
textual style consistency via fine-grained editing to original responses.
Experiments and analysis on two benchmarks demonstrate that postEdit
outperforms all baselines and achieves strong generalization, especially with
huge improvements on style retention (average $+20.82\%\uparrow$).",http://arxiv.org/abs/2402.08631v1,85,17,15,18,18,17
Nesting Particle Filters for Experimental Design in Dynamical Systems,"In this paper, we propose a novel approach to Bayesian Experimental Design
(BED) for non-exchangeable data that formulates it as risk-sensitive policy
optimization. We develop the Inside-Out SMC^2 algorithm that uses a nested
sequential Monte Carlo (SMC) estimator of the expected information gain and
embeds it into a particle Markov chain Monte Carlo (pMCMC) framework to perform
gradient-based policy optimization. This is in contrast to recent approaches
that rely on biased estimators of the expected information gain (EIG) to
amortize the cost of experiments by learning a design policy in advance.
Numerical validation on a set of dynamical systems showcases the efficacy of
our method in comparison to other state-of-the-art strategies.",http://arxiv.org/abs/2402.07868v1,85,18,17,17,16,17
BBSEA: An Exploration of Brain-Body Synchronization for Embodied Agents,"Embodied agents capable of complex physical skills can improve productivity,
elevate life quality, and reshape human-machine collaboration. We aim at
autonomous training of embodied agents for various tasks involving mainly large
foundation models. It is believed that these models could act as a brain for
embodied agents; however, existing methods heavily rely on humans for task
proposal and scene customization, limiting the learning autonomy, training
efficiency, and generalization of the learned policies. In contrast, we
introduce a brain-body synchronization ({\it BBSEA}) scheme to promote embodied
learning in unknown environments without human involvement. The proposed
combines the wisdom of foundation models (``brain'') with the physical
capabilities of embodied agents (``body''). Specifically, it leverages the
``brain'' to propose learnable physical tasks and success metrics, enabling the
``body'' to automatically acquire various skills by continuously interacting
with the scene. We carry out an exploration of the proposed autonomous learning
scheme in a table-top setting, and we demonstrate that the proposed
synchronization can generate diverse tasks and develop multi-task policies with
promising adaptability to new tasks and configurations. We will release our
data, code, and trained models to facilitate future studies in building
autonomously learning agents with large foundation models in more complex
scenarios. More visualizations are available at
\href{https://bbsea-embodied-ai.github.io}{https://bbsea-embodied-ai.github.io}",http://arxiv.org/abs/2402.08212v1,85,18,17,18,16,16
GeoFormer: A Vision and Sequence Transformer-based Approach for Greenhouse Gas Monitoring,"Air pollution represents a pivotal environmental challenge globally, playing
a major role in climate change via greenhouse gas emissions and negatively
affecting the health of billions. However predicting the spatial and temporal
patterns of pollutants remains challenging. The scarcity of ground-based
monitoring facilities and the dependency of air pollution modeling on
comprehensive datasets, often inaccessible for numerous areas, complicate this
issue. In this work, we introduce GeoFormer, a compact model that combines a
vision transformer module with a highly efficient time-series transformer
module to predict surface-level nitrogen dioxide (NO2) concentrations from
Sentinel-5P satellite imagery. We train the proposed model to predict
surface-level NO2 measurements using a dataset we constructed with Sentinel-5P
images of ground-level monitoring stations, and their corresponding NO2
concentration readings. The proposed model attains high accuracy (MAE 5.65),
demonstrating the efficacy of combining vision and time-series transformer
architectures to harness satellite-derived data for enhanced GHG emission
insights, proving instrumental in advancing climate change monitoring and
emission regulation efforts globally.",http://arxiv.org/abs/2402.07164v1,85,18,19,17,16,15
Towards Fast Stochastic Sampling in Diffusion Generative Models,"Diffusion models suffer from slow sample generation at inference time.
Despite recent efforts, improving the sampling efficiency of stochastic
samplers for diffusion models remains a promising direction. We propose
Splitting Integrators for fast stochastic sampling in pre-trained diffusion
models in augmented spaces. Commonly used in molecular dynamics,
splitting-based integrators attempt to improve sampling efficiency by cleverly
alternating between numerical updates involving the data, auxiliary, or noise
variables. However, we show that a naive application of splitting integrators
is sub-optimal for fast sampling. Consequently, we propose several principled
modifications to naive splitting samplers for improving sampling efficiency and
denote the resulting samplers as Reduced Splitting Integrators. In the context
of Phase Space Langevin Diffusion (PSLD) [Pandey \& Mandt, 2023] on CIFAR-10,
our stochastic sampler achieves an FID score of 2.36 in only 100 network
function evaluations (NFE) as compared to 2.63 for the best baselines.",http://arxiv.org/abs/2402.07211v2,85,18,17,16,18,16
RepQuant: Towards Accurate Post-Training Quantization of Large Transformer Models via Scale Reparameterization,"Large transformer models have demonstrated remarkable success. Post-training
quantization (PTQ), which requires only a small dataset for calibration and
avoids end-to-end retraining, is a promising solution for compressing these
large models. Regrettably, existing PTQ methods typically exhibit non-trivial
performance loss. We find that the performance bottleneck stems from
over-consideration of hardware compatibility in the quantization process,
compelling them to reluctantly employ simple quantizers, albeit at the expense
of accuracy. With the above insights, we propose RepQuant, a novel PTQ
framework with quantization-inference decoupling paradigm to address the above
issues. RepQuant employs complex quantizers in the quantization process and
simplified quantizers in the inference process, and performs mathematically
equivalent transformations between the two through quantization scale
reparameterization, thus ensuring both accurate quantization and efficient
inference. More specifically, we focus on two components with extreme
distributions: LayerNorm activations and Softmax activations. Initially, we
apply channel-wise quantization and log$\sqrt{2}$ quantization, respectively,
which are tailored to their distributions. In particular, for the former, we
introduce a learnable per-channel dual clipping scheme, which is designed to
efficiently identify outliers in the unbalanced activations with fine
granularity. Then, we reparameterize the scales to hardware-friendly layer-wise
quantization and log2 quantization for inference. Moreover, quantized weight
reconstruction is seamlessly integrated into the above procedure to further
push the performance limits. Extensive experiments are performed on different
large-scale transformer variants on multiple tasks, including vision, language,
and multi-modal transformers, and RepQuant encouragingly demonstrates
significant performance advantages.",http://arxiv.org/abs/2402.05628v1,85,16,18,18,16,17
ApiQ: Finetuning of 2-Bit Quantized Large Language Model,"Memory-efficient finetuning of large language models (LLMs) has recently
attracted huge attention with the increasing size of LLMs, primarily due to the
constraints posed by GPU memory limitations and the comparable results of these
methods with full finetuning. Despite the advancements, current strategies for
memory-efficient finetuning, such as QLoRA, exhibit inconsistent performance
across diverse bit-width quantizations and multifaceted tasks. This
inconsistency largely stems from the detrimental impact of the quantization
process on preserved knowledge, leading to catastrophic forgetting and
undermining the utilization of pretrained models for finetuning purposes. In
this work, we introduce a novel quantization framework named ApiQ, designed to
restore the lost information from quantization by concurrently initializing
LoRA components and quantizing the weights of LLMs. This approach ensures the
maintenance of the original LLM's activation precision while mitigating the
error propagation from shallower into deeper layers. Through comprehensive
evaluations conducted on a spectrum of language tasks with various models, ApiQ
demonstrably minimizes activation error during quantization. Consequently, it
consistently achieves superior finetuning outcomes across various bit-widths of
quantization.",http://arxiv.org/abs/2402.05147v2,85,18,16,18,16,17
IR-Aware ECO Timing Optimization Using Reinforcement Learning,"Engineering change orders (ECOs) in late stages make minimal design fixes to
recover from timing shifts due to excessive IR drops. This paper integrates
IR-drop-aware timing analysis and ECO timing optimization using reinforcement
learning (RL). The method operates after physical design and power grid
synthesis, and rectifies IR-drop-induced timing degradation through gate
sizing. It incorporates the Lagrangian relaxation (LR) technique into a novel
RL framework, which trains a relational graph convolutional network (R-GCN)
agent to sequentially size gates to fix timing violations. The R-GCN agent
outperforms a classical LR-only algorithm: in an open 45nm technology, it (a)
moves the Pareto front of the delay-area tradeoff curve to the left and (b)
saves runtime over the classical method by running fast inference using trained
models at iso-quality. The RL model is transferable across timing
specifications, and transferable to unseen designs with zero-shot learning or
fine tuning.",http://arxiv.org/abs/2402.07781v1,85,17,18,18,16,16
Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes,"Given the generational gap in available hardware between lay practitioners
and the most endowed institutions, LLMs are becoming increasingly inaccessible
as they grow in size. Whilst many approaches have been proposed to compress
LLMs to make their resource consumption manageable, these methods themselves
tend to be resource intensive, putting them out of the reach of the very user
groups they target. In this work, we explore the problem of structured pruning
of LLMs using only forward passes. We seek to empower practitioners to prune
models so large that their available hardware has just enough memory to run
inference. We develop Bonsai, a gradient-free, perturbative pruning method
capable of delivering small, fast, and accurate pruned models.
  We observe that Bonsai outputs pruned models that (i) outperform those
generated by more expensive gradient-based structured pruning methods, and (ii)
are twice as fast (with comparable accuracy) as those generated by
semi-structured pruning methods requiring comparable resources as Bonsai. We
also leverage Bonsai to produce a new sub-2B model using a single A6000 that
yields state-of-the-art performance on 4/6 tasks on the Huggingface Open LLM
leaderboard.",http://arxiv.org/abs/2402.05406v2,85,18,17,18,16,16
Rethinking Propagation for Unsupervised Graph Domain Adaptation,"Unsupervised Graph Domain Adaptation (UGDA) aims to transfer knowledge from a
labelled source graph to an unlabelled target graph in order to address the
distribution shifts between graph domains. Previous works have primarily
focused on aligning data from the source and target graph in the representation
space learned by graph neural networks (GNNs). However, the inherent
generalization capability of GNNs has been largely overlooked. Motivated by our
empirical analysis, we reevaluate the role of GNNs in graph domain adaptation
and uncover the pivotal role of the propagation process in GNNs for adapting to
different graph domains. We provide a comprehensive theoretical analysis of
UGDA and derive a generalization bound for multi-layer GNNs. By formulating GNN
Lipschitz for k-layer GNNs, we show that the target risk bound can be tighter
by removing propagation layers in source graph and stacking multiple
propagation layers in target graph. Based on the empirical and theoretical
analysis mentioned above, we propose a simple yet effective approach called
A2GNN for graph domain adaptation. Through extensive experiments on real-world
datasets, we demonstrate the effectiveness of our proposed A2GNN framework.",http://arxiv.org/abs/2402.05660v1,85,18,17,17,17,16
Evolving AI for Wellness: Dynamic and Personalized Real-time Loneliness Detection Using Passive Sensing,"Loneliness is a growing health concern as it can lead to depression and other
associated mental health problems for people who experience feelings of
loneliness over prolonged periods of time. Utilizing passive sensing methods
that use smartphone and wearable sensor data to capture daily behavioural
patterns offers a promising approach for the early detection of loneliness.
Given the subjective nature of loneliness and people's varying daily routines,
past detection approaches using machine learning models often face challenges
with effectively detecting loneliness. This paper proposes a methodologically
novel approach, particularly developing a loneliness detection system that
evolves over time, adapts to new data, and provides real-time detection. Our
study utilized the Globem dataset, a comprehensive collection of passive
sensing data acquired over 10 weeks from university students. The base of our
approach is the continuous identification and refinement of similar behavioural
groups among students using an incremental clustering method. As we add new
data, the model improves based on changing behavioural patterns. Parallel to
this, we create and update classification models to detect loneliness among the
evolving behavioural groups of students. When unique behavioural patterns are
observed among student data, specialized classification models have been
created. For predictions of loneliness, a collaborative effort between the
generalized and specialized models is employed, treating each prediction as a
vote. This study's findings reveal that group-based loneliness detection models
exhibit superior performance compared to generic models, underscoring the
necessity for more personalized approaches tailored to specific behavioural
patterns. These results pave the way for future research, emphasizing the
development of finely-tuned, individualized mental health interventions.",http://arxiv.org/abs/2402.05698v1,85,18,16,18,16,17
Real-World Atmospheric Turbulence Correction via Domain Adaptation,"Atmospheric turbulence, a common phenomenon in daily life, is primarily
caused by the uneven heating of the Earth's surface. This phenomenon results in
distorted and blurred acquired images or videos and can significantly impact
downstream vision tasks, particularly those that rely on capturing clear,
stable images or videos from outdoor environments, such as accurately detecting
or recognizing objects. Therefore, people have proposed ways to simulate
atmospheric turbulence and designed effective deep learning-based methods to
remove the atmospheric turbulence effect. However, these synthesized turbulent
images can not cover all the range of real-world turbulence effects. Though the
models have achieved great performance for synthetic scenarios, there always
exists a performance drop when applied to real-world cases. Moreover, reducing
real-world turbulence is a more challenging task as there are no clean ground
truth counterparts provided to the models during training. In this paper, we
propose a real-world atmospheric turbulence mitigation model under a domain
adaptation framework, which links the supervised simulated atmospheric
turbulence correction with the unsupervised real-world atmospheric turbulence
correction. We will show our proposed method enhances performance in real-world
atmospheric turbulence scenarios, improving both image quality and downstream
vision tasks.",http://arxiv.org/abs/2402.07371v1,85,18,17,18,16,16
CounterCLR: Counterfactual Contrastive Learning with Non-random Missing Data in Recommendation,"Recommender systems are designed to learn user preferences from observed
feedback and comprise many fundamental tasks, such as rating prediction and
post-click conversion rate (pCVR) prediction. However, the observed feedback
usually suffer from two issues: selection bias and data sparsity, where biased
and insufficient feedback seriously degrade the performance of recommender
systems in terms of accuracy and ranking. Existing solutions for handling the
issues, such as data imputation and inverse propensity score, are highly
susceptible to additional trained imputation or propensity models. In this
work, we propose a novel counterfactual contrastive learning framework for
recommendation, named CounterCLR, to tackle the problem of non-random missing
data by exploiting the advances in contrast learning. Specifically, the
proposed CounterCLR employs a deep representation network, called CauNet, to
infer non-random missing data in recommendations and perform user preference
modeling by further introducing a self-supervised contrastive learning task.
Our CounterCLR mitigates the selection bias problem without the need for
additional models or estimators, while also enhancing the generalization
ability in cases of sparse data. Experiments on real-world datasets demonstrate
the effectiveness and superiority of our method.",http://arxiv.org/abs/2402.05740v1,85,16,16,18,18,17
Real-World Robot Applications of Foundation Models: A Review,"Recent developments in foundation models, like Large Language Models (LLMs)
and Vision-Language Models (VLMs), trained on extensive data, facilitate
flexible application across different tasks and modalities. Their impact spans
various fields, including healthcare, education, and robotics. This paper
provides an overview of the practical application of foundation models in
real-world robotics, with a primary emphasis on the replacement of specific
components within existing robot systems. The summary encompasses the
perspective of input-output relationships in foundation models, as well as
their role in perception, motion planning, and control within the field of
robotics. This paper concludes with a discussion of future challenges and
implications for practical robot applications.",http://arxiv.org/abs/2402.05741v1,85,16,18,18,18,15
An Interpretable Low-complexity Model for Wireless Channel Estimation,"With the advent of machine learning, there has been renewed interest in the
problem of wireless channel estimation. This paper presents a novel
low-complexity wireless channel estimation scheme based on a tapped delay line
(TDL) model of wireless signal propagation, where a data-driven machine
learning approach is used to estimate the path delays and gains. Advantages of
this approach include low computation time and training data requirements, as
well as interpretability since the estimated model parameters and their
variance provide comprehensive representation of the dynamic wireless multipath
environment. We evaluate this model's performance using Matlab's ray-tracing
tool under static and dynamic conditions for increased realism instead of the
standard evaluation approaches using statistical channel models. Our results
show that our TDL-based model can accurately estimate the path delays and
associated gains for a broad-range of locations and operating conditions.
Root-mean-square estimation error remained less than $10^{-4}$, or $-40$dB, for
SNR $\geq 30$dB in all of our experiments.
  The key motivation for the novel channel estimation model is to gain
environment awareness, i.e., detecting changes in path delays and gains related
to interesting objects and events in the field. The channel state with
multipath delays and gains is a detailed measure to sense the field than the
single-tap channel state indicator calculated in current OFDM systems.",http://arxiv.org/abs/2402.07385v1,85,18,16,18,18,15
Generalized Preference Optimization: A Unified Approach to Offline Alignment,"Offline preference optimization allows fine-tuning large models directly from
offline data, and has proved effective in recent alignment practices. We
propose generalized preference optimization (GPO), a family of offline losses
parameterized by a general class of convex functions. GPO enables a unified
view over preference optimization, encompassing existing algorithms such as
DPO, IPO and SLiC as special cases, while naturally introducing new variants.
The GPO framework also sheds light on how offline algorithms enforce
regularization, through the design of the convex function that defines the
loss. Our analysis and experiments reveal the connections and subtle
differences between the offline regularization and the KL divergence
regularization intended by the canonical RLHF formulation. In all, our results
present new algorithmic toolkits and empirical insights to alignment
practitioners.",http://arxiv.org/abs/2402.05749v1,85,18,18,17,16,16
Enhancing Programming Error Messages in Real Time with Generative AI,"Generative AI is changing the way that many disciplines are taught, including
computer science. Researchers have shown that generative AI tools are capable
of solving programming problems, writing extensive blocks of code, and
explaining complex code in simple terms. Particular promise has been shown in
using generative AI to enhance programming error messages. Both students and
instructors have complained for decades that these messages are often cryptic
and difficult to understand. Yet recent work has shown that students make fewer
repeated errors when enhanced via GPT-4. We extend this work by implementing
feedback from ChatGPT for all programs submitted to our automated assessment
tool, Athene, providing help for compiler, run-time, and logic errors. Our
results indicate that adding generative AI to an automated assessment tool does
not necessarily make it better and that design of the interface matters greatly
to the usability of the feedback that GPT-4 provided.",http://dx.doi.org/10.1145/3613905.3647967,85,18,16,18,16,17
A Survey on Safe Multi-Modal Learning System,"With the wide deployment of multimodal learning systems (MMLS) in real-world
scenarios, safety concerns have become increasingly prominent. The absence of
systematic research into their safety is a significant barrier to progress in
this field. To bridge the gap, we present the first taxonomy for MMLS safety,
identifying four essential pillars of these concerns. Leveraging this taxonomy,
we conduct in-depth reviews for each pillar, highlighting key limitations based
on the current state of development. Finally, we pinpoint unique challenges in
MMLS safety and provide potential directions for future research.",http://arxiv.org/abs/2402.05355v1,85,18,17,18,16,16
Helping university students to choose elective courses by using a hybrid multi-criteria recommendation system with genetic optimization,"The wide availability of specific courses together with the flexibility of
academic plans in university studies reveal the importance of Recommendation
Systems (RSs) in this area. These systems appear as tools that help students to
choose courses that suit to their personal interests and their academic
performance. This paper presents a hybrid RS that combines Collaborative
Filtering (CF) and Content-based Filtering (CBF) using multiple criteria
related both to student and course information to recommend the most suitable
courses to the students. A Genetic Algorithm (GA) has been developed to
automatically discover the optimal RS configuration which include both the most
relevant criteria and the configuration of the rest of parameters. The
experimental study has used real information of Computer Science Degree of
University of Cordoba (Spain) including information gathered from students
during three academic years, counting on 2500 entries of 95 students and 63
courses. Experimental results show a study of the most relevant criteria for
the course recommendation, the importance of using a hybrid model that combines
both student information and course information to increase the reliability of
the recommendations as well as an excellent performance compared to previous
models.",http://dx.doi.org/10.1016/j.knosys.2019.105385,85,16,18,18,16,17
How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?,"In day-to-day communication, people often approximate the truth - for
example, rounding the time or omitting details - in order to be maximally
helpful to the listener. How do large language models (LLMs) handle such
nuanced trade-offs? To address this question, we use psychological models and
experiments designed to characterize human behavior to analyze LLMs. We test a
range of LLMs and explore how optimization for human preferences or
inference-time reasoning affects these trade-offs. We find that reinforcement
learning from human feedback improves both honesty and helpfulness, while
chain-of-thought prompting skews LLMs towards helpfulness over honesty.
Finally, GPT-4 Turbo demonstrates human-like response patterns including
sensitivity to the conversational framing and listener's decision context. Our
findings reveal the conversational values internalized by LLMs and suggest that
even these abstract values can, to a degree, be steered by zero-shot prompting.",http://arxiv.org/abs/2402.07282v2,85,17,17,16,18,17
FAQ-Gen: An automated system to generate domain-specific FAQs to aid content comprehension,"Frequently Asked Questions (FAQs) refer to the most common inquiries about
specific content. They serve as content comprehension aids by simplifying
topics and enhancing understanding through succinct presentation of
information. In this paper, we address FAQ generation as a well-defined Natural
Language Processing (NLP) task through the development of an end-to-end system
leveraging text-to-text transformation models. We present a literature review
covering traditional question-answering systems, highlighting their limitations
when applied directly to the FAQ generation task. We propose our system capable
of building FAQs from textual content tailored to specific domains, enhancing
their accuracy and relevance. We utilise self-curated algorithms for obtaining
optimal representation of information to be provided as input and also for
ranking the question-answer pairs to maximise human comprehension. Qualitative
human evaluation showcases the generated FAQs to be well-constructed and
readable, while also utilising domain-specific constructs to highlight
domain-based nuances and jargon in the original content.",http://arxiv.org/abs/2402.05812v1,85,18,17,17,16,17
"Neural Circuit Diagrams: Robust Diagrams for the Communication, Implementation, and Analysis of Deep Learning Architectures","Diagrams matter. Unfortunately, the deep learning community has no standard
method for diagramming architectures. The current combination of linear algebra
notation and ad-hoc diagrams fails to offer the necessary precision to
understand architectures in all their detail. However, this detail is critical
for faithful implementation, mathematical analysis, further innovation, and
ethical assurances. I present neural circuit diagrams, a graphical language
tailored to the needs of communicating deep learning architectures. Neural
circuit diagrams naturally keep track of the changing arrangement of data,
precisely show how operations are broadcast over axes, and display the critical
parallel behavior of linear operations. A lingering issue with existing
diagramming methods is the inability to simultaneously express the detail of
axes and the free arrangement of data, which neural circuit diagrams solve.
Their compositional structure is analogous to code, creating a close
correspondence between diagrams and implementation.
  In this work, I introduce neural circuit diagrams for an audience of machine
learning researchers. After introducing neural circuit diagrams, I cover a host
of architectures to show their utility and breed familiarity. This includes the
transformer architecture, convolution (and its difficult-to-explain
extensions), residual networks, the U-Net, and the vision transformer. I
include a Jupyter notebook that provides evidence for the close correspondence
between diagrams and code. Finally, I examine backpropagation using neural
circuit diagrams. I show their utility in providing mathematical insight and
analyzing algorithms' time and space complexities.",http://arxiv.org/abs/2402.05424v1,85,18,17,18,16,16
Is it Possible to Edit Large Language Models Robustly?,"Large language models (LLMs) have played a pivotal role in building
communicative AI to imitate human behaviors but face the challenge of efficient
customization. To tackle this challenge, recent studies have delved into the
realm of model editing, which manipulates specific memories of language models
and changes the related language generation. However, the robustness of model
editing remains an open question. This work seeks to understand the strengths
and limitations of editing methods, thus facilitating robust, realistic
applications of communicative AI. Concretely, we conduct extensive analysis to
address the three key research questions. Q1: Can edited LLMs behave
consistently resembling communicative AI in realistic situations? Q2: To what
extent does the rephrasing of prompts lead LLMs to deviate from the edited
knowledge memory? Q3: Which knowledge features are correlated with the
performance and robustness of editing? Our experimental results uncover a
substantial disparity between existing editing methods and the practical
application of LLMs. On rephrased prompts that are complex and flexible but
common in realistic applications, the performance of editing experiences a
significant decline. Further analysis shows that more popular knowledge is
memorized better, easier to recall, and more challenging to edit effectively.",http://arxiv.org/abs/2402.05827v1,85,16,19,18,17,15
Learning to Produce Semi-dense Correspondences for Visual Localization,"This study addresses the challenge of performing visual localization in
demanding conditions such as night-time scenarios, adverse weather, and
seasonal changes. While many prior studies have focused on improving
image-matching performance to facilitate reliable dense keypoint matching
between images, existing methods often heavily rely on predefined feature
points on a reconstructed 3D model. Consequently, they tend to overlook
unobserved keypoints during the matching process. Therefore, dense keypoint
matches are not fully exploited, leading to a notable reduction in accuracy,
particularly in noisy scenes. To tackle this issue, we propose a novel
localization method that extracts reliable semi-dense 2D-3D matching points
based on dense keypoint matches. This approach involves regressing semi-dense
2D keypoints into 3D scene coordinates using a point inference network. The
network utilizes both geometric and visual cues to effectively infer 3D
coordinates for unobserved keypoints from the observed ones. The abundance of
matching information significantly enhances the accuracy of camera pose
estimation, even in scenarios involving noisy or sparse 3D models.
Comprehensive evaluations demonstrate that the proposed method outperforms
other methods in challenging scenes and achieves competitive results in
large-scale visual localization benchmarks. The code will be available.",http://arxiv.org/abs/2402.08359v1,85,18,16,18,16,17
Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model,"Stepwise inference protocols, such as scratchpads and chain-of-thought, help
language models solve complex problems by decomposing them into a sequence of
simpler subproblems. Despite the significant gain in performance achieved via
these protocols, the underlying mechanisms of stepwise inference have remained
elusive. To address this, we propose to study autoregressive Transformer models
on a synthetic task that embodies the multi-step nature of problems where
stepwise inference is generally most useful. Specifically, we define a graph
navigation problem wherein a model is tasked with traversing a path from a
start to a goal node on the graph. Despite is simplicity, we find we can
empirically reproduce and analyze several phenomena observed at scale: (i) the
stepwise inference reasoning gap, the cause of which we find in the structure
of the training data; (ii) a diversity-accuracy tradeoff in model generations
as sampling temperature varies; (iii) a simplicity bias in the model's output;
and (iv) compositional generalization and a primacy bias with in-context
exemplars. Overall, our work introduces a grounded, synthetic framework for
studying stepwise inference and offers mechanistic hypotheses that can lay the
foundation for a deeper understanding of this phenomenon.",http://arxiv.org/abs/2402.07757v1,85,18,17,17,16,17
For Better or For Worse? Learning Minimum Variance Features With Label Augmentation,"Data augmentation has been pivotal in successfully training deep learning
models on classification tasks over the past decade. An important subclass of
data augmentation techniques - which includes both label smoothing and Mixup -
involves modifying not only the input data but also the input label during
model training. In this work, we analyze the role played by the label
augmentation aspect of such methods. We prove that linear models on linearly
separable data trained with label augmentation learn only the minimum variance
features in the data, while standard training (which includes weight decay) can
learn higher variance features. An important consequence of our results is
negative: label smoothing and Mixup can be less robust to adversarial
perturbations of the training data when compared to standard training. We
verify that our theory reflects practice via a range of experiments on
synthetic data and image classification benchmarks.",http://arxiv.org/abs/2402.06855v1,85,18,16,18,16,17
Learning to Route Among Specialized Experts for Zero-Shot Generalization,"Recently, there has been a widespread proliferation of ""expert"" language
models that are specialized to a specific task or domain through
parameter-efficient fine-tuning. How can we recycle large collections of expert
language models to improve zero-shot generalization to unseen tasks? In this
work, we propose Post-Hoc Adaptive Tokenwise Gating Over an Ocean of
Specialized Experts (PHATGOOSE), which learns to route among specialized
modules that were produced through parameter-efficient fine-tuning. Unlike past
methods that learn to route among specialized models, PHATGOOSE explores the
possibility that zero-shot generalization will be improved if different experts
can be adaptively chosen for each token and at each layer in the model.
Crucially, our method is post-hoc - it does not require simultaneous access to
the datasets used to create the specialized models and only requires a modest
amount of additional compute after each expert model is trained. In experiments
covering a range of specialized model collections and zero-shot generalization
benchmarks, we find that PHATGOOSE outperforms past methods for post-hoc
routing and, in some cases, outperforms explicit multitask training (which
requires simultaneous data access). To better understand the routing strategy
learned by PHATGOOSE, we perform qualitative experiments to validate that
PHATGOOSE's performance stems from its ability to make adaptive per-token and
per-module expert choices. We release all of our code to support future work on
improving zero-shot generalization by recycling specialized experts.",http://arxiv.org/abs/2402.05859v1,85,17,18,18,16,16
Enhancing Amharic-LLaMA: Integrating Task Specific and Generative Datasets,"Large language models (LLMs) have received a lot of attention in natural
language processing (NLP) research because of their exceptional performance in
understanding and generating human languages. However, low-resource languages
are left behind due to the unavailability of resources. In this work, we focus
on enhancing the LLaMA-2-Amharic model by integrating task-specific and
generative datasets to improve language model performance for Amharic. We
compile an Amharic instruction fine-tuning dataset and fine-tuned
LLaMA-2-Amharic model. The fine-tuned model shows promising results in
different NLP tasks. We open-source our dataset creation pipeline, instruction
datasets, trained models, and evaluation outputs to promote language-specific
studies on these models.",http://arxiv.org/abs/2402.08015v1,85,16,18,16,18,17
PIVOT-Net: Heterogeneous Point-Voxel-Tree-based Framework for Point Cloud Compression,"The universality of the point cloud format enables many 3D applications,
making the compression of point clouds a critical phase in practice. Sampled as
discrete 3D points, a point cloud approximates 2D surface(s) embedded in 3D
with a finite bit-depth. However, the point distribution of a practical point
cloud changes drastically as its bit-depth increases, requiring different
methodologies for effective consumption/analysis. In this regard, a
heterogeneous point cloud compression (PCC) framework is proposed. We unify
typical point cloud representations -- point-based, voxel-based, and tree-based
representations -- and their associated backbones under a learning-based
framework to compress an input point cloud at different bit-depth levels.
Having recognized the importance of voxel-domain processing, we augment the
framework with a proposed context-aware upsampling for decoding and an enhanced
voxel transformer for feature aggregation. Extensive experimentation
demonstrates the state-of-the-art performance of our proposal on a wide range
of point clouds.",http://arxiv.org/abs/2402.07243v1,85,16,18,18,16,17
LOSS-GAT: Label Propagation and One-Class Semi-Supervised Graph Attention Network for Fake News Detection,"In the era of widespread social networks, the rapid dissemination of fake
news has emerged as a significant threat, inflicting detrimental consequences
across various dimensions of people's lives. Machine learning and deep learning
approaches have been extensively employed for identifying fake news. However, a
significant challenge in identifying fake news is the limited availability of
labeled news datasets. Therefore, the One-Class Learning (OCL) approach,
utilizing only a small set of labeled data from the interest class, can be a
suitable approach to address this challenge. On the other hand, representing
data as a graph enables access to diverse content and structural information,
and label propagation methods on graphs can be effective in predicting node
labels. In this paper, we adopt a graph-based model for data representation and
introduce a semi-supervised and one-class approach for fake news detection,
called LOSS-GAT. Initially, we employ a two-step label propagation algorithm,
utilizing Graph Neural Networks (GNNs) as an initial classifier to categorize
news into two groups: interest (fake) and non-interest (real). Subsequently, we
enhance the graph structure using structural augmentation techniques.
Ultimately, we predict the final labels for all unlabeled data using a GNN that
induces randomness within the local neighborhood of nodes through the
aggregation function. We evaluate our proposed method on five common datasets
and compare the results against a set of baseline models, including both OCL
and binary labeled models. The results demonstrate that LOSS-GAT achieves a
notable improvement, surpassing 10%, with the advantage of utilizing only a
limited set of labeled fake news. Noteworthy, LOSS-GAT even outperforms binary
labeled models.",http://arxiv.org/abs/2402.08401v1,85,16,18,18,16,17
ML-Enabled Systems Model Deployment and Monitoring: Status Quo and Problems,"[Context] Systems incorporating Machine Learning (ML) models, often called
ML-enabled systems, have become commonplace. However, empirical evidence on how
ML-enabled systems are engineered in practice is still limited, especially for
activities surrounding ML model dissemination. [Goal] We investigate
contemporary industrial practices and problems related to ML model
dissemination, focusing on the model deployment and the monitoring of ML life
cycle phases. [Method] We conducted an international survey to gather
practitioner insights on how ML-enabled systems are engineered. We gathered a
total of 188 complete responses from 25 countries. We analyze the status quo
and problems reported for the model deployment and monitoring phases. We
analyzed contemporary practices using bootstrapping with confidence intervals
and conducted qualitative analyses on the reported problems applying open and
axial coding procedures. [Results] Practitioners perceive the model deployment
and monitoring phases as relevant and difficult. With respect to model
deployment, models are typically deployed as separate services, with limited
adoption of MLOps principles. Reported problems include difficulties in
designing the architecture of the infrastructure for production deployment and
legacy application integration. Concerning model monitoring, many models in
production are not monitored. The main monitored aspects are inputs, outputs,
and decisions. Reported problems involve the absence of monitoring practices,
the need to create custom monitoring tools, and the selection of suitable
metrics. [Conclusion] Our results help provide a better understanding of the
adopted practices and problems in practice and support guiding ML deployment
and monitoring research in a problem-driven manner.",http://arxiv.org/abs/2402.05333v1,85,16,18,18,18,15
Dual-disentangled Deep Multiple Clustering,"Multiple clustering has gathered significant attention in recent years due to
its potential to reveal multiple hidden structures of the data from different
perspectives. Most of multiple clustering methods first derive feature
representations by controlling the dissimilarity among them, subsequently
employing traditional clustering methods (e.g., k-means) to achieve the final
multiple clustering outcomes. However, the learned feature representations can
exhibit a weak relevance to the ultimate goal of distinct clustering. Moreover,
these features are often not explicitly learned for the purpose of clustering.
Therefore, in this paper, we propose a novel Dual-Disentangled deep Multiple
Clustering method named DDMC by learning disentangled representations.
Specifically, DDMC is achieved by a variational Expectation-Maximization (EM)
framework. In the E-step, the disentanglement learning module employs
coarse-grained and fine-grained disentangled representations to obtain a more
diverse set of latent factors from the data. In the M-step, the cluster
assignment module utilizes a cluster objective function to augment the
effectiveness of the cluster output. Our extensive experiments demonstrate that
DDMC consistently outperforms state-of-the-art methods across seven commonly
used tasks. Our code is available at https://github.com/Alexander-Yao/DDMC.",http://arxiv.org/abs/2402.05310v1,85,18,16,18,18,15
GET-Tok: A GenAI-Enriched Multimodal TikTok Dataset Documenting the 2022 Attempted Coup in Peru,"TikTok is one of the largest and fastest-growing social media sites in the
world. TikTok features, however, such as voice transcripts, are often missing
and other important features, such as OCR or video descriptions, do not exist.
We introduce the Generative AI Enriched TikTok (GET-Tok) data, a pipeline for
collecting TikTok videos and enriched data by augmenting the TikTok Research
API with generative AI models. As a case study, we collect videos about the
attempted coup in Peru initiated by its former President, Pedro Castillo, and
its accompanying protests. The data includes information on 43,697 videos
published from November 20, 2022 to March 1, 2023 (102 days). Generative AI
augments the collected data via transcripts of TikTok videos, text descriptions
of what is shown in the videos, what text is displayed within the video, and
the stances expressed in the video. Overall, this pipeline will contribute to a
better understanding of online discussion in a multimodal setting with
applications of Generative AI, especially outlining the utility of this
pipeline in non-English-language social media. Our code used to produce the
pipeline is in a public Github repository:
https://github.com/gabbypinto/GET-Tok-Peru.",http://arxiv.org/abs/2402.05882v1,85,17,18,17,16,17
Discriminative Adversarial Unlearning,"We introduce a novel machine unlearning framework founded upon the
established principles of the min-max optimization paradigm. We capitalize on
the capabilities of strong Membership Inference Attacks (MIA) to facilitate the
unlearning of specific samples from a trained model. We consider the scenario
of two networks, the attacker $\mathbf{A}$ and the trained defender
$\mathbf{D}$ pitted against each other in an adversarial objective, wherein the
attacker aims at teasing out the information of the data to be unlearned in
order to infer membership, and the defender unlearns to defend the network
against the attack, whilst preserving its general performance. The algorithm
can be trained end-to-end using backpropagation, following the well known
iterative min-max approach in updating the attacker and the defender. We
additionally incorporate a self-supervised objective effectively addressing the
feature space discrepancies between the forget set and the validation set,
enhancing unlearning performance. Our proposed algorithm closely approximates
the ideal benchmark of retraining from scratch for both random sample
forgetting and class-wise forgetting schemes on standard machine-unlearning
datasets. Specifically, on the class unlearning scheme, the method demonstrates
near-optimal performance and comprehensively overcomes known methods over the
random sample forgetting scheme across all metrics and multiple network pruning
strategies.",http://arxiv.org/abs/2402.06864v2,85,18,17,16,18,16
An Empirical Study Into What Matters for Calibrating Vision-Language Models,"Vision--Language Models (VLMs) have emerged as the dominant approach for
zero-shot recognition, adept at handling diverse scenarios and significant
distribution changes. However, their deployment in risk-sensitive areas
requires a deeper understanding of their uncertainty estimation capabilities, a
relatively uncharted area. In this study, we explore the calibration properties
of VLMs across different architectures, datasets, and training strategies. In
particular, we analyze the uncertainty estimation performance of VLMs when
calibrated in one domain, label set or hierarchy level, and tested in a
different one. Our findings reveal that while VLMs are not inherently
calibrated for uncertainty, temperature scaling significantly and consistently
improves calibration, even across shifts in distribution and changes in label
set. Moreover, VLMs can be calibrated with a very small set of examples.
Through detailed experimentation, we highlight the potential applications and
importance of our insights, aiming for more reliable and effective use of VLMs
in critical, real-world scenarios.",http://arxiv.org/abs/2402.07417v1,85,17,16,18,17,17
On the Transferability of Large-Scale Self-Supervision to Few-Shot Audio Classification,"In recent years, self-supervised learning has excelled for its capacity to
learn robust feature representations from unlabelled data. Networks pretrained
through self-supervision serve as effective feature extractors for downstream
tasks, including Few-Shot Learning. While the evaluation of unsupervised
approaches for few-shot learning is well-established in imagery, it is notably
absent in acoustics. This study addresses this gap by assessing large-scale
self-supervised models' performance in few-shot audio classification.
Additionally, we explore the relationship between a model's few-shot learning
capability and other downstream task benchmarks. Our findings reveal
state-of-the-art performance in some few-shot problems such as
SpeechCommandsv2, as well as strong correlations between speech-based few-shot
problems and various downstream audio tasks.",http://arxiv.org/abs/2402.01274v2,85,16,18,18,18,15
LB-KBQA: Large-language-model and BERT based Knowledge-Based Question and Answering System,"Generative Artificial Intelligence (AI), because of its emergent abilities,
has empowered various fields, one typical of which is large language models
(LLMs). One of the typical application fields of Generative AI is large
language models (LLMs), and the natural language understanding capability of
LLM is dramatically improved when compared with conventional AI-based methods.
The natural language understanding capability has always been a barrier to the
intent recognition performance of the Knowledge-Based-Question-and-Answer
(KBQA) system, which arises from linguistic diversity and the newly appeared
intent. Conventional AI-based methods for intent recognition can be divided
into semantic parsing-based and model-based approaches. However, both of the
methods suffer from limited resources in intent recognition. To address this
issue, we propose a novel KBQA system based on a Large Language Model(LLM) and
BERT (LB-KBQA). With the help of generative AI, our proposed method could
detect newly appeared intent and acquire new knowledge. In experiments on
financial domain question answering, our model has demonstrated superior
effectiveness.",http://arxiv.org/abs/2402.05130v2,85,16,18,16,18,17
Only the Curve Shape Matters: Training Foundation Models for Zero-Shot Multivariate Time Series Forecasting through Next Curve Shape Prediction,"We present General Time Transformer (GTT), an encoder-only style foundation
model for zero-shot multivariate time series forecasting. GTT is pretrained on
a large dataset of 200M high-quality time series samples spanning diverse
domains. In our proposed framework, the task of multivariate time series
forecasting is formulated as a channel-wise next curve shape prediction
problem, where each time series sample is represented as a sequence of
non-overlapping curve shapes with a unified numerical magnitude. GTT is trained
to predict the next curve shape based on a window of past curve shapes in a
channel-wise manner. Experimental results demonstrate that GTT exhibits
superior zero-shot multivariate forecasting capabilities on unseen time series
datasets, even surpassing state-of-the-art supervised baselines. Additionally,
we investigate the impact of varying GTT model parameters and training dataset
scales, observing that the scaling law also holds in the context of zero-shot
multivariate time series forecasting.",http://arxiv.org/abs/2402.07570v1,85,18,17,17,16,17
"Retrieve, Merge, Predict: Augmenting Tables with Data Lakes","We present an in-depth analysis of data discovery in data lakes, focusing on
table augmentation for given machine learning tasks. We analyze alternative
methods used in the three main steps: retrieving joinable tables, merging
information, and predicting with the resultant table. As data lakes, the paper
uses YADL (Yet Another Data Lake) -- a novel dataset we developed as a tool for
benchmarking this data discovery task -- and Open Data US, a well-referenced
real data lake. Through systematic exploration on both lakes, our study
outlines the importance of accurately retrieving join candidates and the
efficiency of simple merging methods. We report new insights on the benefits of
existing solutions and on their limitations, aiming at guiding future research
in this space.",http://arxiv.org/abs/2402.06282v2,85,16,18,18,16,17
Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine,"We present LARL-RM (Large language model-generated Automaton for
Reinforcement Learning with Reward Machine) algorithm in order to encode
high-level knowledge into reinforcement learning using automaton to expedite
the reinforcement learning. Our method uses Large Language Models (LLM) to
obtain high-level domain-specific knowledge using prompt engineering instead of
providing the reinforcement learning algorithm directly with the high-level
knowledge which requires an expert to encode the automaton. We use
chain-of-thought and few-shot methods for prompt engineering and demonstrate
that our method works using these approaches. Additionally, LARL-RM allows for
fully closed-loop reinforcement learning without the need for an expert to
guide and supervise the learning since LARL-RM can use the LLM directly to
generate the required high-level knowledge for the task at hand. We also show
the theoretical guarantee of our algorithm to converge to an optimal policy. We
demonstrate that LARL-RM speeds up the convergence by 30% by implementing our
method in two case studies.",http://arxiv.org/abs/2402.07069v1,85,18,17,18,16,16
Improving 2D-3D Dense Correspondences with Diffusion Models for 6D Object Pose Estimation,"Estimating 2D-3D correspondences between RGB images and 3D space is a
fundamental problem in 6D object pose estimation. Recent pose estimators use
dense correspondence maps and Point-to-Point algorithms to estimate object
poses. The accuracy of pose estimation depends heavily on the quality of the
dense correspondence maps and their ability to withstand occlusion, clutter,
and challenging material properties. Currently, dense correspondence maps are
estimated using image-to-image translation models based on GANs, Autoencoders,
or direct regression models. However, recent advancements in image-to-image
translation have led to diffusion models being the superior choice when
evaluated on benchmarking datasets. In this study, we compare image-to-image
translation networks based on GANs and diffusion models for the downstream task
of 6D object pose estimation. Our results demonstrate that the diffusion-based
image-to-image translation model outperforms the GAN, revealing potential for
further improvements in 6D object pose estimation models.",http://arxiv.org/abs/2402.06436v1,85,16,17,18,18,16
MLS2LoD3: Refining low LoDs building models with MLS point clouds to reconstruct semantic LoD3 building models,"Although highly-detailed LoD3 building models reveal great potential in
various applications, they have yet to be available. The primary challenges in
creating such models concern not only automatic detection and reconstruction
but also standard-consistent modeling. In this paper, we introduce a novel
refinement strategy enabling LoD3 reconstruction by leveraging the ubiquity of
lower LoD building models and the accuracy of MLS point clouds. Such a strategy
promises at-scale LoD3 reconstruction and unlocks LoD3 applications, which we
also describe and illustrate in this paper. Additionally, we present guidelines
for reconstructing LoD3 facade elements and their embedding into the CityGML
standard model, disseminating gained knowledge to academics and professionals.
We believe that our method can foster development of LoD3 reconstruction
algorithms and subsequently enable their wider adoption.",http://arxiv.org/abs/2402.06288v1,85,18,18,16,17,16
The SpongeNet Attack: Sponge Weight Poisoning of Deep Neural Networks,"Sponge attacks aim to increase the energy consumption and computation time of
neural networks deployed on hardware accelerators. Existing sponge attacks can
be performed during inference via sponge examples or during training via Sponge
Poisoning. Sponge examples leverage perturbations added to the model's input to
increase energy and latency, while Sponge Poisoning alters the objective
function of a model to induce inference-time energy/latency effects.
  In this work, we propose a novel sponge attack called SpongeNet. SpongeNet is
the first sponge attack that is performed directly on the parameters of a
pre-trained model. Our experiments show that SpongeNet can successfully
increase the energy consumption of vision models with fewer samples required
than Sponge Poisoning. Our experiments indicate that poisoning defenses are
ineffective if not adjusted specifically for the defense against Sponge
Poisoning (i.e., they decrease batch normalization bias values). Our work shows
that SpongeNet is more effective on StarGAN than the state-of-the-art.
Additionally, SpongeNet is stealthier than the previous Sponge Poisoning attack
as it does not require significant changes in the victim model's weights. Our
experiments indicate that the SpongeNet attack can be performed even when an
attacker has access to only 1% of the entire dataset and reach up to 11% energy
increase.",http://arxiv.org/abs/2402.06357v1,85,18,18,17,16,16
Overconfident and Unconfident AI Hinder Human-AI Collaboration,"As artificial intelligence (AI) advances, human-AI collaboration has become
increasingly prevalent across both professional and everyday settings. In such
collaboration, AI can express its confidence level about its performance,
serving as a crucial indicator for humans to evaluate AI's suggestions.
However, AI may exhibit overconfidence or underconfidence--its expressed
confidence is higher or lower than its actual performance--which may lead
humans to mistakenly evaluate AI advice. Our study investigates the influences
of AI's overconfidence and underconfidence on human trust, their acceptance of
AI suggestions, and collaboration outcomes. Our study reveal that disclosing AI
confidence levels and performance feedback facilitates better recognition of AI
confidence misalignments. However, participants tend to withhold their trust as
perceiving such misalignments, leading to a rejection of AI suggestions and
subsequently poorer performance in collaborative tasks. Conversely, without
such information, participants struggle to identify misalignments, resulting in
either the neglect of correct AI advice or the following of incorrect AI
suggestions, adversely affecting collaboration. This study offers valuable
insights for enhancing human-AI collaboration by underscoring the importance of
aligning AI's expressed confidence with its actual performance and the
necessity of calibrating human trust towards AI confidence.",http://arxiv.org/abs/2402.07632v1,85,18,16,18,17,16
On the Resurgence of Recurrent Models for Long Sequences: Survey and Research Opportunities in the Transformer Era,"A longstanding challenge for the Machine Learning community is the one of
developing models that are capable of processing and learning from very long
sequences of data. The outstanding results of Transformers-based networks
(e.g., Large Language Models) promotes the idea of parallel attention as the
key to succeed in such a challenge, obfuscating the role of classic sequential
processing of Recurrent Models. However, in the last few years, researchers who
were concerned by the quadratic complexity of self-attention have been
proposing a novel wave of neural models, which gets the best from the two
worlds, i.e., Transformers and Recurrent Nets. Meanwhile, Deep Space-State
Models emerged as robust approaches to function approximation over time, thus
opening a new perspective in learning from sequential data, followed by many
people in the field and exploited to implement a special class of (linear)
Recurrent Neural Networks. This survey is aimed at providing an overview of
these trends framed under the unifying umbrella of Recurrence. Moreover, it
emphasizes novel research opportunities that become prominent when abandoning
the idea of processing long sequences whose length is known-in-advance for the
more realistic setting of potentially infinite-length sequences, thus
intersecting the field of lifelong-online learning from streamed data.",http://arxiv.org/abs/2402.08132v1,85,16,18,18,19,14
SWITCH: An Exemplar for Evaluating Self-Adaptive ML-Enabled Systems,"Addressing runtime uncertainties in Machine Learning-Enabled Systems (MLS) is
crucial for maintaining Quality of Service (QoS). The Machine Learning Model
Balancer is a concept that addresses these uncertainties by facilitating
dynamic ML model switching, showing promise in improving QoS in MLS. Leveraging
this concept, this paper introduces SWITCH, an exemplar developed to enhance
self-adaptive capabilities in such systems through dynamic model switching in
runtime. SWITCH is designed as a comprehensive web service catering to a broad
range of ML scenarios, with its implementation demonstrated through an object
detection use case. SWITCH provides researchers with a flexible platform to
apply and evaluate their ML model switching strategies, aiming to enhance QoS
in MLS. SWITCH features advanced input handling, real-time data processing, and
logging for adaptation metrics supplemented with an interactive real-time
dashboard for enhancing system observability. This paper details SWITCH's
architecture, self-adaptation strategies through ML model switching, and its
empirical validation through a case study, illustrating its potential to
improve QoS in MLS. By enabling a hands-on approach to explore adaptive
behaviors in ML systems, SWITCH contributes a valuable tool to the SEAMS
community for research into self-adaptive mechanisms for MLS and their
practical applications.",http://arxiv.org/abs/2402.06351v1,85,18,18,17,16,16
Hierarchical Position Embedding of Graphs with Landmarks and Clustering for Link Prediction,"Learning positional information of nodes in a graph is important for link
prediction tasks. We propose a representation of positional information using
representative nodes called landmarks. A small number of nodes with high degree
centrality are selected as landmarks, which serve as reference points for the
nodes' positions. We justify this selection strategy for well-known random
graph models and derive closed-form bounds on the average path lengths
involving landmarks. In a model for power-law graphs, we prove that landmarks
provide asymptotically exact information on inter-node distances. We apply
theoretical insights to practical networks and propose Hierarchical Position
embedding with Landmarks and Clustering (HPLC). HPLC combines landmark
selection and graph clustering, where the graph is partitioned into densely
connected clusters in which nodes with the highest degree are selected as
landmarks. HPLC leverages the positional information of nodes based on
landmarks at various levels of hierarchy such as nodes' distances to landmarks,
inter-landmark distances and hierarchical grouping of clusters. Experiments
show that HPLC achieves state-of-the-art performances of link prediction on
various datasets in terms of HIT@K, MRR, and AUC. The code is available at
\url{https://github.com/kmswin1/HPLC}.",http://dx.doi.org/10.1145/3589334.3645372,85,16,18,18,16,17
ASAP-Repair: API-Specific Automated Program Repair Based on API Usage Graphs,"Modern software development relies on the reuse of code via Application
Programming Interfaces (APIs). Such reuse relieves developers from learning and
developing established algorithms and data structures anew, enabling them to
focus on their problem at hand. However, there is also the risk of misusing an
API due to a lack of understanding or proper documentation. While many
techniques target API misuse detection, only limited efforts have been put into
automatically repairing API misuses. In this paper, we present our advances on
our technique API-Specific Automated Program Repair (ASAP-Repair). ASAP-Repair
is intended to fix API misuses based on API Usage Graphs (AUGs) by leveraging
API usage templates of state-of-the-art API misuse detectors. We demonstrate
that ASAP-Repair is in principle applicable on an established API misuse
dataset. Moreover, we discuss next steps and challenges to evolve ASAP-Repair
towards a full-fledged Automatic Program Repair (APR) technique.",http://dx.doi.org/10.1145/3643788.3648011,85,17,16,17,17,18
Continual Learning on Graphs: A Survey,"Recently, continual graph learning has been increasingly adopted for diverse
graph-structured data processing tasks in non-stationary environments. Despite
its promising learning capability, current studies on continual graph learning
mainly focus on mitigating the catastrophic forgetting problem while ignoring
continuous performance improvement. To bridge this gap, this article aims to
provide a comprehensive survey of recent efforts on continual graph learning.
Specifically, we introduce a new taxonomy of continual graph learning from the
perspective of overcoming catastrophic forgetting. Moreover, we systematically
analyze the challenges of applying these continual graph learning methods in
improving performance continuously and then discuss the possible solutions.
Finally, we present open issues and future directions pertaining to the
development of continual graph learning and discuss how they impact continuous
performance improvement.",http://arxiv.org/abs/2402.06330v1,85,17,18,17,16,17
Graph Mamba: Towards Learning on Graphs with State Space Models,"Graph Neural Networks (GNNs) have shown promising potential in graph
representation learning. The majority of GNNs define a local message-passing
mechanism, propagating information over the graph by stacking multiple layers.
These methods, however, are known to suffer from two major limitations:
over-squashing and poor capturing of long-range dependencies. Recently, Graph
Transformers (GTs) emerged as a powerful alternative to Message-Passing Neural
Networks (MPNNs). GTs, however, have quadratic computational cost, lack
inductive biases on graph structures, and rely on complex Positional/Structural
Encodings (SE/PE). In this paper, we show that while Transformers, complex
message-passing, and SE/PE are sufficient for good performance in practice,
neither is necessary. Motivated by the recent success of State Space Models
(SSMs), such as Mamba, we present Graph Mamba Networks (GMNs), a general
framework for a new class of GNNs based on selective SSMs. We discuss and
categorize the new challenges when adopting SSMs to graph-structured data, and
present four required and one optional steps to design GMNs, where we choose
(1) Neighborhood Tokenization, (2) Token Ordering, (3) Architecture of
Bidirectional Selective SSM Encoder, (4) Local Encoding, and dispensable (5) PE
and SE. We further provide theoretical justification for the power of GMNs.
Experiments demonstrate that despite much less computational cost, GMNs attain
an outstanding performance in long-range, small-scale, large-scale, and
heterophilic benchmark datasets.",http://arxiv.org/abs/2402.08678v1,85,18,16,18,18,15
Dynamic Q-planning for Online UAV Path Planning in Unknown and Complex Environments,"Unmanned Aerial Vehicles need an online path planning capability to move in
high-risk missions in unknown and complex environments to complete them safely.
However, many algorithms reported in the literature may not return reliable
trajectories to solve online problems in these scenarios. The Q-Learning
algorithm, a Reinforcement Learning Technique, can generate trajectories in
real-time and has demonstrated fast and reliable results. This technique,
however, has the disadvantage of defining the iteration number. If this value
is not well defined, it will take a long time or not return an optimal
trajectory. Therefore, we propose a method to dynamically choose the number of
iterations to obtain the best performance of Q-Learning. The proposed method is
compared to the Q-Learning algorithm with a fixed number of iterations, A*,
Rapid-Exploring Random Tree, and Particle Swarm Optimization. As a result, the
proposed Q-learning algorithm demonstrates the efficacy and reliability of
online path planning with a dynamic number of iterations to carry out online
missions in unknown and complex environments.",http://arxiv.org/abs/2402.06297v1,85,16,18,18,18,15
Common-Sense Bias Discovery and Mitigation for Classification Tasks,"Machine learning model bias can arise from dataset composition: sensitive
features correlated to the learning target disturb the model decision rule and
lead to performance differences along the features. Existing de-biasing work
captures prominent and delicate image features which are traceable in model
latent space, like colors of digits or background of animals. However, using
the latent space is not sufficient to understand all dataset feature
correlations. In this work, we propose a framework to extract feature clusters
in a dataset based on image descriptions, allowing us to capture both subtle
and coarse features of the images. The feature co-occurrence pattern is
formulated and correlation is measured, utilizing a human-in-the-loop for
examination. The analyzed features and correlations are human-interpretable, so
we name the method Common-Sense Bias Discovery (CSBD). Having exposed sensitive
correlations in a dataset, we demonstrate that downstream model bias can be
mitigated by adjusting image sampling weights, without requiring a sensitive
group label supervision. Experiments show that our method discovers novel
biases on multiple classification tasks for two benchmark image datasets, and
the intervention outperforms state-of-the-art unsupervised bias mitigation
methods.",http://arxiv.org/abs/2401.13213v2,85,18,17,17,16,17
Electricity Price Forecasting in the Irish Balancing Market,"Short-term electricity markets are becoming more relevant due to
less-predictable renewable energy sources, attracting considerable attention
from the industry. The balancing market is the closest to real-time and the
most volatile among them. Its price forecasting literature is limited,
inconsistent and outdated, with few deep learning attempts and no public
dataset. This work applies to the Irish balancing market a variety of price
prediction techniques proven successful in the widely studied day-ahead market.
We compare statistical, machine learning, and deep learning models using a
framework that investigates the impact of different training sizes. The
framework defines hyperparameters and calibration settings; the dataset and
models are made public to ensure reproducibility and to be used as benchmarks
for future works. An extensive numerical study shows that well-performing
models in the day-ahead market do not perform well in the balancing one,
highlighting that these markets are fundamentally different constructs. The
best model is LEAR, a statistical approach based on LASSO, which outperforms
more complex and computationally demanding approaches.",http://arxiv.org/abs/2402.06714v1,85,16,17,18,17,17
CoRe-GD: A Hierarchical Framework for Scalable Graph Visualization with GNNs,"Graph Visualization, also known as Graph Drawing, aims to find geometric
embeddings of graphs that optimize certain criteria. Stress is a widely used
metric; stress is minimized when every pair of nodes is positioned at their
shortest path distance. However, stress optimization presents computational
challenges due to its inherent complexity and is usually solved using
heuristics in practice. We introduce a scalable Graph Neural Network (GNN)
based Graph Drawing framework with sub-quadratic runtime that can learn to
optimize stress. Inspired by classical stress optimization techniques and
force-directed layout algorithms, we create a coarsening hierarchy for the
input graph. Beginning at the coarsest level, we iteratively refine and
un-coarsen the layout, until we generate an embedding for the original graph.
To enhance information propagation within the network, we propose a novel
positional rewiring technique based on intermediate node positions. Our
empirical evaluation demonstrates that the framework achieves state-of-the-art
performance while remaining scalable.",http://arxiv.org/abs/2402.06706v1,85,18,16,18,17,16
Target Score Matching,"Denoising Score Matching estimates the score of a noised version of a target
distribution by minimizing a regression loss and is widely used to train the
popular class of Denoising Diffusion Models. A well known limitation of
Denoising Score Matching, however, is that it yields poor estimates of the
score at low noise levels. This issue is particularly unfavourable for problems
in the physical sciences and for Monte Carlo sampling tasks for which the score
of the clean original target is known. Intuitively, estimating the score of a
slightly noised version of the target should be a simple task in such cases. In
this paper, we address this shortcoming and show that it is indeed possible to
leverage knowledge of the target score. We present a Target Score Identity and
corresponding Target Score Matching regression loss which allows us to obtain
score estimates admitting favourable properties at low noise levels.",http://arxiv.org/abs/2402.08667v1,85,18,16,18,18,15
Moonwalk: Advancing Gait-Based User Recognition on Wearable Devices with Metric Learning,"Personal devices have adopted diverse authentication methods, including
biometric recognition and passcodes. In contrast, headphones have limited input
mechanisms, depending solely on the authentication of connected devices. We
present Moonwalk, a novel method for passive user recognition utilizing the
built-in headphone accelerometer. Our approach centers on gait recognition;
enabling users to establish their identity simply by walking for a brief
interval, despite the sensor's placement away from the feet. We employ
self-supervised metric learning to train a model that yields a highly
discriminative representation of a user's 3D acceleration, with no retraining
required. We tested our method in a study involving 50 participants, achieving
an average F1 score of 92.9% and equal error rate of 2.3%. We extend our
evaluation by assessing performance under various conditions (e.g. shoe types
and surfaces). We discuss the opportunities and challenges these variations
introduce and propose new directions for advancing passive authentication for
wearable devices.",http://arxiv.org/abs/2402.08451v1,85,18,16,18,16,17
Speech Rhythm-Based Speaker Embeddings Extraction from Phonemes and Phoneme Duration for Multi-Speaker Speech Synthesis,"This paper proposes a speech rhythm-based method for speaker embeddings to
model phoneme duration using a few utterances by the target speaker. Speech
rhythm is one of the essential factors among speaker characteristics, along
with acoustic features such as F0, for reproducing individual utterances in
speech synthesis. A novel feature of the proposed method is the rhythm-based
embeddings extracted from phonemes and their durations, which are known to be
related to speaking rhythm. They are extracted with a speaker identification
model similar to the conventional spectral feature-based one. We conducted
three experiments, speaker embeddings generation, speech synthesis with
generated embeddings, and embedding space analysis, to evaluate the
performance. The proposed method demonstrated a moderate speaker identification
performance (15.2% EER), even with only phonemes and their duration
information. The objective and subjective evaluation results demonstrated that
the proposed method can synthesize speech with speech rhythm closer to the
target speaker than the conventional method. We also visualized the embeddings
to evaluate the relationship between the distance of the embeddings and the
perceptual similarity. The visualization of the embedding space and the
relation analysis between the closeness indicated that the distribution of
embeddings reflects the subjective and objective similarity.",http://dx.doi.org/10.1587/transinf.2023EDP7039,85,16,18,17,19,15
Two-Stage Multi-task Self-Supervised Learning for Medical Image Segmentation,"Medical image segmentation has been significantly advanced by deep learning
(DL) techniques, though the data scarcity inherent in medical applications
poses a great challenge to DL-based segmentation methods. Self-supervised
learning offers a solution by creating auxiliary learning tasks from the
available dataset and then leveraging the knowledge acquired from solving
auxiliary tasks to help better solve the target segmentation task. Different
auxiliary tasks may have different properties and thus can help the target task
to different extents. It is desired to leverage their complementary advantages
to enhance the overall assistance to the target task. To achieve this, existing
methods often adopt a joint training paradigm, which co-solves segmentation and
auxiliary tasks by integrating their losses or intermediate gradients. However,
direct coupling of losses or intermediate gradients risks undesirable
interference because the knowledge acquired from solving each auxiliary task at
every training step may not always benefit the target task. To address this
issue, we propose a two-stage training approach. In the first stage, the target
segmentation task will be independently co-solved with each auxiliary task in
both joint training and pre-training modes, with the better model selected via
validation performance. In the second stage, the models obtained with respect
to each auxiliary task are converted into a single model using an ensemble
knowledge distillation method. Our approach allows for making best use of each
auxiliary task to create multiple elite segmentation models and then combine
them into an even more powerful model. We employed five auxiliary tasks of
different proprieties in our approach and applied it to train the U-Net model
on an X-ray pneumothorax segmentation dataset. Experimental results demonstrate
the superiority of our approach over several existing methods.",http://arxiv.org/abs/2402.07119v1,85,18,16,17,16,18
Architectural Neural Backdoors from First Principles,"While previous research backdoored neural networks by changing their
parameters, recent work uncovered a more insidious threat: backdoors embedded
within the definition of the network's architecture. This involves injecting
common architectural components, such as activation functions and pooling
layers, to subtly introduce a backdoor behavior that persists even after (full
re-)training. However, the full scope and implications of architectural
backdoors have remained largely unexplored. Bober-Irizar et al. [2023]
introduced the first architectural backdoor; they showed how to create a
backdoor for a checkerboard pattern, but never explained how to target an
arbitrary trigger pattern of choice. In this work we construct an arbitrary
trigger detector which can be used to backdoor an architecture with no human
supervision. This leads us to revisit the concept of architecture backdoors and
taxonomise them, describing 12 distinct types. To gauge the difficulty of
detecting such backdoors, we conducted a user study, revealing that ML
developers can only identify suspicious components in common model definitions
as backdoors in 37% of cases, while they surprisingly preferred backdoored
models in 33% of cases. To contextualize these results, we find that language
models outperform humans at the detection of backdoors. Finally, we discuss
defenses against architectural backdoors, emphasizing the need for robust and
comprehensive strategies to safeguard the integrity of ML systems.",http://arxiv.org/abs/2402.06957v1,85,18,17,18,16,16
A harmonized and interoperable format for storing and processing polysomnography data,"Polysomnography (PSG) data is recorded and stored in various formats
depending on the recording software. Although the PSG data can usually be
exported to open formats, such as the European Data Format (EDF), they are
limited in data types, validation, and readability. Moreover, the exported data
is not harmonized, which means different datasets need customized preprocessing
to conduct research on multiple datasets. In this work, we designed and
implemented an open format for storage and processing of PSG data, called the
Sleeplab format (SLF), which is both human and machine-readable, and has
built-in validation of both data types and structures. SLF provides tools for
reading, writing, and compression of the PSG datasets. In addition, SLF
promotes harmonization of data from different sources, which reduces the amount
of work needed to apply the same analytics pipelines to different datasets. SLF
is interoperable as it utilizes the file system and commonly used file formats
to store the data. The goal of developing SLF was to enable fast exploration
and experimentation on PSG data, and to streamline the workflow of building
analytics and machine learning applications that combine PSG data from multiple
sources. The performance of SLF was tested with two open datasets of different
formats (EDF and HDF5). SLF is fully open source and available at
https://github.com/UEF-SmartSleepLab/sleeplab-format.",http://arxiv.org/abs/2402.06702v1,85,18,16,18,18,15
Lumos : Empowering Multimodal LLMs with Scene Text Recognition,"We introduce Lumos, the first end-to-end multimodal question-answering system
with text understanding capabilities. At the core of Lumos is a Scene Text
Recognition (STR) component that extracts text from first person point-of-view
images, the output of which is used to augment input to a Multimodal Large
Language Model (MM-LLM). While building Lumos, we encountered numerous
challenges related to STR quality, overall latency, and model inference. In
this paper, we delve into those challenges, and discuss the system
architecture, design choices, and modeling techniques employed to overcome
these obstacles. We also provide a comprehensive evaluation for each component,
showcasing high quality and efficiency.",http://arxiv.org/abs/2402.08017v1,85,18,17,17,18,15
Learning the Expected Core of Strictly Convex Stochastic Cooperative Games,"Reward allocation, also known as the credit assignment problem, has been an
important topic in economics, engineering, and machine learning. An important
concept in credit assignment is the core, which is the set of stable
allocations where no agent has the motivation to deviate from the grand
coalition. In this paper, we consider the stable allocation learning problem of
stochastic cooperative games, where the reward function is characterised as a
random variable with an unknown distribution. Given an oracle that returns a
stochastic reward for an enquired coalition each round, our goal is to learn
the expected core, that is, the set of allocations that are stable in
expectation. Within the class of strictly convex games, we present an algorithm
named \texttt{Common-Points-Picking} that returns a stable allocation given a
polynomial number of samples, with high probability. The analysis of our
algorithm involves the development of several new results in convex geometry,
including an extension of the separation hyperplane theorem for multiple convex
sets, and may be of independent interest.",http://arxiv.org/abs/2402.07067v1,85,18,16,16,18,17
Event-Keyed Summarization,"We introduce event-keyed summarization (EKS), a novel task that marries
traditional summarization and document-level event extraction, with the goal of
generating a contextualized summary for a specific event, given a document and
an extracted event structure. We introduce a dataset for this task, MUCSUM,
consisting of summaries of all events in the classic MUC-4 dataset, along with
a set of baselines that comprises both pretrained LM standards in the
summarization literature, as well as larger frontier models. We show that
ablations that reduce EKS to traditional summarization or structure-to-text
yield inferior summaries of target events and that MUCSUM is a robust benchmark
for this task. Lastly, we conduct a human evaluation of both reference and
model summaries, and provide some detailed analysis of the results.",http://arxiv.org/abs/2402.06973v1,85,18,17,18,16,16
Privacy Profiles for Private Selection,"Private selection mechanisms (e.g., Report Noisy Max, Sparse Vector) are
fundamental primitives of differentially private (DP) data analysis with wide
applications to private query release, voting, and hyperparameter tuning.
Recent work (Liu and Talwar, 2019; Papernot and Steinke, 2022) has made
significant progress in both generalizing private selection mechanisms and
tightening their privacy analysis using modern numerical privacy accounting
tools, e.g., R\'enyi DP. But R\'enyi DP is known to be lossy when
$(\epsilon,\delta)$-DP is ultimately needed, and there is a trend to close the
gap by directly handling privacy profiles, i.e., $\delta$ as a function of
$\epsilon$ or its equivalent dual form known as $f$-DPs. In this paper, we work
out an easy-to-use recipe that bounds the privacy profiles of ReportNoisyMax
and PrivateTuning using the privacy profiles of the base algorithms they
corral. Numerically, our approach improves over the RDP-based accounting in all
regimes of interest and leads to substantial benefits in end-to-end private
learning experiments. Our analysis also suggests new distributions, e.g.,
binomial distribution for randomizing the number of rounds that leads to more
substantial improvements in certain regimes.",http://arxiv.org/abs/2402.06701v1,85,18,17,17,16,17
Challenging Low Homophily in Social Recommendation,"Social relations are leveraged to tackle the sparsity issue of user-item
interaction data in recommendation under the assumption of social homophily.
However, social recommendation paradigms predominantly focus on homophily based
on user preferences. While social information can enhance recommendations, its
alignment with user preferences is not guaranteed, thereby posing the risk of
introducing informational redundancy. We empirically discover that social
graphs in real recommendation data exhibit low preference-aware homophily,
which limits the effect of social recommendation models. To comprehensively
extract preference-aware homophily information latent in the social graph, we
propose Social Heterophily-alleviating Rewiring (SHaRe), a data-centric
framework for enhancing existing graph-based social recommendation models. We
adopt Graph Rewiring technique to capture and add highly homophilic social
relations, and cut low homophilic (or heterophilic) relations. To better refine
the user representations from reliable social relations, we integrate a
contrastive learning method into the training of SHaRe, aiming to calibrate the
user representations for enhancing the result of Graph Rewiring. Experiments on
real-world datasets show that the proposed framework not only exhibits enhanced
performances across varying homophily ratios but also improves the performance
of existing state-of-the-art (SOTA) social recommendation models.",http://arxiv.org/abs/2401.14606v2,85,17,18,16,17,17
Group Decision-Making among Privacy-Aware Agents,"How can individuals exchange information to learn from each other despite
their privacy needs and security concerns? For example, consider individuals
deliberating a contentious topic and being concerned about divulging their
private experiences. Preserving individual privacy and enabling efficient
social learning are both important desiderata but seem fundamentally at odds
with each other and very hard to reconcile. We do so by controlling information
leakage using rigorous statistical guarantees that are based on differential
privacy (DP). Our agents use log-linear rules to update their beliefs after
communicating with their neighbors. Adding DP randomization noise to beliefs
provides communicating agents with plausible deniability with regard to their
private information and their network neighborhoods. We consider two learning
environments one for distributed maximum-likelihood estimation given a finite
number of private signals and another for online learning from an infinite,
intermittent signal stream. Noisy information aggregation in the finite case
leads to interesting tradeoffs between rejecting low-quality states and making
sure all high-quality states are accepted in the algorithm output. Our results
flesh out the nature of the trade-offs in both cases between the quality of the
group decision outcomes, learning accuracy, communication cost, and the level
of privacy protections that the agents are afforded.",http://arxiv.org/abs/2402.08156v1,85,18,16,17,18,16
Adaptive Block Sparse Regularization under Arbitrary Linear Transform,"We propose a convex and fast signal reconstruction method for block sparsity
under arbitrary linear transform with unknown block structure. The proposed
method is a generalization of the similar existing method and can reconstruct
signals with block sparsity under non-invertible transforms, unlike the
existing method. Our work broadens the scope of block sparse regularization,
enabling more versatile and powerful applications across various signal
processing domains. We derive an iterative algorithm for solving proposed
method and provide conditions for its convergence to the optimal solution.
Numerical experiments demonstrate the effectiveness of the proposed method.",http://arxiv.org/abs/2401.15292v4,85,18,16,18,16,17
Scalable Kernel Logistic Regression with Nystrm Approximation: Theoretical Analysis and Application to Discrete Choice Modelling,"The application of kernel-based Machine Learning (ML) techniques to discrete
choice modelling using large datasets often faces challenges due to memory
requirements and the considerable number of parameters involved in these
models. This complexity hampers the efficient training of large-scale models.
This paper addresses these problems of scalability by introducing the Nystr\""om
approximation for Kernel Logistic Regression (KLR) on large datasets. The study
begins by presenting a theoretical analysis in which: i) the set of KLR
solutions is characterised, ii) an upper bound to the solution of KLR with
Nystr\""om approximation is provided, and finally iii) a specialisation of the
optimisation algorithms to Nystr\""om KLR is described. After this, the
Nystr\""om KLR is computationally validated. Four landmark selection methods are
tested, including basic uniform sampling, a k-means sampling strategy, and two
non-uniform methods grounded in leverage scores. The performance of these
strategies is evaluated using large-scale transport mode choice datasets and is
compared with traditional methods such as Multinomial Logit (MNL) and
contemporary ML techniques. The study also assesses the efficiency of various
optimisation techniques for the proposed Nystr\""om KLR model. The performance
of gradient descent, Momentum, Adam, and L-BFGS-B optimisation methods is
examined on these datasets. Among these strategies, the k-means Nystr\""om KLR
approach emerges as a successful solution for applying KLR to large datasets,
particularly when combined with the L-BFGS-B and Adam optimisation methods. The
results highlight the ability of this strategy to handle datasets exceeding
200,000 observations while maintaining robust performance.",http://arxiv.org/abs/2402.06763v1,84,16,18,18,16,16
Efficient Resource Scheduling for Distributed Infrastructures Using Negotiation Capabilities,"In the past few decades, the rapid development of information and internet
technologies has spawned massive amounts of data and information. The
information explosion drives many enterprises or individuals to seek to rent
cloud computing infrastructure to put their applications in the cloud. However,
the agreements reached between cloud computing providers and clients are often
not efficient. Many factors affect the efficiency, such as the idleness of the
providers' cloud computing infrastructure, and the additional cost to the
clients. One possible solution is to introduce a comprehensive, bargaining game
(a type of negotiation), and schedule resources according to the negotiation
results. We propose an agent-based auto-negotiation system for resource
scheduling based on fuzzy logic. The proposed method can complete a one-to-one
auto-negotiation process and generate optimal offers for the provider and
client. We compare the impact of different member functions, fuzzy rule sets,
and negotiation scenario cases on the offers to optimize the system. It can be
concluded that our proposed method can utilize resources more efficiently and
is interpretable, highly flexible, and customizable. We successfully train
machine learning models to replace the fuzzy negotiation system to improve
processing speed. The article also highlights possible future improvements to
the proposed system and machine learning models. All the codes and data are
available in the open-source repository.",http://arxiv.org/abs/2402.06938v2,84,17,15,18,17,17
Randomized Algorithms for Symmetric Nonnegative Matrix Factorization,"Symmetric Nonnegative Matrix Factorization (SymNMF) is a technique in data
analysis and machine learning that approximates a symmetric matrix with a
product of a nonnegative, low-rank matrix and its transpose. To design faster
and more scalable algorithms for SymNMF we develop two randomized algorithms
for its computation. The first algorithm uses randomized matrix sketching to
compute an initial low-rank input matrix and proceeds to use this input to
rapidly compute a SymNMF. The second algorithm uses randomized leverage score
sampling to approximately solve constrained least squares problems. Many
successful methods for SymNMF rely on (approximately) solving sequences of
constrained least squares problems. We prove theoretically that leverage score
sampling can approximately solve nonnegative least squares problems to a chosen
accuracy with high probability. Finally we demonstrate that both methods work
well in practice by applying them to graph clustering tasks on large real world
data sets. These experiments show that our methods approximately maintain
solution quality and achieve significant speed ups for both large dense and
large sparse problems.",http://arxiv.org/abs/2402.08134v1,84,18,16,16,18,16
Identifying architectural design decisions for achieving green ML serving,"The growing use of large machine learning models highlights concerns about
their increasing computational demands. While the energy consumption of their
training phase has received attention, fewer works have considered the
inference phase. For ML inference, the binding of ML models to the ML system
for user access, known as ML serving, is a critical yet understudied step for
achieving efficiency in ML applications.
  We examine the literature in ML architectural design decisions and Green AI,
with a special focus on ML serving. The aim is to analyze ML serving
architectural design decisions for the purpose of understanding and identifying
them with respect to quality characteristics from the point of view of
researchers and practitioners in the context of ML serving literature.
  Our results (i) identify ML serving architectural design decisions along with
their corresponding components and associated technological stack, and (ii)
provide an overview of the quality characteristics studied in the literature,
including energy efficiency.
  This preliminary study is the first step in our goal to achieve green ML
serving. Our analysis may aid ML researchers and practitioners in making
green-aware architecture design decisions when serving their models.",http://dx.doi.org/10.1145/3644815.3644962,84,16,18,18,16,16
Low-Resource Counterspeech Generation for Indic Languages: The Case of Bengali and Hindi,"With the rise of online abuse, the NLP community has begun investigating the
use of neural architectures to generate counterspeech that can ""counter"" the
vicious tone of such abusive speech and dilute/ameliorate their rippling effect
over the social network. However, most of the efforts so far have been
primarily focused on English. To bridge the gap for low-resource languages such
as Bengali and Hindi, we create a benchmark dataset of 5,062 abusive
speech/counterspeech pairs, of which 2,460 pairs are in Bengali and 2,602 pairs
are in Hindi. We implement several baseline models considering various
interlingual transfer mechanisms with different configurations to generate
suitable counterspeech to set up an effective benchmark. We observe that the
monolingual setup yields the best performance. Further, using synthetic
transfer, language models can generate counterspeech to some extent;
specifically, we notice that transferability is better when languages belong to
the same language family.",http://arxiv.org/abs/2402.07262v1,84,18,16,16,18,16
Evaluation Metrics for Text Data Augmentation in NLP,"Recent surveys on data augmentation for natural language processing have
reported different techniques and advancements in the field. Several
frameworks, tools, and repositories promote the implementation of text data
augmentation pipelines. However, a lack of evaluation criteria and standards
for method comparison due to different tasks, metrics, datasets, architectures,
and experimental settings makes comparisons meaningless. Also, a lack of
methods unification exists and text data augmentation research would benefit
from unified metrics to compare different augmentation methods. Thus, academics
and the industry endeavor relevant evaluation metrics for text data
augmentation techniques. The contribution of this work is to provide a taxonomy
of evaluation metrics for text augmentation methods and serve as a direction
for a unified benchmark. The proposed taxonomy organizes categories that
include tools for implementation and metrics calculation. Finally, with this
study, we intend to present opportunities to explore the unification and
standardization of text data augmentation metrics.",http://arxiv.org/abs/2402.06766v1,84,18,16,18,16,16
Physics-Informed Neural Networks with Hard Linear Equality Constraints,"Surrogate modeling is used to replace computationally expensive simulations.
Neural networks have been widely applied as surrogate models that enable
efficient evaluations over complex physical systems. Despite this, neural
networks are data-driven models and devoid of any physics. The incorporation of
physics into neural networks can improve generalization and data efficiency.
The physics-informed neural network (PINN) is an approach to leverage known
physical constraints present in the data, but it cannot strictly satisfy them
in the predictions. This work proposes a novel physics-informed neural network,
KKT-hPINN, which rigorously guarantees hard linear equality constraints through
projection layers derived from KKT conditions. Numerical experiments on Aspen
models of a continuous stirred-tank reactor (CSTR) unit, an extractive
distillation subsystem, and a chemical plant demonstrate that this model can
further enhance the prediction accuracy.",http://arxiv.org/abs/2402.07251v1,84,16,18,18,16,16
Explicit References to Social Values in Fairy Tales: A Comparison between Three European Cultures,"The study of social values in fairy tales opens the possibility to learn
about the communication of values across space and time. We propose to study
the communication of values in fairy tales from Portugal, Italy and Germany
using a technique called word embedding with a compass to quantify vocabulary
differences and commonalities. We study how these three national traditions of
fairy tales differ in their explicit references to values. To do this, we
specify a list of value-charged tokens, consider their word stems and analyse
the distance between these in a bespoke pre-trained Word2Vec model. We
triangulate and critically discuss the validity of the resulting hypotheses
emerging from this quantitative model. Our claim is that this is a reusable and
reproducible method for the study of the values explicitly referenced in
historical corpora. Finally, our preliminary findings hint at a shared cultural
understanding and the expression of values such as Benevolence, Conformity, and
Universalism across European societies, suggesting the existence of a
pan-European cultural memory.",http://arxiv.org/abs/2402.08318v1,84,16,18,18,18,14
Towards Generalized Inverse Reinforcement Learning,"This paper studies generalized inverse reinforcement learning (GIRL) in
Markov decision processes (MDPs), that is, the problem of learning the basic
components of an MDP given observed behavior (policy) that might not be
optimal. These components include not only the reward function and transition
probability matrices, but also the action space and state space that are not
exactly known but are known to belong to given uncertainty sets. We address two
key challenges in GIRL: first, the need to quantify the discrepancy between the
observed policy and the underlying optimal policy; second, the difficulty of
mathematically characterizing the underlying optimal policy when the basic
components of an MDP are unobservable or partially observable. Then, we propose
the mathematical formulation for GIRL and develop a fast heuristic algorithm.
Numerical results on both finite and infinite state problems show the merit of
our formulation and algorithm.",http://arxiv.org/abs/2402.07246v1,84,18,16,18,16,16
Multi-Intent Attribute-Aware Text Matching in Searching,"Text matching systems have become a fundamental service in most searching
platforms. For instance, they are responsible for matching user queries to
relevant candidate items, or rewriting the user-input query to a pre-selected
high-performing one for a better search experience. In practice, both the
queries and items often contain multiple attributes, such as the category of
the item and the location mentioned in the query, which represent condensed key
information that is helpful for matching. However, most of the existing works
downplay the effectiveness of attributes by integrating them into text
representations as supplementary information. Hence, in this work, we focus on
exploring the relationship between the attributes from two sides. Since
attributes from two ends are often not aligned in terms of number and type, we
propose to exploit the benefit of attributes by multiple-intent modeling. The
intents extracted from attributes summarize the diverse needs of queries and
provide rich content of items, which are more refined and abstract, and can be
aligned for paired inputs. Concretely, we propose a multi-intent
attribute-aware matching model (MIM), which consists of three main components:
attribute-aware encoder, multi-intent modeling, and intent-aware matching. In
the attribute-aware encoder, the text and attributes are weighted and processed
through a scaled attention mechanism with regard to the attributes' importance.
Afterward, the multi-intent modeling extracts intents from two ends and aligns
them. Herein, we come up with a distribution loss to ensure the learned intents
are diverse but concentrated, and a kullback-leibler divergence loss that
aligns the learned intents. Finally, in the intent-aware matching, the intents
are evaluated by a self-supervised masking task, and then incorporated to
output the final matching result.",http://arxiv.org/abs/2402.07788v1,84,18,17,18,16,15
A Sampling Theory Perspective on Activations for Implicit Neural Representations,"Implicit Neural Representations (INRs) have gained popularity for encoding
signals as compact, differentiable entities. While commonly using techniques
like Fourier positional encodings or non-traditional activation functions
(e.g., Gaussian, sinusoid, or wavelets) to capture high-frequency content,
their properties lack exploration within a unified theoretical framework.
Addressing this gap, we conduct a comprehensive analysis of these activations
from a sampling theory perspective. Our investigation reveals that sinc
activations, previously unused in conjunction with INRs, are theoretically
optimal for signal encoding. Additionally, we establish a connection between
dynamical systems and INRs, leveraging sampling theory to bridge these two
paradigms.",http://arxiv.org/abs/2402.05427v1,84,18,16,16,18,16
Maia: A Real-time Non-Verbal Chat for Human-AI Interaction,"Face-to-face communication modeling in computer vision is an area of research
focusing on developing algorithms that can recognize and analyze non-verbal
cues and behaviors during face-to-face interactions. We propose an alternative
to text chats for Human-AI interaction, based on non-verbal visual
communication only, using facial expressions and head movements that mirror,
but also improvise over the human user, to efficiently engage with the users,
and capture their attention in a low-cost and real-time fashion. Our goal is to
track and analyze facial expressions, and other non-verbal cues in real-time,
and use this information to build models that can predict and understand human
behavior. We offer three different complementary approaches, based on
retrieval, statistical, and deep learning techniques. We provide human as well
as automatic evaluations and discuss the advantages and disadvantages of each
direction.",http://arxiv.org/abs/2402.06385v1,84,18,18,16,16,16
Potential-Based Reward Shaping For Intrinsic Motivation,"Recently there has been a proliferation of intrinsic motivation (IM)
reward-shaping methods to learn in complex and sparse-reward environments.
These methods can often inadvertently change the set of optimal policies in an
environment, leading to suboptimal behavior. Previous work on mitigating the
risks of reward shaping, particularly through potential-based reward shaping
(PBRS), has not been applicable to many IM methods, as they are often complex,
trainable functions themselves, and therefore dependent on a wider set of
variables than the traditional reward functions that PBRS was developed for. We
present an extension to PBRS that we prove preserves the set of optimal
policies under a more general set of functions than has been previously proven.
We also present {\em Potential-Based Intrinsic Motivation} (PBIM), a method for
converting IM rewards into a potential-based form that is useable without
altering the set of optimal policies. Testing in the MiniGrid DoorKey and Cliff
Walking environments, we demonstrate that PBIM successfully prevents the agent
from converging to a suboptimal policy and can speed up training.",http://arxiv.org/abs/2402.07411v1,84,16,18,20,16,14
Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy,"The evaluation of text-generative vision-language models is a challenging yet
crucial endeavor. By addressing the limitations of existing Visual Question
Answering (VQA) benchmarks and proposing innovative evaluation methodologies,
our research seeks to advance our understanding of these models' capabilities.
We propose a novel VQA benchmark based on well-known visual classification
datasets which allows a granular evaluation of text-generative vision-language
models and their comparison with discriminative vision-language models. To
improve the assessment of coarse answers on fine-grained classification tasks,
we suggest using the semantic hierarchy of the label space to ask automatically
generated follow-up questions about the ground-truth category. Finally, we
compare traditional NLP and LLM-based metrics for the problem of evaluating
model predictions given ground-truth answers. We perform a human evaluation
study upon which we base our decision on the final metric. We apply our
benchmark to a suite of vision-language models and show a detailed comparison
of their abilities on object, action, and attribute classification. Our
contributions aim to lay the foundation for more precise and meaningful
assessments, facilitating targeted progress in the exciting field of
vision-language modeling.",http://arxiv.org/abs/2402.07270v1,84,18,17,17,16,16
A Convergence Analysis of Approximate Message Passing with Non-Separable Functions and Applications to Multi-Class Classification,"Motivated by the recent application of approximate message passing (AMP) to
the analysis of convex optimizations in multi-class classifications [Loureiro,
et. al., 2021], we present a convergence analysis of AMP dynamics with
non-separable multivariate nonlinearities. As an application, we present a
complete (and independent) analysis of the motivated convex optimization
problem.",http://arxiv.org/abs/2402.08676v1,84,18,16,18,16,16
Improving the accuracy of freight mode choice models: A case study using the 2017 CFS PUF data set and ensemble learning techniques,"The US Census Bureau has collected two rounds of experimental data from the
Commodity Flow Survey, providing shipment-level characteristics of nationwide
commodity movements, published in 2012 (i.e., Public Use Microdata) and in 2017
(i.e., Public Use File). With this information, data-driven methods have become
increasingly valuable for understanding detailed patterns in freight logistics.
In this study, we used the 2017 Commodity Flow Survey Public Use File data set
to explore building a high-performance freight mode choice model, considering
three main improvements: (1) constructing local models for each separate
commodity/industry category; (2) extracting useful geographical features,
particularly the derived distance of each freight mode between
origin/destination zones; and (3) applying additional ensemble learning methods
such as stacking or voting to combine results from local and unified models for
improved performance. The proposed method achieved over 92% accuracy without
incorporating external information, an over 19% increase compared to directly
fitting Random Forests models over 10,000 samples. Furthermore, SHAP (Shapely
Additive Explanations) values were computed to explain the outputs and major
patterns obtained from the proposed model. The model framework could enhance
the performance and interpretability of existing freight mode choice models.",http://dx.doi.org/10.1016/j.eswa.2023.122478,84,16,18,18,16,16
Are We Asking the Right Questions?: Designing for Community Stakeholders' Interactions with AI in Policing,"Research into recidivism risk prediction in the criminal legal system has
garnered significant attention from HCI, critical algorithm studies, and the
emerging field of human-AI decision-making. This study focuses on algorithmic
crime mapping, a prevalent yet underexplored form of algorithmic decision
support (ADS) in this context. We conducted experiments and follow-up
interviews with 60 participants, including community members, technical
experts, and law enforcement agents (LEAs), to explore how lived experiences,
technical knowledge, and domain expertise shape interactions with the ADS,
impacting human-AI decision-making. Surprisingly, we found that domain experts
(LEAs) often exhibited anchoring bias, readily accepting and engaging with the
first crime map presented to them. Conversely, community members and technical
experts were more inclined to engage with the tool, adjust controls, and
generate different maps. Our findings highlight that all three stakeholders
were able to provide critical feedback regarding AI design and use - community
members questioned the core motivation of the tool, technical experts drew
attention to the elastic nature of data science practice, and LEAs suggested
redesign pathways such that the tool could complement their domain expertise.",http://dx.doi.org/10.1145/3613904.3642738,84,16,18,18,18,14
Automated Design of Affine Maximizer Mechanisms in Dynamic Settings,"Dynamic mechanism design is a challenging extension to ordinary mechanism
design in which the mechanism designer must make a sequence of decisions over
time in the face of possibly untruthful reports of participating agents.
Optimizing dynamic mechanisms for welfare is relatively well understood.
However, there has been less work on optimizing for other goals (e.g. revenue),
and without restrictive assumptions on valuations, it is remarkably challenging
to characterize good mechanisms. Instead, we turn to automated mechanism design
to find mechanisms with good performance in specific problem instances. In
fact, the situation is similar even in static mechanism design. However, in the
static case, optimization/machine learning-based automated mechanism design
techniques have been successful in finding high-revenue mechanisms in cases
beyond the reach of analytical results. We extend the class of affine maximizer
mechanisms to MDPs where agents may untruthfully report their rewards. This
extension results in a challenging bilevel optimization problem in which the
upper problem involves choosing optimal mechanism parameters, and the lower
problem involves solving the resulting MDP. Our approach can find truthful
dynamic mechanisms that achieve strong performance on goals other than welfare,
and can be applied to essentially any problem setting-without restrictions on
valuations-for which RL can learn optimal policies.",http://arxiv.org/abs/2402.08129v1,84,18,16,16,18,16
Guided Sketch-Based Program Induction by Search Gradients,"Many tasks can be easily solved using machine learning techniques. However,
some tasks cannot readily be solved using statistical models, requiring a
symbolic approach instead. Program induction is one of the ways that such tasks
can be solved by means of capturing an interpretable and generalizable
algorithm through training. However, contemporary approaches to program
induction are not sophisticated enough to readily be applied to various types
of tasks as they tend to be formulated as a single, all-encompassing model,
usually parameterized by neural networks. In an attempt to make program
induction a viable solution for many scenarios, we propose a framework for
learning parameterized programs via search gradients using evolution
strategies. This formulation departs from traditional program induction as it
allows for the programmer to impart task-specific code to the program 'sketch',
while also enjoying the benefits of accelerated learning through end-to-end
gradient-based optimization.",http://arxiv.org/abs/2402.06990v1,84,18,18,16,18,14
Reconstructing facade details using MLS point clouds and Bag-of-Words approach,"In the reconstruction of fa\c{c}ade elements, the identification of specific
object types remains challenging and is often circumvented by rectangularity
assumptions or the use of bounding boxes. We propose a new approach for the
reconstruction of 3D fa\c{c}ade details. We combine MLS point clouds and a
pre-defined 3D model library using a BoW concept, which we augment by
incorporating semi-global features. We conduct experiments on the models
superimposed with random noise and on the TUM-FA\c{C}ADE dataset. Our method
demonstrates promising results, improving the conventional BoW approach. It
holds the potential to be utilized for more realistic facade reconstruction
without rectangularity assumptions, which can be used in applications such as
testing automated driving functions or estimating fa\c{c}ade solar potential.",http://arxiv.org/abs/2402.06521v1,84,18,16,16,18,16
High-Rate Fair-Density Parity-Check Codes,"We introduce fair-density parity-check (FDPC) codes targeting high-rate
applications. In particular, we start with a base parity-check matrix $H_b$ of
dimension $2 \sqrt{n} \times n$, where $n$ is the code block length, and the
number of ones in each row and column of $H_b$ is equal to $\sqrt{n}$ and $2$,
respectively. We propose a deterministic combinatorial method for picking the
base matrix $H_b$, assuming $n=4t^2$ for some integer $t \geq 2$. We then
extend this by obtaining permuted versions of $H_b$ (e.g., via random
permutations of its columns) and stacking them on top of each other leading to
codes of dimension $k \geq n-2s\sqrt{n}+s$, for some $s \geq 2$, referred to as
order-$s$ FDPC codes. We propose methods to explicitly characterize and bound
the weight distribution of the new codes and utilize them to derive union-type
approximate upper bounds on their error probability under Maximum Likelihood
(ML) decoding. For the binary erasure channel (BEC), we demonstrate that the
approximate ML bound of FDPC codes closely follows the random coding upper
bound (RCU) for a wide range of channel parameters. Also, remarkably, FDPC
codes, under the low-complexity min-sum decoder, improve upon 5G-LDPC codes for
transmission over the binary-input additive white Gaussian noise (B-AWGN)
channel by almost 0.5dB (for $n=1024$, and rate $=0.878$). Furthermore, we
propose a new decoder as a combination of weighted min-sum message-passing (MP)
decoding algorithm together with a new progressive list (PL) decoding
component, referred to as the MP-PL decoder, to further boost the performance
of FDPC codes.
  This paper opens new avenues for a fresh investigation of new code
constructions and decoding algorithms in high-rate regimes suitable for
ultra-high throughput (high-frequency/optical) applications.",http://arxiv.org/abs/2402.06814v1,84,18,16,18,18,14
System-level Analysis of Adversarial Attacks and Defenses on Intelligence in O-RAN based Cellular Networks,"While the open architecture, open interfaces, and integration of intelligence
within Open Radio Access Network technology hold the promise of transforming 5G
and 6G networks, they also introduce cybersecurity vulnerabilities that hinder
its widespread adoption. In this paper, we conduct a thorough system-level
investigation of cyber threats, with a specific focus on machine learning (ML)
intelligence components known as xApps within the O-RAN's near-real-time RAN
Intelligent Controller (near-RT RIC) platform. Our study begins by developing a
malicious xApp designed to execute adversarial attacks on two types of test
data - spectrograms and key performance metrics (KPMs), stored in the RIC
database within the near-RT RIC. To mitigate these threats, we utilize a
distillation technique that involves training a teacher model at a high softmax
temperature and transferring its knowledge to a student model trained at a
lower softmax temperature, which is deployed as the robust ML model within
xApp. We prototype an over-the-air LTE/5G O-RAN testbed to assess the impact of
these attacks and the effectiveness of the distillation defense technique by
leveraging an ML-based Interference Classification (InterClass) xApp as an
example. We examine two versions of InterClass xApp under distinct scenarios,
one based on Convolutional Neural Networks (CNNs) and another based on Deep
Neural Networks (DNNs) using spectrograms and KPMs as input data respectively.
Our findings reveal up to 100% and 96.3% degradation in the accuracy of both
the CNN and DNN models respectively resulting in a significant decline in
network performance under considered adversarial attacks. Under the strict
latency constraints of the near-RT RIC closed control loop, our analysis shows
that the distillation technique outperforms classical adversarial training by
achieving an accuracy of up to 98.3% for mitigating such attacks.",http://arxiv.org/abs/2402.06846v2,84,16,18,18,16,16
Explain Variance of Prediction in Variational Time Series Models for Clinical Deterioration Prediction,"In healthcare, thanks to many model agnostic methods, explainability of the
prediction scores made by deep learning applications has improved. However, we
note that for daily or hourly risk of deterioration prediction of in-hospital
patients, not only the predicted risk probability score matters, but also the
variance of the risk scores play key roles in aiding clinical decision making.
In this paper, we propose to use delta's method to approximate variance of
prediction deterministically, such that the SHAP method can be adopted to
attribute contribution of variance. The prediction variance is estimated by
sampling the conditional hidden space in variational models and is propagated
to input clinical variables based on Shapley values of the variance game. This
approach works with variational time series models such as variational
recurrent neural networks and variational transformers. We further argue that
variational time series models are perfect fits for achieving a balance between
predictive power and explainability through a series of experiments on a public
clinical ICU datasets. Since SHAP values are additive, we also postulate that
the SHAP importance of clinical variables with respect to prediction variations
can guide their frequency of measurements.",http://arxiv.org/abs/2402.06808v1,84,16,18,18,16,16
Empowering Federated Learning for Massive Models with NVIDIA FLARE,"In the ever-evolving landscape of artificial intelligence (AI) and large
language models (LLMs), handling and leveraging data effectively has become a
critical challenge. Most state-of-the-art machine learning algorithms are
data-centric. However, as the lifeblood of model performance, necessary data
cannot always be centralized due to various factors such as privacy,
regulation, geopolitics, copyright issues, and the sheer effort required to
move vast datasets. In this paper, we explore how federated learning enabled by
NVIDIA FLARE can address these challenges with easy and scalable integration
capabilities, enabling parameter-efficient and full supervised fine-tuning of
LLMs for natural language processing and biopharmaceutical applications to
enhance their accuracy and robustness.",http://arxiv.org/abs/2402.07792v1,84,16,18,18,16,16
Efficient Contextual Bandits with Uninformed Feedback Graphs,"Bandits with feedback graphs are powerful online learning models that
interpolate between the full information and classic bandit problems, capturing
many real-life applications. A recent work by Zhang et al. (2023) studies the
contextual version of this problem and proposes an efficient and optimal
algorithm via a reduction to online regression. However, their algorithm
crucially relies on seeing the feedback graph before making each decision,
while in many applications, the feedback graph is uninformed, meaning that it
is either only revealed after the learner makes her decision or even never
fully revealed at all. This work develops the first contextual algorithm for
such uninformed settings, via an efficient reduction to online regression over
both the losses and the graphs. Importantly, we show that it is critical to
learn the graphs using log loss instead of squared loss to obtain favorable
regret guarantees. We also demonstrate the empirical effectiveness of our
algorithm on a bidding application using both synthetic and real-world data.",http://arxiv.org/abs/2402.08127v1,84,16,19,19,15,15
Towards Principled Assessment of Tabular Data Synthesis Algorithms,"Data synthesis has been advocated as an important approach for utilizing data
while protecting data privacy. A large number of tabular data synthesis
algorithms (which we call synthesizers) have been proposed. Some synthesizers
satisfy Differential Privacy, while others aim to provide privacy in a
heuristic fashion. A comprehensive understanding of the strengths and
weaknesses of these synthesizers remains elusive due to lacking principled
evaluation metrics and missing head-to-head comparisons of newly developed
synthesizers that take advantage of diffusion models and large language models
with state-of-the-art marginal-based synthesizers.
  In this paper, we present a principled and systematic evaluation framework
for assessing tabular data synthesis algorithms. Specifically, we examine and
critique existing evaluation metrics, and introduce a set of new metrics in
terms of fidelity, privacy, and utility to address their limitations. Based on
the proposed metrics, we also devise a unified objective for tuning, which can
consistently improve the quality of synthetic data for all methods. We
conducted extensive evaluations of 8 different types of synthesizers on 12
datasets and identified some interesting findings, which offer new directions
for privacy-preserving data synthesis.",http://arxiv.org/abs/2402.06806v1,84,16,18,18,18,14
Insights into Natural Language Database Query Errors: From Attention Misalignment to User Handling Strategies,"Querying structured databases with natural language (NL2SQL) has remained a
difficult problem for years. Recently, the advancement of machine learning
(ML), natural language processing (NLP), and large language models (LLM) have
led to significant improvements in performance, with the best model achieving
~85% percent accuracy on the benchmark Spider dataset. However, there is a lack
of a systematic understanding of the types, causes, and effectiveness of
error-handling mechanisms of errors for erroneous queries nowadays. To bridge
the gap, a taxonomy of errors made by four representative NL2SQL models was
built in this work, along with an in-depth analysis of the errors. Second, the
causes of model errors were explored by analyzing the model-human attention
alignment to the natural language query. Last, a within-subjects user study
with 26 participants was conducted to investigate the effectiveness of three
interactive error-handling mechanisms in NL2SQL. Findings from this paper shed
light on the design of model structure and error discovery and repair
strategies for natural language data query interfaces in the future.",http://arxiv.org/abs/2402.07304v1,84,18,16,18,16,16
Exploring Perceptual Limitation of Multimodal Large Language Models,"Multimodal Large Language Models (MLLMs) have recently shown remarkable
perceptual capability in answering visual questions, however, little is known
about the limits of their perception. In particular, while prior works have
provided anecdotal evidence of MLLMs' sensitivity to object size, this
phenomenon and its underlying causes have not been explored comprehensively. In
this work, we quantitatively study the perception of small visual objects in
several state-of-the-art MLLMs and reveal a pervasive limitation in answering
questions about small objects in images. Next, we identify four independent
factors that can contribute to this limitation -- object quality, size,
distractors, and location -- and conduct controlled intervention studies to
measure the effect of each factor on MLLMs' perception. In particular, we find
that lower object quality and smaller object size can both independently reduce
MLLMs' ability to answer visual questions. More surprisingly, we find that the
location of the object in the image and the presence of visual distractors can
also significantly reduce MLLMs' question answering accuracy. Our study
provides a better understanding of the perceptual limitation of MLLMs and
contributes new evaluation protocols for analyzing the perception of future
MLLMs. To facilitate further investigations, we release our code and data.",http://arxiv.org/abs/2402.07384v1,84,18,17,17,16,16
Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey,"Knowledge Graphs (KGs) play a pivotal role in advancing various AI
applications, with the semantic web community's exploration into multi-modal
dimensions unlocking new avenues for innovation. In this survey, we carefully
review over 300 articles, focusing on KG-aware research in two principal
aspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal
tasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into
the MMKG realm. We begin by defining KGs and MMKGs, then explore their
construction progress. Our review includes two primary task categories:
KG-aware multi-modal learning tasks, such as Image Classification and Visual
Question Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph
Completion and Entity Alignment, highlighting specific research trajectories.
For most of these tasks, we provide definitions, evaluation benchmarks, and
additionally outline essential insights for conducting relevant research.
Finally, we discuss current challenges and identify emerging trends, such as
progress in Large Language Modeling and Multi-modal Pre-training strategies.
This survey aims to serve as a comprehensive reference for researchers already
involved in or considering delving into KG and multi-modal learning research,
offering insights into the evolving landscape of MMKG research and supporting
future work.",http://arxiv.org/abs/2402.05391v2,84,18,16,16,18,16
Learning-augmented Online Algorithm for Two-level Ski-rental Problem,"In this paper, we study the two-level ski-rental problem,where a user needs
to fulfill a sequence of demands for multiple items by choosing one of the
three payment options: paying for the on-demand usage (i.e., rent), buying
individual items (i.e., single purchase), and buying all the items (i.e., combo
purchase). Without knowing future demands, the user aims to minimize the total
cost (i.e., the sum of the rental, single purchase, and combo purchase costs)
by balancing the trade-off between the expensive upfront costs (for purchase)
and the potential future expenses (for rent). We first design a robust online
algorithm (RDTSR) that offers a worst-case performance guarantee. While online
algorithms are robust against the worst-case scenarios, they are often overly
cautious and thus suffer a poor average performance in typical scenarios. On
the other hand, Machine Learning (ML) algorithms typically show promising
average performance in various applications but lack worst-case performance
guarantees. To harness the benefits of both methods, we develop a
learning-augmented algorithm (LADTSR) by integrating ML predictions into the
robust online algorithm, which outperforms the robust online algorithm under
accurate predictions while ensuring worst-case performance guarantees even when
predictions are inaccurate. Finally, we conduct numerical experiments on both
synthetic and real-world trace data to corroborate the effectiveness of our
approach.",http://arxiv.org/abs/2402.06715v1,84,16,18,18,18,14
Transferring facade labels between point clouds with semantic octrees while considering change detection,"Point clouds and high-resolution 3D data have become increasingly important
in various fields, including surveying, construction, and virtual reality.
However, simply having this data is not enough; to extract useful information,
semantic labeling is crucial. In this context, we propose a method to transfer
annotations from a labeled to an unlabeled point cloud using an octree
structure. The structure also analyses changes between the point clouds. Our
experiments confirm that our method effectively transfers annotations while
addressing changes. The primary contribution of this project is the development
of the method for automatic label transfer between two different point clouds
that represent the same real-world object. The proposed method can be of great
importance for data-driven deep learning algorithms as it can also allow
circumventing stochastic transfer learning by deterministic label transfer
between datasets depicting the same objects.",http://arxiv.org/abs/2402.06531v1,84,16,18,16,18,16
On Calibration and Conformal Prediction of Deep Classifiers,"In many classification applications, the prediction of a deep neural network
(DNN) based classifier needs to be accompanied with some confidence indication.
Two popular post-processing approaches for that aim are: 1) calibration:
modifying the classifier's softmax values such that their maximum (associated
with the prediction) better estimates the correctness probability; and 2)
conformal prediction (CP): devising a score (based on the softmax values) from
which a set of predictions with theoretically guaranteed marginal coverage of
the correct class is produced. While in practice both types of indications can
be desired, so far the interplay between them has not been investigated. Toward
filling this gap, in this paper we study the effect of temperature scaling,
arguably the most common calibration technique, on prominent CP methods. We
start with an extensive empirical study that among other insights shows that,
surprisingly, calibration has a detrimental effect on popular adaptive CP
methods: it frequently leads to larger prediction sets. Then, we turn to
theoretically analyze this behavior. We reveal several mathematical properties
of the procedure, according to which we provide a reasoning for the phenomenon.
Our study suggests that it may be worthwhile to utilize adaptive CP methods,
chosen for their enhanced conditional coverage, based on softmax values prior
to (or after canceling) temperature scaling calibration.",http://arxiv.org/abs/2402.05806v1,84,16,18,16,18,16
Tradeoffs of Diagonal Fisher Information Matrix Estimators,"The Fisher information matrix characterizes the local geometry in the
parameter space of neural networks. It elucidates insightful theories and
useful tools to understand and optimize neural networks. Given its high
computational cost, practitioners often use random estimators and evaluate only
the diagonal entries. We examine two such estimators, whose accuracy and sample
complexity depend on their associated variances. We derive bounds of the
variances and instantiate them in regression and classification networks. We
navigate trade-offs of both estimators based on analytical and numerical
studies. We find that the variance quantities depend on the non-linearity with
respect to different parameter groups and should not be neglected when
estimating the Fisher information.",http://arxiv.org/abs/2402.05379v1,84,16,18,16,18,16
Fairness Auditing with Multi-Agent Collaboration,"Existing work in fairness audits assumes that agents operate independently.
In this paper, we consider the case of multiple agents auditing the same
platform for different tasks. Agents have two levers: their collaboration
strategy, with or without coordination beforehand, and their sampling method.
We theoretically study their interplay when agents operate independently or
collaborate. We prove that, surprisingly, coordination can sometimes be
detrimental to audit accuracy, whereas uncoordinated collaboration generally
yields good results. Experimentation on real-world datasets confirms this
observation, as the audit accuracy of uncoordinated collaboration matches that
of collaborative optimal sampling.",http://arxiv.org/abs/2402.08522v1,84,16,18,18,18,14
Principled Preferential Bayesian Optimization,"We study the problem of preferential Bayesian optimization (BO), where we aim
to optimize a black-box function with only preference feedback over a pair of
candidate solutions. Inspired by the likelihood ratio idea, we construct a
confidence set of the black-box function using only the preference feedback. An
optimistic algorithm with an efficient computational method is then developed
to solve the problem, which enjoys an information-theoretic bound on the
cumulative regret, a first-of-its-kind for preferential BO. This bound further
allows us to design a scheme to report an estimated best solution, with a
guaranteed convergence rate. Experimental results on sampled instances from
Gaussian processes, standard test functions, and a thermal comfort optimization
problem all show that our method stably achieves better or competitive
performance as compared to the existing state-of-the-art heuristics, which,
however, do not have theoretical guarantees on regret bounds or convergence.",http://arxiv.org/abs/2402.05367v1,84,16,16,18,18,16
Stable Autonomous Flow Matching,"In contexts where data samples represent a physically stable state, it is
often assumed that the data points represent the local minima of an energy
landscape. In control theory, it is well-known that energy can serve as an
effective Lyapunov function. Despite this, connections between control theory
and generative models in the literature are sparse, even though there are
several machine learning applications with physically stable data points. In
this paper, we focus on such data and a recent class of deep generative models
called flow matching. We apply tools of stochastic stability for
time-independent systems to flow matching models. In doing so, we characterize
the space of flow matching models that are amenable to this treatment, as well
as draw connections to other control theory principles. We demonstrate our
theoretical results on two examples.",http://arxiv.org/abs/2402.05774v1,84,16,18,18,18,14
"Leveraging AI to Advance Science and Computing Education across Africa: Progress, Challenges, and Opportunities","Across the African continent, students grapple with various educational
challenges, including limited access to essential resources such as computers,
internet connectivity, reliable electricity, and a shortage of qualified
teachers. Despite these challenges, recent advances in AI such as BERT, and
GPT-4 have demonstrated their potential for advancing education. Yet, these AI
tools tend to be deployed and evaluated predominantly within the context of
Western educational settings, with limited attention directed towards the
unique needs and challenges faced by students in Africa. In this book chapter,
we describe our works developing and deploying AI in Education tools in Africa:
(1) SuaCode, an AI-powered app that enables Africans to learn to code using
their smartphones, (2) AutoGrad, an automated grading, and feedback tool for
graphical and interactive coding assignments, (3) a tool for code plagiarism
detection that shows visual evidence of plagiarism, (4) Kwame, a bilingual AI
teaching assistant for coding courses, (5) Kwame for Science, a web-based AI
teaching assistant that provides instant answers to students' science questions
and (6) Brilla AI, an AI contestant for the National Science and Maths Quiz
competition. We discuss challenges and potential opportunities to use AI to
advance science and computing education across Africa.",http://arxiv.org/abs/2402.07397v1,84,19,18,17,16,14
Exploring Learning Complexity for Downstream Data Pruning,"The over-parameterized pre-trained models pose a great challenge to
fine-tuning with limited computation resources. An intuitive solution is to
prune the less informative samples from the fine-tuning dataset. A series of
training-based scoring functions are proposed to quantify the informativeness
of the data subset but the pruning cost becomes non-negligible due to the heavy
parameter updating. For efficient pruning, it is viable to adapt the similarity
scoring function of geometric-based methods from training-based to
training-free. However, we empirically show that such adaption distorts the
original pruning and results in inferior performance on the downstream tasks.
In this paper, we propose to treat the learning complexity (LC) as the scoring
function for classification and regression tasks. Specifically, the learning
complexity is defined as the average predicted confidence of subnets with
different capacities, which encapsulates data processing within a converged
model. Then we preserve the diverse and easy samples for fine-tuning. Extensive
experiments with vision datasets demonstrate the effectiveness and efficiency
of the proposed scoring function for classification tasks. For the instruction
fine-tuning of large language models, our method achieves state-of-the-art
performance with stable convergence, outperforming the full training with only
10\% of the instruction dataset.",http://arxiv.org/abs/2402.05356v1,84,16,18,18,16,16
Text-to-Code Generation with Modality-relative Pre-training,"Large pre-trained language models have recently been expanded and applied to
programming language tasks with great success, often through further
pre-training of a strictly-natural language model--where training sequences
typically contain both natural and (linearised) programming language. Such
approaches effectively map both modalities of the sequence into the same
embedding space. However, programming language keywords (e.g. ""while"") often
have very strictly defined semantics. As such, transfer learning from their
natural language usage may not necessarily be beneficial to their code
application and vise versa. Assuming an already pre-trained language model, in
this work we investigate how sequence tokens can be adapted and represented
differently, depending on which modality they belong to, and to the ultimate
benefit of the downstream task. We experiment with separating embedding spaces
between modalities during further model pre-training with modality-relative
training objectives. We focus on text-to-code generation and observe consistent
improvements across two backbone models and two test sets, measuring pass@$k$
and a novel incremental variation.",http://arxiv.org/abs/2402.05783v2,84,16,18,18,16,16
CLIPPER: Robust Data Association without an Initial Guess,"Identifying correspondences in noisy data is a critically important step in
estimation processes. When an informative initial estimation guess is
available, the data association challenge is less acute; however, the existence
of a high-quality initial guess is rare in most contexts. We explore
graph-theoretic formulations for data association, which do not require an
initial estimation guess. Existing graph-theoretic approaches optimize over
unweighted graphs, discarding important consistency information encoded in
weighted edges, and frequently attempt to solve NP-hard problems exactly. In
contrast, we formulate a new optimization problem that fully leverages weighted
graphs and seeks the densest edge-weighted clique. We introduce two relaxations
to this problem: a convex semidefinite relaxation which we find to be
empirically tight, and a fast first-order algorithm called CLIPPER which
frequently arrives at nearly-optimal solutions in milliseconds. When evaluated
on point cloud registration problems, our algorithms remain robust up to at
least 95% outliers while existing algorithms begin breaking down at 80%
outliers. Code is available at https://mit-acl.github.io/clipper.",http://dx.doi.org/10.1109/LRA.2024.3364842,84,18,16,16,18,16
Improving Pallet Detection Using Synthetic Data,"The use of synthetic data in machine learning saves a significant amount of
time when implementing an effective object detector. However, there is limited
research in this domain. This study aims to improve upon previously applied
implementations in the task of instance segmentation of pallets in a warehouse
environment. This study proposes using synthetically generated
domain-randomised data as well as data generated through Unity to achieve this.
This study achieved performance improvements on the stacked and racked pallet
categories by 69% and 50% mAP50, respectively when being evaluated on real
data. Additionally, it was found that there was a considerable impact on the
performance of a model when it was evaluated against images in a darker
environment, dropping as low as 3% mAP50 when being evaluated on images with an
80% brightness reduction. This study also created a two-stage detector that
used YOLOv8 and SAM, but this proved to have unstable performance. The use of
domain-randomised data proved to have negligible performance improvements when
compared to the Unity-generated data.",http://arxiv.org/abs/2402.07098v1,84,18,16,18,16,16
Concept-1K: A Novel Benchmark for Instance Incremental Learning,"Incremental learning (IL) is essential to realize the human-level
intelligence in the neural network. However, existing IL scenarios and datasets
are unqualified for assessing forgetting in PLMs, giving an illusion that PLMs
do not suffer from catastrophic forgetting. To this end, we propose a
challenging IL scenario called instance-incremental learning (IIL) and a novel
dataset called Concept-1K, which supports an order of magnitude larger IL
steps. Based on the experiments on Concept-1K, we reveal that billion-parameter
PLMs still suffer from catastrophic forgetting, and the forgetting is affected
by both model scale, pretraining, and buffer size. Furthermore, existing IL
methods and a popular finetuning technique, LoRA, fail to achieve satisfactory
performance. Our study provides a novel scenario for future studies to explore
the catastrophic forgetting of PLMs and encourage more powerful techniques to
be designed for alleviating the forgetting in PLMs. The data, code and scripts
are publicly available at
https://github.com/zzz47zzz/pretrained-lm-for-incremental-learning.",http://arxiv.org/abs/2402.08526v1,84,18,18,16,16,16
"""When He Feels Cold, He Goes to the Seahorse""-Blending Generative AI into Multimaterial Storymaking for Family Expressive Arts Therapy","Storymaking, as an integrative form of expressive arts therapy, is an
effective means to foster family communication. Yet, the integration of
generative AI as expressive materials in therapeutic storymaking remains
underexplored. And there is a lack of HCI implications on how to support
families and therapists in this context. Addressing this, our study involved
five weeks of storymaking sessions with seven families guided by a professional
therapist. In these sessions, the families used both traditional art-making
materials and image-based generative AI to create and evolve their family
stories. Via the rich empirical data and commentaries from four expert
therapists, we contextualize how families creatively melded AI and traditional
expressive materials to externalize their ideas and feelings. Through the lens
of Expressive Therapies Continuum (ETC), we characterize the therapeutic
implications of AI as expressive materials. Desirable interaction qualities to
support children, parents, and therapists are distilled for future HCI
research.",http://dx.doi.org/10.1145/3613904.3642852,84,16,18,18,16,16
Grounding Data Science Code Generation with Input-Output Specifications,"Large language models (LLMs) have recently demonstrated a remarkable ability
to generate code from natural language (NL) prompts. However, in the real
world, NL is often too ambiguous to capture the true intent behind programming
problems, requiring additional input-output (I/O) specifications.
Unfortunately, LLMs can have difficulty aligning their outputs with both the NL
prompt and the I/O specification. In this paper, we give a way to mitigate this
issue in the context of data science programming, where tasks require explicit
I/O specifications for clarity. Specifically, we propose GIFT4Code, a novel
approach for the instruction fine-tuning of LLMs with respect to I/O
specifications. Our method leverages synthetic data produced by the LLM itself
and utilizes execution-derived feedback as a key learning signal. This
feedback, in the form of program I/O specifications, is provided to the LLM to
facilitate instruction fine-tuning. We evaluated our approach on two
challenging data science benchmarks, Arcade and DS-1000. The results
demonstrate a significant improvement in the LLM's ability to generate code
that is not only executable but also accurately aligned with user
specifications, substantially improving the quality of code generation for
complex data science tasks.",http://arxiv.org/abs/2402.08073v1,84,18,18,16,16,16
Comparison of machine learning and statistical approaches for digital elevation model (DEM) correction: interim results,"Several methods have been proposed for correcting the elevation bias in
digital elevation models (DEMs) for example, linear regression. Nowadays,
supervised machine learning enables the modelling of complex relationships
between variables, and has been deployed by researchers in a variety of fields.
In the existing literature, several studies have adopted either machine
learning or statistical approaches in the task of DEM correction. However, to
our knowledge, none of these studies have compared the performance of both
approaches, especially with regard to open-access global DEMs. Our previous
work has already shown the potential of machine learning approaches,
specifically gradient boosted decision trees (GBDTs) for DEM correction. In
this study, we share some results from the comparison of three recent
implementations of gradient boosted decision trees (XGBoost, LightGBM and
CatBoost), versus multiple linear regression (MLR) for enhancing the vertical
accuracy of 30 m Copernicus and AW3D global DEMs in Cape Town, South Africa.",http://arxiv.org/abs/2402.06688v1,84,16,18,16,18,16
An Order-Complexity Aesthetic Assessment Model for Aesthetic-aware Music Recommendation,"Computational aesthetic evaluation has made remarkable contribution to visual
art works, but its application to music is still rare. Currently, subjective
evaluation is still the most effective form of evaluating artistic works.
However, subjective evaluation of artistic works will consume a lot of human
and material resources. The popular AI generated content (AIGC) tasks nowadays
have flooded all industries, and music is no exception. While compared to music
produced by humans, AI generated music still sounds mechanical, monotonous, and
lacks aesthetic appeal. Due to the lack of music datasets with rating
annotations, we have to choose traditional aesthetic equations to objectively
measure the beauty of music. In order to improve the quality of AI music
generation and further guide computer music production, synthesis,
recommendation and other tasks, we use Birkhoff's aesthetic measure to design a
aesthetic model, objectively measuring the aesthetic beauty of music, and form
a recommendation list according to the aesthetic feeling of music. Experiments
show that our objective aesthetic model and recommendation method are
effective.",http://arxiv.org/abs/2402.08300v1,84,16,18,18,16,16
Rethinking Graph Masked Autoencoders through Alignment and Uniformity,"Self-supervised learning on graphs can be bifurcated into contrastive and
generative methods. Contrastive methods, also known as graph contrastive
learning (GCL), have dominated graph self-supervised learning in the past few
years, but the recent advent of graph masked autoencoder (GraphMAE) rekindles
the momentum behind generative methods. Despite the empirical success of
GraphMAE, there is still a dearth of theoretical understanding regarding its
efficacy. Moreover, while both generative and contrastive methods have been
shown to be effective, their connections and differences have yet to be
thoroughly investigated. Therefore, we theoretically build a bridge between
GraphMAE and GCL, and prove that the node-level reconstruction objective in
GraphMAE implicitly performs context-level GCL. Based on our theoretical
analysis, we further identify the limitations of the GraphMAE from the
perspectives of alignment and uniformity, which have been considered as two key
properties of high-quality representations in GCL. We point out that GraphMAE's
alignment performance is restricted by the masking strategy, and the uniformity
is not strictly guaranteed. To remedy the aforementioned limitations, we
propose an Alignment-Uniformity enhanced Graph Masked AutoEncoder, named
AUG-MAE. Specifically, we propose an easy-to-hard adversarial masking strategy
to provide hard-to-align samples, which improves the alignment performance.
Meanwhile, we introduce an explicit uniformity regularizer to ensure the
uniformity of the learned representations. Experimental results on benchmark
datasets demonstrate the superiority of our model over existing
state-of-the-art methods.",http://arxiv.org/abs/2402.07225v1,84,16,18,18,18,14
"Multi-Modal Emotion Recognition by Text, Speech and Video Using Pretrained Transformers","Due to the complex nature of human emotions and the diversity of emotion
representation methods in humans, emotion recognition is a challenging field.
In this research, three input modalities, namely text, audio (speech), and
video, are employed to generate multimodal feature vectors. For generating
features for each of these modalities, pre-trained Transformer models with
fine-tuning are utilized. In each modality, a Transformer model is used with
transfer learning to extract feature and emotional structure. These features
are then fused together, and emotion recognition is performed using a
classifier. To select an appropriate fusion method and classifier, various
feature-level and decision-level fusion techniques have been experimented with,
and ultimately, the best model, which combines feature-level fusion by
concatenating feature vectors and classification using a Support Vector Machine
on the IEMOCAP multimodal dataset, achieves an accuracy of 75.42%. Keywords:
Multimodal Emotion Recognition, IEMOCAP, Self-Supervised Learning, Transfer
Learning, Transformer.",http://arxiv.org/abs/2402.07327v1,84,18,16,18,16,16
Mixture Density Networks for Classification with an Application to Product Bundling,"While mixture density networks (MDNs) have been extensively used for
regression tasks, they have not been used much for classification tasks. One
reason for this is that the usability of MDNs for classification is not clear
and straightforward. In this paper, we propose two MDN-based models for
classification tasks. Both models fit mixtures of Gaussians to the the data and
use the fitted distributions to classify a given sample by evaluating the
learnt cumulative distribution function for the given input features. While the
proposed MDN-based models perform slightly better than, or on par with, five
baseline classification models on three publicly available datasets, the real
utility of our models comes out through a real-world product bundling
application. Specifically, we use our MDN-based models to learn the
willingness-to-pay (WTP) distributions for two products from synthetic sales
data of the individual products. The Gaussian mixture representation of the
learnt WTP distributions is then exploited to obtain the WTP distribution of
the bundle consisting of both the products. The proposed MDN-based models are
able to approximate the true WTP distributions of both products and the bundle
well.",http://arxiv.org/abs/2402.05428v1,84,16,15,18,17,18
The Implicit Bias of Gradient Noise: A Symmetry Perspective,"We characterize the learning dynamics of stochastic gradient descent (SGD)
when continuous symmetry exists in the loss function, where the divergence
between SGD and gradient descent is dramatic. We show that depending on how the
symmetry affects the learning dynamics, we can divide a family of symmetry into
two classes. For one class of symmetry, SGD naturally converges to solutions
that have a balanced and aligned gradient noise. For the other class of
symmetry, SGD will almost always diverge. Then, we show that our result remains
applicable and can help us understand the training dynamics even when the
symmetry is not present in the loss function. Our main result is universal in
the sense that it only depends on the existence of the symmetry and is
independent of the details of the loss function. We demonstrate that the
proposed theory offers an explanation of progressive sharpening and flattening
and can be applied to common practical problems such as representation
normalization, matrix factorization, and the use of warmup.",http://arxiv.org/abs/2402.07193v1,84,18,18,16,16,16
$L^*LM$: Learning Automata from Examples using Natural Language Oracles,"Expert demonstrations have proven an easy way to indirectly specify complex
tasks. Recent algorithms even support extracting unambiguous formal
specifications, e.g. deterministic finite automata (DFA), from demonstrations.
Unfortunately, these techniques are generally not sample efficient. In this
work, we introduce $L^*LM$, an algorithm for learning DFAs from both
demonstrations and natural language. Due to the expressivity of natural
language, we observe a significant improvement in the data efficiency of
learning DFAs from expert demonstrations. Technically, $L^*LM$ leverages large
language models to answer membership queries about the underlying task. This is
then combined with recent techniques for transforming learning from
demonstrations into a sequence of labeled example learning problems. In our
experiments, we observe the two modalities complement each other, yielding a
powerful few-shot learner.",http://arxiv.org/abs/2402.07051v1,84,18,16,16,18,16
Improving Image Coding for Machines through Optimizing Encoder via Auxiliary Loss,"Image coding for machines (ICM) aims to compress images for machine analysis
using recognition models rather than human vision. Hence, in ICM, it is
important for the encoder to recognize and compress the information necessary
for the machine recognition task. There are two main approaches in learned ICM;
optimization of the compression model based on task loss, and Region of
Interest (ROI) based bit allocation. These approaches provide the encoder with
the recognition capability. However, optimization with task loss becomes
difficult when the recognition model is deep, and ROI-based methods often
involve extra overhead during evaluation. In this study, we propose a novel
training method for learned ICM models that applies auxiliary loss to the
encoder to improve its recognition capability and rate-distortion performance.
Our method achieves Bjontegaard Delta rate improvements of 27.7% and 20.3% in
object detection and semantic segmentation tasks, compared to the conventional
training method.",http://arxiv.org/abs/2402.08267v1,84,16,18,18,16,16
Probabilistic Forecasting of Irregular Time Series via Conditional Flows,"Probabilistic forecasting of irregularly sampled multivariate time series
with missing values is an important problem in many fields, including health
care, astronomy, and climate. State-of-the-art methods for the task estimate
only marginal distributions of observations in single channels and at single
timepoints, assuming a fixed-shape parametric distribution. In this work, we
propose a novel model, ProFITi, for probabilistic forecasting of irregularly
sampled time series with missing values using conditional normalizing flows.
The model learns joint distributions over the future values of the time series
conditioned on past observations and queried channels and times, without
assuming any fixed shape of the underlying distribution. As model components,
we introduce a novel invertible triangular attention layer and an invertible
non-linear activation function on and onto the whole real line. We conduct
extensive experiments on four datasets and demonstrate that the proposed model
provides $4$ times higher likelihood over the previously best model.",http://arxiv.org/abs/2402.06293v1,84,18,16,18,16,16
Evaluating Membership Inference Attacks and Defenses in Federated Learning,"Membership Inference Attacks (MIAs) pose a growing threat to privacy
preservation in federated learning. The semi-honest attacker, e.g., the server,
may determine whether a particular sample belongs to a target client according
to the observed model information. This paper conducts an evaluation of
existing MIAs and corresponding defense strategies. Our evaluation on MIAs
reveals two important findings about the trend of MIAs. Firstly, combining
model information from multiple communication rounds (Multi-temporal) enhances
the overall effectiveness of MIAs compared to utilizing model information from
a single epoch. Secondly, incorporating models from non-target clients
(Multi-spatial) significantly improves the effectiveness of MIAs, particularly
when the clients' data is homogeneous. This highlights the importance of
considering the temporal and spatial model information in MIAs. Next, we assess
the effectiveness via privacy-utility tradeoff for two type defense mechanisms
against MIAs: Gradient Perturbation and Data Replacement. Our results
demonstrate that Data Replacement mechanisms achieve a more optimal balance
between preserving privacy and maintaining model utility. Therefore, we
recommend the adoption of Data Replacement methods as a defense strategy
against MIAs. Our code is available in https://github.com/Liar-Mask/FedMIA.",http://arxiv.org/abs/2402.06289v1,84,16,18,18,16,16
Traditional Machine Learning Models and Bidirectional Encoder Representations From Transformer (BERT)-Based Automatic Classification of Tweets About Eating Disorders: Algorithm Development and Validation Study,"Background: Eating disorders are increasingly prevalent, and social networks
offer valuable information.
  Objective: Our goal was to identify efficient machine learning models for
categorizing tweets related to eating disorders.
  Methods: Over three months, we collected tweets about eating disorders. A
2,000-tweet subset was labeled for: (1) being written by individuals with
eating disorders, (2) promoting eating disorders, (3) informativeness, and (4)
scientific content. Both traditional machine learning and deep learning models
were employed for classification, assessing accuracy, F1 score, and
computational time.
  Results: From 1,058,957 collected tweets, transformer-based bidirectional
encoder representations achieved the highest F1 scores (71.1%-86.4%) across all
four categories.
  Conclusions: Transformer-based models outperform traditional techniques in
classifying eating disorder-related tweets, though they require more
computational resources.",http://dx.doi.org/10.2196/34492,84,16,18,18,16,16
T-RAG: Lessons from the LLM Trenches,"Large Language Models (LLM) have shown remarkable language capabilities
fueling attempts to integrate them into applications across a wide range of
domains. An important application area is question answering over private
enterprise documents where the main considerations are data security, which
necessitates applications that can be deployed on-prem, limited computational
resources and the need for a robust application that correctly responds to
queries. Retrieval-Augmented Generation (RAG) has emerged as the most prominent
framework for building LLM-based applications. While building a RAG is
relatively straightforward, making it robust and a reliable application
requires extensive customization and relatively deep knowledge of the
application domain. We share our experiences building and deploying an LLM
application for question answering over private organizational documents. Our
application combines the use of RAG with a finetuned open-source LLM.
Additionally, our system, which we call Tree-RAG (T-RAG), uses a tree structure
to represent entity hierarchies within the organization. This is used to
generate a textual description to augment the context when responding to user
queries pertaining to entities within the organization's hierarchy. Our
evaluations show that this combination performs better than a simple RAG or
finetuning implementation. Finally, we share some lessons learned based on our
experiences building an LLM application for real-world use.",http://arxiv.org/abs/2402.07483v1,84,16,18,18,16,16
"AI, Meet Human: Learning Paradigms for Hybrid Decision Making Systems","Everyday we increasingly rely on machine learning models to automate and
support high-stake tasks and decisions. This growing presence means that humans
are now constantly interacting with machine learning-based systems, training
and using models everyday. Several different techniques in computer science
literature account for the human interaction with machine learning systems, but
their classification is sparse and the goals varied. This survey proposes a
taxonomy of Hybrid Decision Making Systems, providing both a conceptual and
technical framework for understanding how current computer science literature
models interaction between humans and machines.",http://arxiv.org/abs/2402.06287v1,84,18,17,17,16,16
Investigating the Impact of Data Contamination of Large Language Models in Text-to-SQL Translation,"Understanding textual description to generate code seems to be an achieved
capability of instruction-following Large Language Models (LLMs) in zero-shot
scenario. However, there is a severe possibility that this translation ability
may be influenced by having seen target textual descriptions and the related
code. This effect is known as Data Contamination.
  In this study, we investigate the impact of Data Contamination on the
performance of GPT-3.5 in the Text-to-SQL code-generating tasks. Hence, we
introduce a novel method to detect Data Contamination in GPTs and examine
GPT-3.5's Text-to-SQL performances using the known Spider Dataset and our new
unfamiliar dataset Termite. Furthermore, we analyze GPT-3.5's efficacy on
databases with modified information via an adversarial table disconnection
(ATD) approach, complicating Text-to-SQL tasks by removing structural pieces of
information from the database. Our results indicate a significant performance
drop in GPT-3.5 on the unfamiliar Termite dataset, even with ATD modifications,
highlighting the effect of Data Contamination on LLMs in Text-to-SQL
translation tasks.",http://arxiv.org/abs/2402.08100v1,84,16,18,16,18,16
Feed-Forward Neural Networks as a Mixed-Integer Program,"Deep neural networks (DNNs) are widely studied in various applications. A DNN
consists of layers of neurons that compute affine combinations, apply nonlinear
operations, and produce corresponding activations. The rectified linear unit
(ReLU) is a typical nonlinear operator, outputting the max of its input and
zero. In scenarios like max pooling, where multiple input values are involved,
a fixed-parameter DNN can be modeled as a mixed-integer program (MIP). This
formulation, with continuous variables representing unit outputs and binary
variables for ReLU activation, finds applications across diverse domains. This
study explores the formulation of trained ReLU neurons as MIP and applies MIP
models for training neural networks (NNs). Specifically, it investigates
interactions between MIP techniques and various NN architectures, including
binary DNNs (employing step activation functions) and binarized DNNs (with
weights and activations limited to $-1,0,+1$). The research focuses on training
and evaluating proposed approaches through experiments on handwritten digit
classification models. The comparative study assesses the performance of
trained ReLU NNs, shedding light on the effectiveness of MIP formulations in
enhancing training processes for NNs.",http://arxiv.org/abs/2402.06697v1,84,16,18,17,18,15
Leveraging Self-Supervised Instance Contrastive Learning for Radar Object Detection,"In recent years, driven by the need for safer and more autonomous transport
systems, the automotive industry has shifted toward integrating a growing
number of Advanced Driver Assistance Systems (ADAS). Among the array of sensors
employed for object recognition tasks, radar sensors have emerged as a
formidable contender due to their abilities in adverse weather conditions or
low-light scenarios and their robustness in maintaining consistent performance
across diverse environments. However, the small size of radar datasets and the
complexity of the labelling of those data limit the performance of radar object
detectors. Driven by the promising results of self-supervised learning in
computer vision, this paper presents RiCL, an instance contrastive learning
framework to pre-train radar object detectors. We propose to exploit the
detection from the radar and the temporal information to pre-train the radar
object detection model in a self-supervised way using contrastive learning. We
aim to pre-train an object detector's backbone, head and neck to learn with
fewer data. Experiments on the CARRADA and the RADDet datasets show the
effectiveness of our approach in learning generic representations of objects in
range-Doppler maps. Notably, our pre-training strategy allows us to use only
20% of the labelled data to reach a similar mAP@0.5 than a supervised approach
using the whole training set.",http://arxiv.org/abs/2402.08427v1,84,18,16,18,18,14
Minecraft-ify: Minecraft Style Image Generation with Text-guided Image Editing for In-Game Application,"In this paper, we first present the character texture generation system
\textit{Minecraft-ify}, specified to Minecraft video game toward in-game
application. Ours can generate face-focused image for texture mapping tailored
to 3D virtual character having cube manifold. While existing projects or works
only generate texture, proposed system can inverse the user-provided real
image, or generate average/random appearance from learned distribution.
Moreover, it can be manipulated with text-guidance using StyleGAN and
StyleCLIP. These features provide a more extended user experience with enlarged
freedom as a user-friendly AI-tool. Project page can be found at
https://gh-bumsookim.github.io/Minecraft-ify/",http://arxiv.org/abs/2402.05448v1,84,18,16,16,18,16
Clustering Dynamics for Improved Speed Prediction Deriving from Topographical GPS Registrations,"A persistent challenge in the field of Intelligent Transportation Systems is
to extract accurate traffic insights from geographic regions with scarce or no
data coverage. To this end, we propose solutions for speed prediction using
sparse GPS data points and their associated topographical and road design
features. Our goal is to investigate whether we can use similarities in the
terrain and infrastructure to train a machine learning model that can predict
speed in regions where we lack transportation data. For this we create a
Temporally Orientated Speed Dictionary Centered on Topographically Clustered
Roads, which helps us to provide speed correlations to selected feature
configurations. Our results show qualitative and quantitative improvement over
new and standard regression methods. The presented framework provides a fresh
perspective on devising strategies for missing data traffic analysis.",http://arxiv.org/abs/2402.07507v1,84,16,18,18,16,16
"PresAIse, A Prescriptive AI Solution for Enterprises","Prescriptive AI represents a transformative shift in decision-making,
offering causal insights and actionable recommendations. Despite its huge
potential, enterprise adoption often faces several challenges. The first
challenge is caused by the limitations of observational data for accurate
causal inference which is typically a prerequisite for good decision-making.
The second pertains to the interpretability of recommendations, which is
crucial for enterprise decision-making settings. The third challenge is the
silos between data scientists and business users, hindering effective
collaboration. This paper outlines an initiative from IBM Research, aiming to
address some of these challenges by offering a suite of prescriptive AI
solutions. Leveraging insights from various research papers, the solution suite
includes scalable causal inference methods, interpretable decision-making
approaches, and the integration of large language models (LLMs) to bridge
communication gaps via a conversation agent. A proof-of-concept, PresAIse,
demonstrates the solutions' potential by enabling non-ML experts to interact
with prescriptive AI models via a natural language interface, democratizing
advanced analytics for strategic decision-making.",http://arxiv.org/abs/2402.02006v2,84,18,17,18,16,15
Learning Cartesian Product Graphs with Laplacian Constraints,"Graph Laplacian learning, also known as network topology inference, is a
problem of great interest to multiple communities. In Gaussian graphical models
(GM), graph learning amounts to endowing covariance selection with the
Laplacian structure. In graph signal processing (GSP), it is essential to infer
the unobserved graph from the outputs of a filtering system. In this paper, we
study the problem of learning Cartesian product graphs under Laplacian
constraints. The Cartesian graph product is a natural way for modeling
higher-order conditional dependencies and is also the key for generalizing GSP
to multi-way tensors. We establish statistical consistency for the penalized
maximum likelihood estimation (MLE) of a Cartesian product Laplacian, and
propose an efficient algorithm to solve the problem. We also extend our method
for efficient joint graph learning and imputation in the presence of structural
missing values. Experiments on synthetic and real-world datasets demonstrate
that our method is superior to previous GSP and GM methods.",http://arxiv.org/abs/2402.08105v1,84,18,16,18,16,16
Skip \n: A Simple Method to Reduce Hallucination in Large Vision-Language Models,"Recent advancements in large vision-language models (LVLMs) have demonstrated
impressive capability in visual information understanding with human language.
Despite these advances, LVLMs still face challenges with multimodal
hallucination, such as generating text descriptions of objects that are not
present in the visual information. However, the underlying fundamental reasons
of multimodal hallucinations remain poorly explored. In this paper, we propose
a new perspective, suggesting that the inherent biases in LVLMs might be a key
factor in hallucinations. Specifically, we systematically identify a semantic
shift bias related to paragraph breaks (\n\n), where the content before and
after '\n\n' in the training data frequently exhibit significant semantic
changes. This pattern leads the model to infer that the contents following
'\n\n' should be obviously different from the preceding contents with less
hallucinatory descriptions, thereby increasing the probability of hallucinatory
descriptions subsequent to the '\n\n'. We have validated this hypothesis on
multiple publicly available LVLMs. Besides, we find that deliberately inserting
'\n\n' at the generated description can induce more hallucinations. A simple
method is proposed to effectively mitigate the hallucination of LVLMs by
skipping the output of '\n'.",http://arxiv.org/abs/2402.01345v4,84,18,17,17,16,16
Monitored Markov Decision Processes,"In reinforcement learning (RL), an agent learns to perform a task by
interacting with an environment and receiving feedback (a numerical reward) for
its actions. However, the assumption that rewards are always observable is
often not applicable in real-world problems. For example, the agent may need to
ask a human to supervise its actions or activate a monitoring system to receive
feedback. There may even be a period of time before rewards become observable,
or a period of time after which rewards are no longer given. In other words,
there are cases where the environment generates rewards in response to the
agent's actions but the agent cannot observe them. In this paper, we formalize
a novel but general RL framework - Monitored MDPs - where the agent cannot
always observe rewards. We discuss the theoretical and practical consequences
of this setting, show challenges raised even in toy environments, and propose
algorithms to begin to tackle this novel setting. This paper introduces a
powerful new formalism that encompasses both new and existing problems and lays
the foundation for future research.",http://arxiv.org/abs/2402.06819v1,84,16,18,18,16,16
Careless Whisper: Speech-to-Text Hallucination Harms,"Speech-to-text services aim to transcribe input audio as accurately as
possible. They increasingly play a role in everyday life, for example in
personal voice assistants or in customer-company interactions. We evaluate Open
AI's Whisper, a state-of-the-art service outperforming industry competitors.
While many of Whisper's transcriptions were highly accurate, we found that
roughly 1% of audio transcriptions contained entire hallucinated phrases or
sentences, which did not exist in any form in the underlying audio. We
thematically analyze the Whisper-hallucinated content, finding that 38% of
hallucinations include explicit harms such as violence, made up personal
information, or false video-based authority. We further provide hypotheses on
why hallucinations occur, uncovering potential disparities due to speech type
by health status. We call on industry practitioners to ameliorate these
language-model-based hallucinations in Whisper, and to raise awareness of
potential biases in downstream applications of speech-to-text models.",http://arxiv.org/abs/2402.08021v1,84,16,18,18,16,16
Strategizing against No-Regret Learners in First-Price Auctions,"We study repeated first-price auctions and general repeated Bayesian games
between two players, where one player, the learner, employs a no-regret
learning algorithm, and the other player, the optimizer, knowing the learner's
algorithm, strategizes to maximize its own utility. For a commonly used class
of no-regret learning algorithms called mean-based algorithms, we show that (i)
in standard (i.e., full-information) first-price auctions, the optimizer cannot
get more than the Stackelberg utility -- a standard benchmark in the
literature, but (ii) in Bayesian first-price auctions, there are instances
where the optimizer can achieve much higher than the Stackelberg utility.
  On the other hand, Mansour et al. (2022) showed that a more sophisticated
class of algorithms called no-polytope-swap-regret algorithms are sufficient to
cap the optimizer's utility at the Stackelberg utility in any repeated Bayesian
game (including Bayesian first-price auctions), and they pose the open question
whether no-polytope-swap-regret algorithms are necessary to cap the optimizer's
utility. For general Bayesian games, under a reasonable and necessary
condition, we prove that no-polytope-swap-regret algorithms are indeed
necessary to cap the optimizer's utility and thus answer their open question.
For Bayesian first-price auctions, we give a simple improvement of the standard
algorithm for minimizing the polytope swap regret by exploiting the structure
of Bayesian first-price auctions.",http://arxiv.org/abs/2402.08637v1,84,18,16,18,16,16
PirateNets: Physics-informed Deep Learning with Residual Adaptive Networks,"While physics-informed neural networks (PINNs) have become a popular deep
learning framework for tackling forward and inverse problems governed by
partial differential equations (PDEs), their performance is known to degrade
when larger and deeper neural network architectures are employed. Our study
identifies that the root of this counter-intuitive behavior lies in the use of
multi-layer perceptron (MLP) architectures with non-suitable initialization
schemes, which result in poor trainablity for the network derivatives, and
ultimately lead to an unstable minimization of the PDE residual loss. To
address this, we introduce Physics-informed Residual Adaptive Networks
(PirateNets), a novel architecture that is designed to facilitate stable and
efficient training of deep PINN models. PirateNets leverage a novel adaptive
residual connection, which allows the networks to be initialized as shallow
networks that progressively deepen during training. We also show that the
proposed initialization scheme allows us to encode appropriate inductive biases
corresponding to a given PDE system into the network architecture. We provide
comprehensive empirical evidence showing that PirateNets are easier to optimize
and can gain accuracy from considerably increased depth, ultimately achieving
state-of-the-art results across various benchmarks. All code and data
accompanying this manuscript will be made publicly available at
\url{https://github.com/PredictiveIntelligenceLab/jaxpi}.",http://arxiv.org/abs/2402.00326v3,84,18,18,16,16,16
OrderBkd: Textual backdoor attack through repositioning,"The use of third-party datasets and pre-trained machine learning models poses
a threat to NLP systems due to possibility of hidden backdoor attacks. Existing
attacks involve poisoning the data samples such as insertion of tokens or
sentence paraphrasing, which either alter the semantics of the original texts
or can be detected. Our main difference from the previous work is that we use
the reposition of a two words in a sentence as a trigger. By designing and
applying specific part-of-speech (POS) based rules for selecting these tokens,
we maintain high attack success rate on SST-2 and AG classification datasets
while outperforming existing attacks in terms of perplexity and semantic
similarity to the clean samples. In addition, we show the robustness of our
attack to the ONION defense method. All the code and data for the paper can be
obtained at https://github.com/alekseevskaia/OrderBkd.",http://arxiv.org/abs/2402.07689v1,84,18,17,17,16,16
Thresholding Data Shapley for Data Cleansing Using Multi-Armed Bandits,"Data cleansing aims to improve model performance by removing a set of harmful
instances from the training dataset. Data Shapley is a common theoretically
guaranteed method to evaluate the contribution of each instance to model
performance; however, it requires training on all subsets of the training data,
which is computationally expensive. In this paper, we propose an
iterativemethod to fast identify a subset of instances with low data Shapley
values by using the thresholding bandit algorithm. We provide a theoretical
guarantee that the proposed method can accurately select harmful instances if a
sufficiently large number of iterations is conducted. Empirical evaluation
using various models and datasets demonstrated that the proposed method
efficiently improved the computational speed while maintaining the model
performance.",http://arxiv.org/abs/2402.08209v1,84,16,18,16,18,16
Inherent Diverse Redundant Safety Mechanisms for AI-based Software Elements in Automotive Applications,"This paper explores the role and challenges of Artificial Intelligence (AI)
algorithms, specifically AI-based software elements, in autonomous driving
systems. These AI systems are fundamental in executing real-time critical
functions in complex and high-dimensional environments. They handle vital tasks
like multi-modal perception, cognition, and decision-making tasks such as
motion planning, lane keeping, and emergency braking. A primary concern relates
to the ability (and necessity) of AI models to generalize beyond their initial
training data. This generalization issue becomes evident in real-time
scenarios, where models frequently encounter inputs not represented in their
training or validation data. In such cases, AI systems must still function
effectively despite facing distributional or domain shifts. This paper
investigates the risk associated with overconfident AI models in
safety-critical applications like autonomous driving. To mitigate these risks,
methods for training AI models that help maintain performance without
overconfidence are proposed. This involves implementing certainty reporting
architectures and ensuring diverse training data. While various
distribution-based methods exist to provide safety mechanisms for AI models,
there is a noted lack of systematic assessment of these methods, especially in
the context of safety-critical automotive applications. Many methods in the
literature do not adapt well to the quick response times required in
safety-critical edge applications. This paper reviews these methods, discusses
their suitability for safety-critical applications, and highlights their
strengths and limitations. The paper also proposes potential improvements to
enhance the safety and reliability of AI algorithms in autonomous vehicles in
the context of rapid and accurate decision-making processes.",http://arxiv.org/abs/2402.08208v1,84,18,17,16,17,16
Transferring Ultrahigh-Field Representations for Intensity-Guided Brain Segmentation of Low-Field Magnetic Resonance Imaging,"Ultrahigh-field (UHF) magnetic resonance imaging (MRI), i.e., 7T MRI,
provides superior anatomical details of internal brain structures owing to its
enhanced signal-to-noise ratio and susceptibility-induced contrast. However,
the widespread use of 7T MRI is limited by its high cost and lower
accessibility compared to low-field (LF) MRI. This study proposes a
deep-learning framework that systematically fuses the input LF magnetic
resonance feature representations with the inferred 7T-like feature
representations for brain image segmentation tasks in a 7T-absent environment.
Specifically, our adaptive fusion module aggregates 7T-like features derived
from the LF image by a pre-trained network and then refines them to be
effectively assimilable UHF guidance into LF image features. Using
intensity-guided features obtained from such aggregation and assimilation,
segmentation models can recognize subtle structural representations that are
usually difficult to recognize when relying only on LF features. Beyond such
advantages, this strategy can seamlessly be utilized by modulating the contrast
of LF features in alignment with UHF guidance, even when employing arbitrary
segmentation models. Exhaustive experiments demonstrated that the proposed
method significantly outperformed all baseline models on both brain tissue and
whole-brain segmentation tasks; further, it exhibited remarkable adaptability
and scalability by successfully integrating diverse segmentation models and
tasks. These improvements were not only quantifiable but also visible in the
superlative visual quality of segmentation masks.",http://arxiv.org/abs/2402.08409v1,84,18,17,18,16,15
"History, Development, and Principles of Large Language Models-An Introductory Survey","Language models serve as a cornerstone in natural language processing (NLP),
utilizing mathematical methods to generalize language laws and knowledge for
prediction and generation. Over extensive research spanning decades, language
modeling has progressed from initial statistical language models (SLMs) to the
contemporary landscape of large language models (LLMs). Notably, the swift
evolution of LLMs has reached the ability to process, understand, and generate
human-level text. Nevertheless, despite the significant advantages that LLMs
offer in improving both work and personal lives, the limited understanding
among general practitioners about the background and principles of these models
hampers their full potential. Notably, most LLMs reviews focus on specific
aspects and utilize specialized language, posing a challenge for practitioners
lacking relevant background knowledge. In light of this, this survey aims to
present a comprehensible overview of LLMs to assist a broader audience. It
strives to facilitate a comprehensive understanding by exploring the historical
background of language models and tracing their evolution over time. The survey
further investigates the factors influencing the development of LLMs,
emphasizing key contributions. Additionally, it concentrates on elucidating the
underlying principles of LLMs, equipping audiences with essential theoretical
knowledge. The survey also highlights the limitations of existing work and
points out promising future directions.",http://arxiv.org/abs/2402.06853v1,84,16,18,18,18,14
Towards Quantifying the Preconditioning Effect of Adam,"There is a notable dearth of results characterizing the preconditioning
effect of Adam and showing how it may alleviate the curse of ill-conditioning
-- an issue plaguing gradient descent (GD). In this work, we perform a detailed
analysis of Adam's preconditioning effect for quadratic functions and quantify
to what extent Adam can mitigate the dependence on the condition number of the
Hessian. Our key finding is that Adam can suffer less from the condition number
but at the expense of suffering a dimension-dependent quantity. Specifically,
for a $d$-dimensional quadratic with a diagonal Hessian having condition number
$\kappa$, we show that the effective condition number-like quantity controlling
the iteration complexity of Adam without momentum is $\mathcal{O}(\min(d,
\kappa))$. For a diagonally dominant Hessian, we obtain a bound of
$\mathcal{O}(\min(d \sqrt{d \kappa}, \kappa))$ for the corresponding quantity.
Thus, when $d < \mathcal{O}(\kappa^p)$ where $p = 1$ for a diagonal Hessian and
$p = 1/3$ for a diagonally dominant Hessian, Adam can outperform GD (which has
an $\mathcal{O}(\kappa)$ dependence). On the negative side, our results suggest
that Adam can be worse than GD for a sufficiently non-diagonal Hessian even if
$d \ll \mathcal{O}(\kappa^{1/3})$; we corroborate this with empirical evidence.
Finally, we extend our analysis to functions satisfying per-coordinate
Lipschitz smoothness and a modified version of the Polyak-\L ojasiewicz
condition.",http://arxiv.org/abs/2402.07114v1,84,18,16,18,18,14
Two Tales of Single-Phase Contrastive Hebbian Learning,"The search for ""biologically plausible"" learning algorithms has converged on
the idea of representing gradients as activity differences. However, most
approaches require a high degree of synchronization (distinct phases during
learning) and introduce substantial computational overhead, which raises doubts
regarding their biological plausibility as well as their potential utility for
neuromorphic computing. Furthermore, they commonly rely on applying
infinitesimal perturbations (nudges) to output units, which is impractical in
noisy environments. Recently it has been shown that by modelling artificial
neurons as dyads with two oppositely nudged compartments, it is possible for a
fully local learning algorithm named ``dual propagation'' to bridge the
performance gap to backpropagation, without requiring separate learning phases
or infinitesimal nudging. However, the algorithm has the drawback that its
numerical stability relies on symmetric nudging, which may be restrictive in
biological and analog implementations. In this work we first provide a solid
foundation for the objective underlying the dual propagation method, which also
reveals a surprising connection with adversarial robustness. Second, we
demonstrate how dual propagation is related to a particular adjoint state
method, which is stable regardless of asymmetric nudging.",http://arxiv.org/abs/2402.08573v1,84,16,17,18,17,16
How to Refactor this Code? An Exploratory Study on Developer-ChatGPT Refactoring Conversations,"Large Language Models (LLMs), like ChatGPT, have gained widespread popularity
and usage in various software engineering tasks, including refactoring,
testing, code review, and program comprehension. Despite recent studies delving
into refactoring documentation in commit messages, issues, and code review,
little is known about how developers articulate their refactoring needs when
interacting with ChatGPT. In this paper, our goal is to explore conversations
between developers and ChatGPT related to refactoring to better understand how
developers identify areas for improvement in code and how ChatGPT addresses
developers' needs. Our approach relies on text mining refactoring-related
conversations from 17,913 ChatGPT prompts and responses, and investigating
developers' explicit refactoring intention. Our results reveal that (1)
developer-ChatGPT conversations commonly involve generic and specific
terms/phrases; (2) developers often make generic refactoring requests, while
ChatGPT typically includes the refactoring intention; and (3) various learning
settings when prompting ChatGPT in the context of refactoring. We envision that
our findings contribute to a broader understanding of the collaboration between
developers and AI models, in the context of code refactoring, with implications
for model improvement, tool development, and best practices in software
engineering.",http://arxiv.org/abs/2402.06013v1,84,17,16,16,18,17
Multimodal Query Suggestion with Multi-Agent Reinforcement Learning from Human Feedback,"In the rapidly evolving landscape of information retrieval, search engines
strive to provide more personalized and relevant results to users. Query
suggestion systems play a crucial role in achieving this goal by assisting
users in formulating effective queries. However, existing query suggestion
systems mainly rely on textual inputs, potentially limiting user search
experiences for querying images. In this paper, we introduce a novel Multimodal
Query Suggestion (MMQS) task, which aims to generate query suggestions based on
user query images to improve the intentionality and diversity of search
results. We present the RL4Sugg framework, leveraging the power of Large
Language Models (LLMs) with Multi-Agent Reinforcement Learning from Human
Feedback to optimize the generation process. Through comprehensive experiments,
we validate the effectiveness of RL4Sugg, demonstrating a 18% improvement
compared to the best existing approach. Moreover, the MMQS has been transferred
into real-world search engine products, which yield enhanced user engagement.
Our research advances query suggestion systems and provides a new perspective
on multimodal information retrieval.",http://arxiv.org/abs/2402.04867v2,84,16,16,18,18,16
Classifying Nodes in Graphs without GNNs,"Graph neural networks (GNNs) are the dominant paradigm for classifying nodes
in a graph, but they have several undesirable attributes stemming from their
message passing architecture. Recently, distillation methods succeeded in
eliminating the use of GNNs at test time but they still require them during
training. We perform a careful analysis of the role that GNNs play in
distillation methods. This analysis leads us to propose a fully GNN-free
approach for node classification, not requiring them at train or test time. Our
method consists of three key components: smoothness constraints,
pseudo-labeling iterations and neighborhood-label histograms. Our final
approach can match the state-of-the-art accuracy on standard popular benchmarks
such as citation and co-purchase networks, without training a GNN.",http://arxiv.org/abs/2402.05934v1,84,19,16,17,16,16
Regret Minimization in Stackelberg Games with Side Information,"In its most basic form, a Stackelberg game is a two-player game in which a
leader commits to a (mixed) strategy, and a follower best-responds. Stackelberg
games are perhaps one of the biggest success stories of algorithmic game theory
over the last decade, as algorithms for playing in Stackelberg games have been
deployed in many real-world domains including airport security, anti-poaching
efforts, and cyber-crime prevention. However, these algorithms often fail to
take into consideration the additional information available to each player
(e.g. traffic patterns, weather conditions, network congestion), a salient
feature of reality which may significantly affect both players' optimal
strategies. We formalize such settings as Stackelberg games with side
information, in which both players observe an external context before playing.
The leader then commits to a (possibly context-dependent) strategy, and the
follower best-responds to both the leader's strategy and the context. We focus
on the online setting in which a sequence of followers arrive over time, and
the context may change from round-to-round. In sharp contrast to the
non-contextual version, we show that it is impossible for the leader to achieve
good performance (measured by regret) in the full adversarial setting (i.e.,
when both the context and the follower are chosen by an adversary). However, it
turns out that a little bit of randomness goes a long way. Motivated by our
impossibility result, we show that no-regret learning is possible in two
natural relaxations: the setting in which the sequence of followers is chosen
stochastically and the sequence of contexts is adversarial, and the setting in
which the sequence of contexts is stochastic and the sequence of followers is
chosen by an adversary.",http://arxiv.org/abs/2402.08576v1,84,18,16,18,18,14
Analyzing Prompt Influence on Automated Method Generation: An Empirical Study with Copilot,"Generative AI is changing the way developers interact with software systems,
providing services that can produce and deliver new content, crafted to satisfy
the actual needs of developers. For instance, developers can ask for new code
directly from within their IDEs by writing natural language prompts, and
integrated services based on generative AI, such as Copilot, immediately
respond to prompts by providing ready-to-use code snippets. Formulating the
prompt appropriately, and incorporating the useful information while avoiding
any information overload, can be an important factor in obtaining the right
piece of code. The task of designing good prompts is known as prompt
engineering. In this paper, we systematically investigate the influence of
eight prompt features on the style and the content of prompts, on the level of
correctness, complexity, size, and similarity to the developers' code of the
generated code. We specifically consider the task of using Copilot with 124,800
prompts obtained by systematically combining the eight considered prompt
features to generate the implementation of 200 Java methods. Results show how
some prompt features, such as the presence of examples and the summary of the
purpose of the method, can significantly influence the quality of the result.",http://dx.doi.org/10.1145/3643916.3644409,84,16,18,18,16,16
Taking Class Imbalance Into Account in Open Set Recognition Evaluation,"In recent years Deep Neural Network-based systems are not only increasing in
popularity but also receive growing user trust. However, due to the
closed-world assumption of such systems, they cannot recognize samples from
unknown classes and often induce an incorrect label with high confidence.
Presented work looks at the evaluation of methods for Open Set Recognition,
focusing on the impact of class imbalance, especially in the dichotomy between
known and unknown samples. As an outcome of problem analysis, we present a set
of guidelines for evaluation of methods in this field.",http://arxiv.org/abs/2402.06331v1,84,16,18,18,16,16
Understanding the Training Speedup from Sampling with Approximate Losses,"It is well known that selecting samples with large losses/gradients can
significantly reduce the number of training steps. However, the selection
overhead is often too high to yield any meaningful gains in terms of overall
training time. In this work, we focus on the greedy approach of selecting
samples with large \textit{approximate losses} instead of exact losses in order
to reduce the selection overhead. For smooth convex losses, we show that such a
greedy strategy can converge to a constant factor of the minimum value of the
average loss in fewer iterations than the standard approach of random
selection. We also theoretically quantify the effect of the approximation
level. We then develop SIFT which uses early exiting to obtain approximate
losses with an intermediate layer's representations for sample selection. We
evaluate SIFT on the task of training a 110M parameter 12-layer BERT base model
and show significant gains (in terms of training hours and number of
backpropagation steps) without any optimized implementation over vanilla
training. For e.g., to reach 64% validation accuracy, SIFT with exit at the
first layer takes ~43 hours compared to ~57 hours of vanilla training.",http://arxiv.org/abs/2402.07052v1,84,18,17,17,16,16
"The Loss Landscape of Shallow ReLU-like Neural Networks: Stationary Points, Saddle Escaping, and Network Embedding","In this paper, we investigate the loss landscape of one-hidden-layer neural
networks with ReLU-like activation functions trained with the empirical squared
loss. As the activation function is non-differentiable, it is so far unclear
how to completely characterize the stationary points. We propose the conditions
for stationarity that apply to both non-differentiable and differentiable
cases. Additionally, we show that, if a stationary point does not contain
""escape neurons"", which are defined with first-order conditions, then it must
be a local minimum. Moreover, for the scalar-output case, the presence of an
escape neuron guarantees that the stationary point is not a local minimum. Our
results refine the description of the saddle-to-saddle training process
starting from infinitesimally small (vanishing) initialization for shallow
ReLU-like networks, linking saddle escaping directly with the parameter changes
of escape neurons. Moreover, we are also able to fully discuss how network
embedding, which is to instantiate a narrower network within a wider network,
reshapes the stationary points.",http://arxiv.org/abs/2402.05626v1,84,16,18,18,18,14
Online Structured Prediction with Fenchel--Young Losses and Improved Surrogate Regret for Online Multiclass Classification with Logistic Loss,"This paper studies online structured prediction with full-information
feedback. For online multiclass classification, van der Hoeven (2020) has
obtained surrogate regret bounds independent of the time horizon, or
\emph{finite}, by introducing an elegant \emph{exploit-the-surrogate-gap}
framework. However, this framework has been limited to multiclass
classification primarily because it relies on a classification-specific
procedure for converting estimated scores to outputs. We extend the
exploit-the-surrogate-gap framework to online structured prediction with
\emph{Fenchel--Young losses}, a large family of surrogate losses including the
logistic loss for multiclass classification, obtaining finite surrogate regret
bounds in various structured prediction problems. To this end, we propose and
analyze \emph{randomized decoding}, which converts estimated scores to general
structured outputs. Moreover, by applying our decoding to online multiclass
classification with the logistic loss, we obtain a surrogate regret bound of
$O(B^2)$, where $B$ is the $\ell_2$-diameter of the domain. This bound is tight
up to logarithmic factors and improves the previous bound of $O(dB^2)$ due to
van der Hoeven (2020) by a factor of $d$, the number of classes.",http://arxiv.org/abs/2402.08180v1,84,16,18,18,18,14
Tighter Bounds on the Information Bottleneck with Application to Deep Learning,"Deep Neural Nets (DNNs) learn latent representations induced by their
downstream task, objective function, and other parameters. The quality of the
learned representations impacts the DNN's generalization ability and the
coherence of the emerging latent space. The Information Bottleneck (IB)
provides a hypothetically optimal framework for data modeling, yet it is often
intractable. Recent efforts combined DNNs with the IB by applying VAE-inspired
variational methods to approximate bounds on mutual information, resulting in
improved robustness to adversarial attacks. This work introduces a new and
tighter variational bound for the IB, improving performance of previous
IB-inspired DNNs. These advancements strengthen the case for the IB and its
variational approximations as a data modeling framework, and provide a simple
method to significantly enhance the adversarial robustness of classifier DNNs.",http://arxiv.org/abs/2402.07639v1,84,16,18,18,16,16
Training Coupled Phase Oscillators as a Neuromorphic Platform using Equilibrium Propagation,"Given the rapidly growing scale and resource requirements of machine learning
applications, the idea of building more efficient learning machines much closer
to the laws of physics is an attractive proposition. One central question for
identifying promising candidates for such neuromorphic platforms is whether not
only inference but also training can exploit the physical dynamics. In this
work, we show that it is possible to successfully train a system of coupled
phase oscillators - one of the most widely investigated nonlinear dynamical
systems with a multitude of physical implementations, comprising laser arrays,
coupled mechanical limit cycles, superfluids, and exciton-polaritons. To this
end, we apply the approach of equilibrium propagation, which permits to extract
training gradients via a physical realization of backpropagation, based only on
local interactions. The complex energy landscape of the XY/ Kuramoto model
leads to multistability, and we show how to address this challenge. Our study
identifies coupled phase oscillators as a new general-purpose neuromorphic
platform and opens the door towards future experimental implementations.",http://arxiv.org/abs/2402.08579v1,84,18,16,18,18,14
Confronting Discrimination in Classification: Smote Based on Marginalized Minorities in the Kernel Space for Imbalanced Data,"Financial fraud detection poses a typical challenge characterized by class
imbalance, where instances of fraud are extremely rare but can lead to
unpredictable economic losses if misidentified. Precisely classifying these
critical minority samples represents a challenging task within the
classification. The primary difficulty arises from mainstream classifiers,
which often exhibit ""implicit discrimination"" against minority samples in
evaluation metrics, which results in frequent misclassifications, and the key
to the problem lies in the overlap of feature spaces between majority and
minority samples. To address these challenges, oversampling is a feasible
solution, yet current classical oversampling methods often lack the necessary
caution in sample selection, exacerbating feature space overlap. In response,
we propose a novel classification oversampling approach based on the decision
boundary and sample proximity relationships. This method carefully considers
the distance between critical samples and the decision hyperplane, as well as
the density of surrounding samples, resulting in an adaptive oversampling
strategy in the kernel space. Finally, we test the proposed method on a classic
financial fraud dataset, and the results show that our proposed method provides
an effective and robust solution that can improve the classification accuracy
of minorities.",http://arxiv.org/abs/2402.08202v1,84,18,17,17,16,16
Gemini Goes to Med School: Exploring the Capabilities of Multimodal Large Language Models on Medical Challenge Problems & Hallucinations,"Large language models have the potential to be valuable in the healthcare
industry, but it's crucial to verify their safety and effectiveness through
rigorous evaluation. For this purpose, we comprehensively evaluated both
open-source LLMs and Google's new multimodal LLM called Gemini across Medical
reasoning, hallucination detection, and Medical Visual Question Answering
tasks. While Gemini showed competence, it lagged behind state-of-the-art models
like MedPaLM 2 and GPT-4 in diagnostic accuracy. Additionally, Gemini achieved
an accuracy of 61.45\% on the medical VQA dataset, significantly lower than
GPT-4V's score of 88\%. Our analysis revealed that Gemini is highly susceptible
to hallucinations, overconfidence, and knowledge gaps, which indicate risks if
deployed uncritically. We also performed a detailed analysis by medical subject
and test type, providing actionable feedback for developers and clinicians. To
mitigate risks, we applied prompting strategies that improved performance.
Additionally, we facilitated future research and development by releasing a
Python module for medical LLM evaluation and establishing a dedicated
leaderboard on Hugging Face for medical domain LLMs. Python module can be found
at https://github.com/promptslab/RosettaEval",http://arxiv.org/abs/2402.07023v1,84,16,18,16,18,16
Distributed Compression in the Era of Machine Learning: A Review of Recent Advances,"Many applications from camera arrays to sensor networks require efficient
compression and processing of correlated data, which in general is collected in
a distributed fashion. While information-theoretic foundations of distributed
compression are well investigated, the impact of theory in practice-oriented
applications to this day has been somewhat limited. As the field of data
compression is undergoing a transformation with the emergence of learning-based
techniques, machine learning is becoming an important tool to reap the
long-promised benefits of distributed compression. In this paper, we review the
recent contributions in the broad area of learned distributed compression
techniques for abstract sources and images. In particular, we discuss
approaches that provide interpretable results operating close to
information-theoretic bounds. We also highlight unresolved research challenges,
aiming to inspire fresh interest and advancements in the field of learned
distributed compression.",http://arxiv.org/abs/2402.07997v1,84,16,17,19,16,16
Mixture of Link Predictors,"Link prediction, which aims to forecast unseen connections in graphs, is a
fundamental task in graph machine learning. Heuristic methods, leveraging a
range of different pairwise measures such as common neighbors and shortest
paths, often rival the performance of vanilla Graph Neural Networks (GNNs).
Therefore, recent advancements in GNNs for link prediction (GNN4LP) have
primarily focused on integrating one or a few types of pairwise information. In
this work, we reveal that different node pairs within the same dataset
necessitate varied pairwise information for accurate prediction and models that
only apply the same pairwise information uniformly could achieve suboptimal
performance. As a result, we propose a simple mixture of experts model Link-MoE
for link prediction. Link-MoE utilizes various GNNs as experts and
strategically selects the appropriate expert for each node pair based on
various types of pairwise information. Experimental results across diverse
real-world datasets demonstrate substantial performance improvement from
Link-MoE. Notably, Link-MoE achieves a relative improvement of 18.82\% on the
MRR metric for the Pubmed dataset and 10.8\% on the Hits@100 metric for the
ogbl-ppa dataset, compared to the best baselines.",http://arxiv.org/abs/2402.08583v1,84,18,16,18,18,14
Evolving AI Risk Management: A Maturity Model based on the NIST AI Risk Management Framework,"Researchers, government bodies, and organizations have been repeatedly
calling for a shift in the responsible AI community from general principles to
tangible and operationalizable practices in mitigating the potential
sociotechnical harms of AI. Frameworks like the NIST AI RMF embody an emerging
consensus on recommended practices in operationalizing sociotechnical harm
mitigation. However, private sector organizations currently lag far behind this
emerging consensus. Implementation is sporadic and selective at best. At worst,
it is ineffective and can risk serving as a misleading veneer of trustworthy
processes, providing an appearance of legitimacy to substantively harmful
practices. In this paper, we provide a foundation for a framework for
evaluating where organizations sit relative to the emerging consensus on
sociotechnical harm mitigation best practices: a flexible maturity model based
on the NIST AI RMF.",http://arxiv.org/abs/2401.15229v2,84,16,18,18,16,16
Higher Layers Need More LoRA Experts,"Parameter-efficient tuning (PEFT) techniques like low-rank adaptation (LoRA)
offer training efficiency on Large Language Models, but their impact on model
performance remains limited. Recent efforts integrate LoRA and
Mixture-of-Experts (MoE) to improve the performance of PEFT methods. Despite
promising results, research on improving the efficiency of LoRA with MoE is
still in its early stages. Recent studies have shown that experts in the MoE
architecture have different strengths and also exhibit some redundancy. Does
this statement also apply to parameter-efficient MoE? In this paper, we
introduce a novel parameter-efficient MoE method,
\textit{\textbf{M}oE-L\textbf{o}RA with \textbf{L}ayer-wise Expert
\textbf{A}llocation (MoLA)} for Transformer-based models, where each model
layer has the flexibility to employ a varying number of LoRA experts. We
investigate several architectures with varying layer-wise expert
configurations. Experiments on six well-known NLP and commonsense QA benchmarks
demonstrate that MoLA achieves equal or superior performance compared to all
baselines. We find that allocating more LoRA experts to higher layers further
enhances the effectiveness of models with a certain number of experts in total.
With much fewer parameters, this allocation strategy outperforms the setting
with the same number of experts in every layer. This work can be widely used as
a plug-and-play parameter-efficient tuning approach for various applications.
The code is available at https://github.com/GCYZSL/MoLA.",http://arxiv.org/abs/2402.08562v1,84,17,18,17,16,16
Task-conditioned adaptation of visual features in multi-task policy learning,"Successfully addressing a wide variety of tasks is a core ability of
autonomous agents, which requires flexibly adapting the underlying
decision-making strategies and, as we argue in this work, also adapting the
underlying perception modules. An analogical argument would be the human visual
system, which uses top-down signals to focus attention determined by the
current task. Similarly, in this work, we adapt pre-trained large vision models
conditioned on specific downstream tasks in the context of multi-task policy
learning. We introduce task-conditioned adapters that do not require finetuning
any pre-trained weights, combined with a single policy trained with behavior
cloning and capable of addressing multiple tasks. We condition the policy and
visual adapters on task embeddings, which can be selected at inference if the
task is known, or alternatively inferred from a set of example demonstrations.
To this end, we propose a new optimization-based estimator. We evaluate the
method on a wide variety of tasks of the CortexBench benchmark and show that,
compared to existing work, it can be addressed with a single policy. In
particular, we demonstrate that adapting visual features is a key design choice
and that the method generalizes to unseen tasks given visual demonstrations.",http://arxiv.org/abs/2402.07739v1,84,18,16,18,16,16
Future Prediction Can be a Strong Evidence of Good History Representation in Partially Observable Environments,"Learning a good history representation is one of the core challenges of
reinforcement learning (RL) in partially observable environments. Recent works
have shown the advantages of various auxiliary tasks for facilitating
representation learning. However, the effectiveness of such auxiliary tasks has
not been fully convincing, especially in partially observable environments that
require long-term memorization and inference. In this empirical study, we
investigate the effectiveness of future prediction for learning the
representations of histories, possibly of extensive length, in partially
observable environments. We first introduce an approach that decouples the task
of learning history representations from policy optimization via future
prediction. Then, our main contributions are two-fold: (a) we demonstrate that
the performance of reinforcement learning is strongly correlated with the
prediction accuracy of future observations in partially observable
environments, and (b) our approach can significantly improve the overall
end-to-end approach by preventing high-variance noisy signals from
reinforcement learning objectives to influence the representation learning. We
illustrate our claims on three types of benchmarks that necessitate the ability
to process long histories for high returns.",http://arxiv.org/abs/2402.07102v1,84,16,18,18,16,16
Feature Density Estimation for Out-of-Distribution Detection via Normalizing Flows,"Out-of-distribution (OOD) detection is a critical task for safe deployment of
learning systems in the open world setting. In this work, we investigate the
use of feature density estimation via normalizing flows for OOD detection and
present a fully unsupervised approach which requires no exposure to OOD data,
avoiding researcher bias in OOD sample selection. This is a post-hoc method
which can be applied to any pretrained model, and involves training a
lightweight auxiliary normalizing flow model to perform the out-of-distribution
detection via density thresholding. Experiments on OOD detection in image
classification show strong results for far-OOD data detection with only a
single epoch of flow training, including 98.2% AUROC for ImageNet-1k vs.
Textures, which exceeds the state of the art by 7.8%. We additionally explore
the connection between the feature space distribution of the pretrained model
and the performance of our method. Finally, we provide insights into training
pitfalls that have plagued normalizing flows for use in OOD detection.",http://arxiv.org/abs/2402.06537v1,84,18,17,17,16,16
Empirically Exploring How Novices Write Software Models in Alloy,"Writing declarative models has numerous benefits, ranging from automated
reasoning and correction of design-level properties before systems are built,
to automated testing and debugging of their implementations after they are
built. Alloy is a declarative modeling language that is well-suited for
verifying system designs. A key strength of Alloy is its scenario-finding
toolset, the Analyzer, which allows users to explore all valid scenarios that
adhere to the model's constraints up to a user-provided scope. However, even
with visualized scenarios, it is difficult to write correct Alloy models. To
address this, a growing body of work explores different techniques for
debugging Alloy models. In order to develop and evaluate these techniques in an
effective manor, this paper presents an empirical study of over 97,000 models
written by novice users trying to learn Alloy. We investigate how users write
both correct and incorrect models in order to produce a comprehensive benchmark
for future use as well as a series of observations to guide debugging and
educational efforts for Alloy model development.",http://arxiv.org/abs/2402.06624v1,84,15,17,18,17,17
LLMs Among Us: Generative AI Participating in Digital Discourse,"The emergence of Large Language Models (LLMs) has great potential to reshape
the landscape of many social media platforms. While this can bring promising
opportunities, it also raises many threats, such as biases and privacy
concerns, and may contribute to the spread of propaganda by malicious actors.
We developed the ""LLMs Among Us"" experimental framework on top of the Mastodon
social media platform for bot and human participants to communicate without
knowing the ratio or nature of bot and human participants. We built 10 personas
with three different LLMs, GPT-4, LLama 2 Chat, and Claude. We conducted three
rounds of the experiment and surveyed participants after each round to measure
the ability of LLMs to pose as human participants without human detection. We
found that participants correctly identified the nature of other users in the
experiment only 42% of the time despite knowing the presence of both bots and
humans. We also found that the choice of persona had substantially more impact
on human perception than the choice of mainstream LLMs.",http://arxiv.org/abs/2402.07940v1,84,18,16,18,16,16
On the Privacy of Selection Mechanisms with Gaussian Noise,"Report Noisy Max and Above Threshold are two classical differentially private
(DP) selection mechanisms. Their output is obtained by adding noise to a
sequence of low-sensitivity queries and reporting the identity of the query
whose (noisy) answer satisfies a certain condition. Pure DP guarantees for
these mechanisms are easy to obtain when Laplace noise is added to the queries.
On the other hand, when instantiated using Gaussian noise, standard analyses
only yield approximate DP guarantees despite the fact that the outputs of these
mechanisms lie in a discrete space. In this work, we revisit the analysis of
Report Noisy Max and Above Threshold with Gaussian noise and show that, under
the additional assumption that the underlying queries are bounded, it is
possible to provide pure ex-ante DP bounds for Report Noisy Max and pure
ex-post DP bounds for Above Threshold. The resulting bounds are tight and
depend on closed-form expressions that can be numerically evaluated using
standard methods. Empirically we find these lead to tighter privacy accounting
in the high privacy, low data regime. Further, we propose a simple privacy
filter for composing pure ex-post DP guarantees, and use it to derive a fully
adaptive Gaussian Sparse Vector Technique mechanism. Finally, we provide
experiments on mobility and energy consumption datasets demonstrating that our
Sparse Vector Technique is practically competitive with previous approaches and
requires less hyper-parameter tuning.",http://arxiv.org/abs/2402.06137v1,83,18,16,18,16,15
GPT-4 Generated Narratives of Life Events using a Structured Narrative Prompt: A Validation Study,"Large Language Models (LLMs) play a pivotal role in generating vast arrays of
narratives, facilitating a systematic exploration of their effectiveness for
communicating life events in narrative form. In this study, we employ a
zero-shot structured narrative prompt to generate 24,000 narratives using
OpenAI's GPT-4. From this dataset, we manually classify 2,880 narratives and
evaluate their validity in conveying birth, death, hiring, and firing events.
Remarkably, 87.43% of the narratives sufficiently convey the intention of the
structured prompt. To automate the identification of valid and invalid
narratives, we train and validate nine Machine Learning models on the
classified datasets. Leveraging these models, we extend our analysis to predict
the classifications of the remaining 21,120 narratives. All the ML models
excelled at classifying valid narratives as valid, but experienced challenges
at simultaneously classifying invalid narratives as invalid. Our findings not
only advance the study of LLM capabilities, limitations, and validity but also
offer practical insights for narrative generation and natural language
processing applications.",http://arxiv.org/abs/2402.05435v1,82,16,18,16,16,16
Multi-Timescale Ensemble Q-learning for Markov Decision Process Policy Optimization,"Reinforcement learning (RL) is a classical tool to solve network control or
policy optimization problems in unknown environments. The original Q-learning
suffers from performance and complexity challenges across very large networks.
Herein, a novel model-free ensemble reinforcement learning algorithm which
adapts the classical Q-learning is proposed to handle these challenges for
networks which admit Markov decision process (MDP) models. Multiple Q-learning
algorithms are run on multiple, distinct, synthetically created and
structurally related Markovian environments in parallel; the outputs are fused
using an adaptive weighting mechanism based on the Jensen-Shannon divergence
(JSD) to obtain an approximately optimal policy with low complexity. The
theoretical justification of the algorithm, including the convergence of key
statistics and Q-functions are provided. Numerical results across several
network models show that the proposed algorithm can achieve up to 55% less
average policy error with up to 50% less runtime complexity than the
state-of-the-art Q-learning algorithms. Numerical results validate assumptions
made in the theoretical analysis.",http://arxiv.org/abs/2402.05476v1,82,16,18,18,16,14
Implicit Diffusion: Efficient Optimization through Stochastic Sampling,"We present a new algorithm to optimize distributions defined implicitly by
parameterized stochastic diffusions. Doing so allows us to modify the outcome
distribution of sampling processes by optimizing over their parameters. We
introduce a general framework for first-order optimization of these processes,
that performs jointly, in a single loop, optimization and sampling steps. This
approach is inspired by recent advances in bilevel optimization and automatic
implicit differentiation, leveraging the point of view of sampling as
optimization over the space of probability distributions. We provide
theoretical guarantees on the performance of our method, as well as
experimental results demonstrating its effectiveness in real-world settings.",http://arxiv.org/abs/2402.05468v1,82,18,16,16,18,14
Taking Training Seriously: Human Guidance and Management-Based Regulation of Artificial Intelligence,"Fervent calls for more robust governance of the harms associated with
artificial intelligence (AI) are leading to the adoption around the world of
what regulatory scholars have called a management-based approach to regulation.
Recent initiatives in the United States and Europe, as well as the adoption of
major self-regulatory standards by the International Organization for
Standardization, share in common a core management-based paradigm. These
management-based initiatives seek to motivate an increase in human oversight of
how AI tools are trained and developed. Refinements and systematization of
human-guided training techniques will thus be needed to fit within this
emerging era of management-based regulatory paradigm. If taken seriously,
human-guided training can alleviate some of the technical and ethical pressures
on AI, boosting AI performance with human intuition as well as better
addressing the needs for fairness and effective explainability. In this paper,
we discuss the connection between the emerging management-based regulatory
frameworks governing AI and the need for human oversight during training. We
broadly cover some of the technical components involved in human-guided
training and then argue that the kinds of high-stakes use cases for AI that
appear of most concern to regulators should lean more on human-guided training
than on data-only training. We hope to foster a discussion between legal
scholars and computer scientists involving how to govern a domain of technology
that is vast, heterogenous, and dynamic in its applications and risks.",http://arxiv.org/abs/2402.08466v1,82,17,16,17,16,16
ROSpace: Intrusion Detection Dataset for a ROS2-Based Cyber-Physical System,"Most of the intrusion detection datasets to research machine learning-based
intrusion detection systems (IDSs) are devoted to cyber-only systems, and they
typically collect data from one architectural layer. Additionally, often the
attacks are generated in dedicated attack sessions, without reproducing the
realistic alternation and overlap of normal and attack actions. We present a
dataset for intrusion detection by performing penetration testing on an
embedded cyber-physical system built over Robot Operating System 2 (ROS2).
Features are monitored from three architectural layers: the Linux operating
system, the network, and the ROS2 services. The dataset is structured as a time
series and describes the expected behavior of the system and its response to
ROS2-specific attacks: it repeatedly alternates periods of attack-free
operation with periods when a specific attack is being performed. Noteworthy,
this allows measuring the time to detect an attacker and the number of
malicious activities performed before detection. Also, it allows training an
intrusion detector to minimize both, by taking advantage of the numerous
alternating periods of normal and attack operations.",http://arxiv.org/abs/2402.08468v1,82,16,18,16,18,14
Mitigating Privacy Risk in Membership Inference by Convex-Concave Loss,"Machine learning models are susceptible to membership inference attacks
(MIAs), which aim to infer whether a sample is in the training set. Existing
work utilizes gradient ascent to enlarge the loss variance of training data,
alleviating the privacy risk. However, optimizing toward a reverse direction
may cause the model parameters to oscillate near local minima, leading to
instability and suboptimal performance. In this work, we propose a novel method
-- Convex-Concave Loss, which enables a high variance of training loss
distribution by gradient descent. Our method is motivated by the theoretical
analysis that convex losses tend to decrease the loss variance during training.
Thus, our key idea behind CCL is to reduce the convexity of loss functions with
a concave term. Trained with CCL, neural networks produce losses with high
variance for training data, reinforcing the defense against MIAs. Extensive
experiments demonstrate the superiority of CCL, achieving state-of-the-art
balance in the privacy-utility trade-off.",http://arxiv.org/abs/2402.05453v1,82,17,16,18,16,15
What We Know About Using Non-Engagement Signals in Content Ranking,"Many online platforms predominantly rank items by predicted user engagement.
We believe that there is much unrealized potential in including non-engagement
signals, which can improve outcomes both for platforms and for society as a
whole. Based on a daylong workshop with experts from industry and academia, we
formulate a series of propositions and document each as best we can from public
evidence, including quantitative results where possible.
  There is strong evidence that ranking by predicted engagement is effective in
increasing user retention. However retention can be further increased by
incorporating other signals, including item ""quality"" proxies and asking users
what they want to see with ""item-level"" surveys. There is also evidence that
""diverse engagement"" is an effective quality signal. Ranking changes can alter
the prevalence of self-reported experiences of various kinds (e.g. harassment)
but seldom have large enough effects on attitude measures like user
satisfaction, well-being, polarization etc. to be measured in typical
experiments. User controls over ranking often have low usage rates, but when
used they do correlate well with quality and item-level surveys. There was no
strong evidence on the impact of transparency/explainability on retention.
There is reason to believe that generative AI could be used to create better
quality signals and enable new kinds of user controls.",http://arxiv.org/abs/2402.06831v1,82,16,16,18,18,14
Indicators for characterising online hate speech and its automatic detection,"We examined four case studies in the context of hate speech on Twitter in
Italian from 2019 to 2020, aiming at comparing the classification of the 3,600
tweets made by expert pedagogists with the automatic classification made by
machine learning algorithms. Pedagogists used a novel classification scheme
based on seven indicators that characterize hate. These indicators are: the
content is public, it affects a target group, it contains hate speech in
explicit verbal form, it will not redeem, it has intention to harm, it can have
a possible violent response, it incites hatred and violence. The case studies
refer to Jews, Muslims, Roma, and immigrants target groups. We find that not
all the types of hateful content are equally detectable by the machine learning
algorithms that we considered. In particular, algorithms perform better in
identifying tweets that incite hatred and violence, and those that can have
possible violent response.",http://arxiv.org/abs/2402.08462v1,82,18,17,17,16,14
Refining Myocardial Infarction Detection: A Novel Multi-Modal Composite Kernel Strategy in One-Class Classification,"Early detection of myocardial infarction (MI), a critical condition arising
from coronary artery disease (CAD), is vital to prevent further myocardial
damage. This study introduces a novel method for early MI detection using a
one-class classification (OCC) algorithm in echocardiography. Our study
overcomes the challenge of limited echocardiography data availability by
adopting a novel approach based on Multi-modal Subspace Support Vector Data
Description. The proposed technique involves a specialized MI detection
framework employing multi-view echocardiography incorporating a composite
kernel in the non-linear projection trick, fusing Gaussian and Laplacian
sigmoid functions. Additionally, we enhance the update strategy of the
projection matrices by adapting maximization for both or one of the modalities
in the optimization process. Our method boosts MI detection capability by
efficiently transforming features extracted from echocardiography data into an
optimized lower-dimensional subspace. The OCC model trained specifically on
target class instances from the comprehensive HMC-QU dataset that includes
multiple echocardiography views indicates a marked improvement in MI detection
accuracy. Our findings reveal that our proposed multi-view approach achieves a
geometric mean of 71.24\%, signifying a substantial advancement in
echocardiography-based MI diagnosis and offering more precise and efficient
diagnostic tools.",http://arxiv.org/abs/2402.06530v1,82,16,18,18,16,14
Diff-RNTraj: A Structure-aware Diffusion Model for Road Network-constrained Trajectory Generation,"Trajectory data is essential for various applications as it records the
movement of vehicles. However, publicly available trajectory datasets remain
limited in scale due to privacy concerns, which hinders the development of
trajectory data mining and trajectory-based applications. To address this
issue, some methods for generating synthetic trajectories have been proposed to
expand the scale of the dataset. However, all existing methods generate
trajectories in the geographical coordinate system, which poses two limitations
for their utilization in practical applications: 1) the inability to ensure
that the generated trajectories are constrained on the road. 2) the lack of
road-related information. In this paper, we propose a new problem to meet the
practical application need, \emph{i.e.}, road network-constrained trajectory
(RNTraj) generation, which can directly generate trajectories on the road
network with road-related information. RNTraj is a hybrid type of data, in
which each point is represented by a discrete road segment and a continuous
moving rate. To generate RNTraj, we design a diffusion model called
Diff-RNTraj. This model can effectively handle the hybrid RNTraj using a
continuous diffusion framework by incorporating a pre-training strategy to
embed hybrid RNTraj into continuous representations. During the sampling stage,
a RNTraj decoder is designed to map the continuous representation generated by
the diffusion model back to the hybrid RNTraj format. Furthermore, Diff-RNTraj
introduces a novel loss function to enhance the spatial validity of the
generated trajectories. Extensive experiments conducted on two real-world
trajectory datasets demonstrate the effectiveness of the proposed model.",http://arxiv.org/abs/2402.07369v1,82,18,16,16,18,14
Privacy-preserving data release leveraging optimal transport and particle gradient descent,"We present a novel approach for differentially private data synthesis of
protected tabular datasets, a relevant task in highly sensitive domains such as
healthcare and government. Current state-of-the-art methods predominantly use
marginal-based approaches, where a dataset is generated from private estimates
of the marginals. In this paper, we introduce PrivPGD, a new generation method
for marginal-based private data synthesis, leveraging tools from optimal
transport and particle gradient descent. Our algorithm outperforms existing
methods on a large range of datasets while being highly scalable and offering
the flexibility to incorporate additional domain-specific constraints.",http://arxiv.org/abs/2401.17823v2,82,16,18,18,16,14
Mixtures of Experts Unlock Parameter Scaling for Deep RL,"The recent rapid progress in (self) supervised learning models is in large
part predicted by empirical scaling laws: a model's performance scales
proportionally to its size. Analogous scaling laws remain elusive for
reinforcement learning domains, however, where increasing the parameter count
of a model often hurts its final performance. In this paper, we demonstrate
that incorporating Mixture-of-Expert (MoE) modules, and in particular Soft MoEs
(Puigcerver et al., 2023), into value-based networks results in more
parameter-scalable models, evidenced by substantial performance increases
across a variety of training regimes and model sizes. This work thus provides
strong empirical evidence towards developing scaling laws for reinforcement
learning.",http://arxiv.org/abs/2402.08609v1,82,16,18,18,14,16
INViT: A Generalizable Routing Problem Solver with Invariant Nested View Transformer,"Recently, deep reinforcement learning has shown promising results for
learning fast heuristics to solve routing problems. Meanwhile, most of the
solvers suffer from generalizing to an unseen distribution or distributions
with different scales. To address this issue, we propose a novel architecture,
called Invariant Nested View Transformer (INViT), which is designed to enforce
a nested design together with invariant views inside the encoders to promote
the generalizability of the learned solver. It applies a modified policy
gradient algorithm enhanced with data augmentations. We demonstrate that the
proposed INViT achieves a dominant generalization performance on both TSP and
CVRP problems with various distributions and different problem scales.",http://arxiv.org/abs/2402.02317v2,82,17,18,16,17,14
Solving Deep Reinforcement Learning Benchmarks with Linear Policy Networks,"Although Deep Reinforcement Learning (DRL) methods can learn effective
policies for challenging problems such as Atari games and robotics tasks,
algorithms are complex and training times are often long. This study
investigates how evolution strategies (ES) perform compared to gradient-based
deep reinforcement learning methods. We use ES to optimize the weights of a
neural network via neuroevolution, performing direct policy search. We
benchmark both regular networks and policy networks consisting of a single
linear layer from observations to actions; for three classical ES methods and
for three gradient-based methods such as PPO. Our results reveal that ES can
find effective linear policies for many RL benchmark tasks, in contrast to DRL
methods that can only find successful policies using much larger networks,
suggesting that current benchmarks are easier to solve than previously assumed.
Interestingly, also for higher complexity tasks, ES achieves results comparable
to gradient-based DRL algorithms. Furthermore, we find that by directly
accessing the memory state of the game, ES are able to find successful policies
in Atari, outperforming DQN. While gradient-based methods have dominated the
field in recent years, ES offers an alternative that is easy to implement,
parallelize, understand, and tune.",http://arxiv.org/abs/2402.06912v1,82,18,17,17,16,14
SQT -- std $Q$-target,"Std $Q$-target is a conservative, actor-critic, ensemble, $Q$-learning-based
algorithm, which is based on a single key $Q$-formula: $Q$-networks standard
deviation, which is an ""uncertainty penalty"", and, serves as a minimalistic
solution to the problem of overestimation bias. We implement SQT on top of
TD3/TD7 code and test it against the state-of-the-art (SOTA) actor-critic
algorithms, DDPG, TD3 and TD7 on seven popular MuJoCo and Bullet tasks. Our
results demonstrate SQT's $Q$-target formula superiority over TD3's $Q$-target
formula as a conservative solution to overestimation bias in RL, while SQT
shows a clear performance advantage on a wide margin over DDPG, TD3, and TD7 on
all tasks.",http://arxiv.org/abs/2402.05950v2,82,16,18,16,16,16
Score-based Diffusion Models via Stochastic Differential Equations -- a Technical Tutorial,"This is an expository article on the score-based diffusion models, with a
particular focus on the formulation via stochastic differential equations
(SDE). After a gentle introduction, we discuss the two pillars in the diffusion
modeling -- sampling and score matching, which encompass the SDE/ODE sampling,
score matching efficiency, the consistency model, and reinforcement learning.
Short proofs are given to illustrate the main idea of the stated results. The
article is primarily for introducing the beginners to the field, and
practitioners may also find some analysis useful in designing new models or
algorithms.",http://arxiv.org/abs/2402.07487v1,82,16,18,18,16,14
Leveraging Digital Cousins for Ensemble Q-Learning in Large-Scale Wireless Networks,"Optimizing large-scale wireless networks, including optimal resource
management, power allocation, and throughput maximization, is inherently
challenging due to their non-observable system dynamics and heterogeneous and
complex nature. Herein, a novel ensemble Q-learning algorithm that addresses
the performance and complexity challenges of the traditional Q-learning
algorithm for optimizing wireless networks is presented. Ensemble learning with
synthetic Markov Decision Processes is tailored to wireless networks via new
models for approximating large state-space observable wireless networks. In
particular, digital cousins are proposed as an extension of the traditional
digital twin concept wherein multiple Q-learning algorithms on multiple
synthetic Markovian environments are run in parallel and their outputs are
fused into a single Q-function. Convergence analyses of key statistics and
Q-functions and derivations of upper bounds on the estimation bias and variance
are provided. Numerical results across a variety of real-world wireless
networks show that the proposed algorithm can achieve up to 50% less average
policy error with up to 40% less runtime complexity than the state-of-the-art
reinforcement learning algorithms. It is also shown that theoretical results
properly predict trends in the experimental results.",http://arxiv.org/abs/2402.08022v1,82,18,16,18,14,16
Latent Enhancing AutoEncoder for Occluded Image Classification,"Large occlusions result in a significant decline in image classification
accuracy. During inference, diverse types of unseen occlusions introduce
out-of-distribution data to the classification model, leading to accuracy
dropping as low as 50%. As occlusions encompass spatially connected regions,
conventional methods involving feature reconstruction are inadequate for
enhancing classification performance. We introduce LEARN: Latent Enhancing
feAture Reconstruction Network -- An auto-encoder based network that can be
incorporated into the classification model before its classifier head without
modifying the weights of classification model. In addition to reconstruction
and classification losses, training of LEARN effectively combines intra- and
inter-class losses calculated over its latent space -- which lead to
improvement in recovering latent space of occluded data, while preserving its
class-specific discriminative information. On the OccludedPASCAL3D+ dataset,
the proposed LEARN outperforms standard classification models (VGG16 and
ResNet-50) by a large margin and up to 2% over state-of-the-art methods. In
cross-dataset testing, our method improves the average classification accuracy
by more than 5% over the state-of-the-art methods. In every experiment, our
model consistently maintains excellent accuracy on in-distribution data.",http://arxiv.org/abs/2402.06936v1,82,18,17,17,15,15
Adaptive Activation Functions for Predictive Modeling with Sparse Experimental Data,"A pivotal aspect in the design of neural networks lies in selecting
activation functions, crucial for introducing nonlinear structures that capture
intricate input-output patterns. While the effectiveness of adaptive or
trainable activation functions has been studied in domains with ample data,
like image classification problems, significant gaps persist in understanding
their influence on classification accuracy and predictive uncertainty in
settings characterized by limited data availability. This research aims to
address these gaps by investigating the use of two types of adaptive activation
functions. These functions incorporate shared and individual trainable
parameters per hidden layer and are examined in three testbeds derived from
additive manufacturing problems containing fewer than one hundred training
instances. Our investigation reveals that adaptive activation functions, such
as Exponential Linear Unit (ELU) and Softplus, with individual trainable
parameters, result in accurate and confident prediction models that outperform
fixed-shape activation functions and the less flexible method of using
identical trainable activation functions in a hidden layer. Therefore, this
work presents an elegant way of facilitating the design of adaptive neural
networks in scientific and engineering problems.",http://arxiv.org/abs/2402.05401v1,82,16,18,18,16,14
Show Me How It's Done: The Role of Explanations in Fine-Tuning Language Models,"Our research demonstrates the significant benefits of using fine-tuning with
explanations to enhance the performance of language models. Unlike prompting,
which maintains the model's parameters, fine-tuning allows the model to learn
and update its parameters during a training phase. In this study, we applied
fine-tuning to various sized language models using data that contained
explanations of the output rather than merely presenting the answers. We found
that even smaller language models with as few as 60 million parameters
benefited substantially from this approach. Interestingly, our results
indicated that the detailed explanations were more beneficial to smaller models
than larger ones, with the latter gaining nearly the same advantage from any
form of explanation, irrespective of its length. Additionally, we demonstrate
that the inclusion of explanations enables the models to solve tasks that they
were not able to solve without explanations. Lastly, we argue that despite the
challenging nature of adding explanations, samples that contain explanations
not only reduce the volume of data required for training but also promote a
more effective generalization by the model. In essence, our findings suggest
that fine-tuning with explanations significantly bolsters the performance of
large language models.",http://arxiv.org/abs/2402.07543v1,82,18,16,18,16,14
VIALM: A Survey and Benchmark of Visually Impaired Assistance with Large Models,"Visually Impaired Assistance (VIA) aims to automatically help the visually
impaired (VI) handle daily activities. The advancement of VIA primarily depends
on developments in Computer Vision (CV) and Natural Language Processing (NLP),
both of which exhibit cutting-edge paradigms with large models (LMs).
Furthermore, LMs have shown exceptional multimodal abilities to tackle
challenging physically-grounded tasks such as embodied robots. To investigate
the potential and limitations of state-of-the-art (SOTA) LMs' capabilities in
VIA applications, we present an extensive study for the task of VIA with LMs
(VIALM). In this task, given an image illustrating the physical environments
and a linguistic request from a VI user, VIALM aims to output step-by-step
guidance to assist the VI user in fulfilling the request grounded in the
environment. The study consists of a survey reviewing recent LM research and
benchmark experiments examining selected LMs' capabilities in VIA. The results
indicate that while LMs can potentially benefit VIA, their output cannot be
well environment-grounded (i.e., 25.7% GPT-4's responses) and lacks
fine-grained guidance (i.e., 32.1% GPT-4's responses).",http://arxiv.org/abs/2402.01735v2,82,18,18,16,16,14
Model Assessment and Selection under Temporal Distribution Shift,"We investigate model assessment and selection in a changing environment, by
synthesizing datasets from both the current time period and historical epochs.
To tackle unknown and potentially arbitrary temporal distribution shift, we
develop an adaptive rolling window approach to estimate the generalization
error of a given model. This strategy also facilitates the comparison between
any two candidate models by estimating the difference of their generalization
errors. We further integrate pairwise comparisons into a single-elimination
tournament, achieving near-optimal model selection from a collection of
candidates. Theoretical analyses and numerical experiments demonstrate the
adaptivity of our proposed methods to the non-stationarity in data.",http://arxiv.org/abs/2402.08672v1,82,16,18,18,16,14
A Multi-Perspective Machine Learning Approach to Evaluate Police-Driver Interaction in Los Angeles,"Interactions between the government officials and civilians affect public
wellbeing and the state legitimacy that is necessary for the functioning of
democratic society. Police officers, the most visible and contacted agents of
the state, interact with the public more than 20 million times a year during
traffic stops. Today, these interactions are regularly recorded by body-worn
cameras (BWCs), which are lauded as a means to enhance police accountability
and improve police-public interactions. However, the timely analysis of these
recordings is hampered by a lack of reliable automated tools that can enable
the analysis of these complex and contested police-public interactions. This
article proposes an approach to developing new multi-perspective, multimodal
machine learning (ML) tools to analyze the audio, video, and transcript
information from this BWC footage. Our approach begins by identifying the
aspects of communication most salient to different stakeholders, including both
community members and police officers. We move away from modeling approaches
built around the existence of a single ground truth and instead utilize new
advances in soft labeling to incorporate variation in how different observers
perceive the same interactions. We argue that this inclusive approach to the
conceptualization and design of new ML tools is broadly applicable to the study
of communication and development of analytic tools across domains of human
interaction, including education, medicine, and the workplace.",http://arxiv.org/abs/2402.01703v3,82,17,16,17,16,16
Towards Risk Analysis of the Impact of AI on the Deliberate Biological Threat Landscape,"The perception that the convergence of biological engineering and artificial
intelligence (AI) could enable increased biorisk has recently drawn attention
to the governance of biotechnology and artificial intelligence. The 2023
Executive Order, Executive Order on the Safe, Secure, and Trustworthy
Development and Use of Artificial Intelligence, requires an assessment of how
artificial intelligence can increase biorisk. Within this perspective, we
present a simplistic framework for evaluating biorisk and demonstrate how this
framework falls short in achieving actionable outcomes for a biorisk manager.
We then suggest a potential path forward that builds upon existing risk
characterization work and justify why characterization efforts of AI-enabled
tools for engineering biology is needed.",http://arxiv.org/abs/2401.12755v2,82,16,18,16,18,14
Quantifying Similarity: Text-Mining Approaches to Evaluate ChatGPT and Google Bard Content in Relation to BioMedical Literature,"Background: The emergence of generative AI tools, empowered by Large Language
Models (LLMs), has shown powerful capabilities in generating content. To date,
the assessment of the usefulness of such content, generated by what is known as
prompt engineering, has become an interesting research question. Objectives
Using the mean of prompt engineering, we assess the similarity and closeness of
such contents to real literature produced by scientists. Methods In this
exploratory analysis, (1) we prompt-engineer ChatGPT and Google Bard to
generate clinical content to be compared with literature counterparts, (2) we
assess the similarities of the contents generated by comparing them with
counterparts from biomedical literature. Our approach is to use text-mining
approaches to compare documents and associated bigrams and to use network
analysis to assess the terms' centrality. Results The experiments demonstrated
that ChatGPT outperformed Google Bard in cosine document similarity (38% to
34%), Jaccard document similarity (23% to 19%), TF-IDF bigram similarity (47%
to 41%), and term network centrality (degree and closeness). We also found new
links that emerged in ChatGPT bigram networks that did not exist in literature
bigram networks. Conclusions: The obtained similarity results show that ChatGPT
outperformed Google Bard in document similarity, bigrams, and degree and
closeness centrality. We also observed that ChatGPT offers linkage to terms
that are connected in the literature. Such connections could inspire asking
interesting questions and generate new hypotheses.",http://arxiv.org/abs/2402.05116v2,82,16,18,16,18,14
Taylor Videos for Action Recognition,"Effectively extracting motions from video is a critical and long-standing
problem for action recognition. This problem is very challenging because
motions (i) do not have an explicit form, (ii) have various concepts such as
displacement, velocity, and acceleration, and (iii) often contain noise caused
by unstable pixels. Addressing these challenges, we propose the Taylor video, a
new video format that highlights the dominate motions (e.g., a waving hand) in
each of its frames named the Taylor frame. Taylor video is named after Taylor
series, which approximates a function at a given point using important terms.
In the scenario of videos, we define an implicit motion-extraction function
which aims to extract motions from video temporal block. In this block, using
the frames, the difference frames, and higher-order difference frames, we
perform Taylor expansion to approximate this function at the starting frame. We
show the summation of the higher-order terms in the Taylor series gives us
dominant motion patterns, where static objects, small and unstable motions are
removed. Experimentally we show that Taylor videos are effective inputs to
popular architectures including 2D CNNs, 3D CNNs, and transformers. When used
individually, Taylor videos yield competitive action recognition accuracy
compared to RGB videos and optical flow. When fused with RGB or optical flow
videos, further accuracy improvement is achieved.",http://arxiv.org/abs/2402.03019v3,82,16,18,16,16,16
Score-Based Physics-Informed Neural Networks for High-Dimensional Fokker-Planck Equations,"The Fokker-Planck (FP) equation is a foundational PDE in stochastic
processes. However, curse of dimensionality (CoD) poses challenge when dealing
with high-dimensional FP PDEs. Although Monte Carlo and vanilla
Physics-Informed Neural Networks (PINNs) have shown the potential to tackle
CoD, both methods exhibit numerical errors in high dimensions when dealing with
the probability density function (PDF) associated with Brownian motion. The
point-wise PDF values tend to decrease exponentially as dimension increases,
surpassing the precision of numerical simulations and resulting in substantial
errors. Moreover, due to its massive sampling, Monte Carlo fails to offer fast
sampling. Modeling the logarithm likelihood (LL) via vanilla PINNs transforms
the FP equation into a difficult HJB equation, whose error grows rapidly with
dimension. To this end, we propose a novel approach utilizing a score-based
solver to fit the score function in SDEs. The score function, defined as the
gradient of the LL, plays a fundamental role in inferring LL and PDF and
enables fast SDE sampling. Three fitting methods, Score Matching (SM), Sliced
SM (SSM), and Score-PINN, are introduced. The proposed score-based SDE solver
operates in two stages: first, employing SM, SSM, or Score-PINN to acquire the
score; and second, solving the LL via an ODE using the obtained score.
Comparative evaluations across these methods showcase varying trade-offs. The
proposed method is evaluated across diverse SDEs, including anisotropic OU
processes, geometric Brownian, and Brownian with varying eigenspace. We also
test various distributions, including Gaussian, Log-normal, Laplace, and
Cauchy. The numerical results demonstrate the score-based SDE solver's
stability, speed, and performance across different settings, solidifying its
potential as a solution to CoD for high-dimensional FP equations.",http://arxiv.org/abs/2402.07465v1,82,16,18,18,16,14
Optimization of Sparse Convolution for 3D-Point Cloud on GPUs with CUDA,"In recent years, there has been a significant increase in the utilization of
deep learning methods, particularly convolutional neural networks (CNNs), which
have emerged as the dominant approach in various domains that involve
structured grid data, such as picture analysis and processing. Nevertheless,
the exponential growth in the utilization of LiDAR and 3D sensors across many
domains has resulted in an increased need for the analysis of 3D point clouds.
The utilization of 3D point clouds is crucial in various applications,
including object recognition and segmentation, as they offer a spatial
depiction of things within a three-dimensional environment. In contrast to
photos, point clouds exhibit sparsity and lack a regular grid, hence posing
distinct processing and computational issues.",http://arxiv.org/abs/2402.07710v1,82,18,16,18,16,14
On the Distance from Calibration in Sequential Prediction,"We study a sequential binary prediction setting where the forecaster is
evaluated in terms of the calibration distance, which is defined as the $L_1$
distance between the predicted values and the set of predictions that are
perfectly calibrated in hindsight. This is analogous to a calibration measure
recently proposed by B{\l}asiok, Gopalan, Hu and Nakkiran (STOC 2023) for the
offline setting. The calibration distance is a natural and intuitive measure of
deviation from perfect calibration, and satisfies a Lipschitz continuity
property which does not hold for many popular calibration measures, such as the
$L_1$ calibration error and its variants.
  We prove that there is a forecasting algorithm that achieves an $O(\sqrt{T})$
calibration distance in expectation on an adversarially chosen sequence of $T$
binary outcomes. At the core of this upper bound is a structural result showing
that the calibration distance is accurately approximated by the lower
calibration distance, which is a continuous relaxation of the former. We then
show that an $O(\sqrt{T})$ lower calibration distance can be achieved via a
simple minimax argument and a reduction to online learning on a Lipschitz
class.
  On the lower bound side, an $\Omega(T^{1/3})$ calibration distance is shown
to be unavoidable, even when the adversary outputs a sequence of independent
random bits, and has an additional ability to early stop (i.e., to stop
producing random bits and output the same bit in the remaining steps).
Interestingly, without this early stopping, the forecaster can achieve a much
smaller calibration distance of $\mathrm{polylog}(T)$.",http://arxiv.org/abs/2402.07458v1,82,18,16,18,16,14
Test-Time Adaptation for Depth Completion,"It is common to observe performance degradation when transferring models
trained on some (source) datasets to target testing data due to a domain gap
between them. Existing methods for bridging this gap, such as domain adaptation
(DA), may require the source data on which the model was trained (often not
available), while others, i.e., source-free DA, require many passes through the
testing data. We propose an online test-time adaptation method for depth
completion, the task of inferring a dense depth map from a single image and
associated sparse depth map, that closes the performance gap in a single pass.
We first present a study on how the domain shift in each data modality affects
model performance. Based on our observations that the sparse depth modality
exhibits a much smaller covariate shift than the image, we design an embedding
module trained in the source domain that preserves a mapping from features
encoding only sparse depth to those encoding image and sparse depth. During
test time, sparse depth features are projected using this map as a proxy for
source domain features and are used as guidance to train a set of auxiliary
parameters (i.e., adaptation layer) to align image and sparse depth features
from the target test domain to that of the source domain. We evaluate our
method on indoor and outdoor scenarios and show that it improves over baselines
by an average of 21.1%.",http://arxiv.org/abs/2402.03312v2,82,18,18,16,16,14
Model Collapse Demystified: The Case of Regression,"In the era of large language models like ChatGPT, the phenomenon of ""model
collapse"" refers to the situation whereby as a model is trained recursively on
data generated from previous generations of itself over time, its performance
degrades until the model eventually becomes completely useless, i.e the model
collapses. In this work, we study this phenomenon in the simplified setting of
kernel regression and obtain results which show a clear crossover between where
the model can cope with fake data, and a regime where the model's performance
completely collapses. Under polynomial decaying spectral and source conditions,
we obtain modified scaling laws which exhibit new crossover phenomena from fast
to slow rates. We also propose a simple strategy based on adaptive
regularization to mitigate model collapse. Our theoretical results are
validated with experiments.",http://arxiv.org/abs/2402.07712v1,82,17,16,18,16,15
Faster Repeated Evasion Attacks in Tree Ensembles,"Tree ensembles are one of the most widely used model classes. However, these
models are susceptible to adversarial examples, i.e., slightly perturbed
examples that elicit a misprediction. There has been significant research on
designing approaches to construct such examples for tree ensembles. But this is
a computationally challenging problem that often must be solved a large number
of times (e.g., for all examples in a training set). This is compounded by the
fact that current approaches attempt to find such examples from scratch. In
contrast, we exploit the fact that multiple similar problems are being solved.
Specifically, our approach exploits the insight that adversarial examples for
tree ensembles tend to perturb a consistent but relatively small set of
features. We show that we can quickly identify this set of features and use
this knowledge to speedup constructing adversarial examples.",http://arxiv.org/abs/2402.08586v1,82,16,18,16,16,16
TriAug: Out-of-Distribution Detection for Robust Classification of Imbalanced Breast Lesion in Ultrasound,"Different diseases, such as histological subtypes of breast lesions, have
severely varying incidence rates. Even trained with substantial amount of
in-distribution (ID) data, models often encounter out-of-distribution (OOD)
samples belonging to unseen classes in clinical reality. To address this, we
propose a novel framework built upon a long-tailed OOD detection task for
breast ultrasound images. It is equipped with a triplet state augmentation
(TriAug) which improves ID classification accuracy while maintaining a
promising OOD detection performance. Meanwhile, we designed a balanced sphere
loss to handle the class imbalanced problem.",http://arxiv.org/abs/2402.07452v1,82,16,18,16,16,16
Densely Multiplied Physics Informed Neural Networks,"Although physics-informed neural networks (PINNs) have shown great potential
in dealing with nonlinear partial differential equations (PDEs), it is common
that PINNs will suffer from the problem of insufficient precision or obtaining
incorrect outcomes. Unlike most of the existing solutions trying to enhance the
ability of PINN by optimizing the training process, this paper improved the
neural network architecture to improve the performance of PINN. We propose a
densely multiply PINN (DM-PINN) architecture, which multiplies the output of a
hidden layer with the outputs of all the behind hidden layers. Without
introducing more trainable parameters, this effective mechanism can
significantly improve the accuracy of PINNs. The proposed architecture is
evaluated on four benchmark examples (Allan-Cahn equation, Helmholtz equation,
Burgers equation and 1D convection equation). Comparisons between the proposed
architecture and different PINN structures demonstrate the superior performance
of the DM-PINN in both accuracy and efficiency.",http://arxiv.org/abs/2402.04390v2,82,16,18,18,16,14
SALAD: Smart AI Language Assistant Daily,"SALAD is an AI-driven language-learning application designed to help
foreigners learn Japanese. It offers translations in Kanji-Kana-Romaji, speech
recognition, translated audio, vocabulary tracking, grammar explanations, and
songs generated from newly learned words. The app targets beginners and
intermediate learners, aiming to make language acquisition more accessible and
enjoyable. SALAD uses daily translations to enhance fluency and comfort in
communication with native speakers. The primary objectives include effective
Japanese language learning, user engagement, and progress tracking. A survey by
us found that 39% of foreigners in Japan face discomfort in conversations with
Japanese speakers. Over 60% of foreigners expressed confidence in SALAD's
ability to enhance their Japanese language skills. The app uses large language
models, speech recognition, and diffusion models to bridge the language gap and
foster a more inclusive community in Japan.",http://arxiv.org/abs/2402.07431v2,82,16,18,16,18,14
Multimodal Learned Sparse Retrieval for Image Suggestion,"Learned Sparse Retrieval (LSR) is a group of neural methods designed to
encode queries and documents into sparse lexical vectors. These vectors can be
efficiently indexed and retrieved using an inverted index. While LSR has shown
promise in text retrieval, its potential in multi-modal retrieval remains
largely unexplored. Motivated by this, in this work, we explore the application
of LSR in the multi-modal domain, i.e., we focus on Multi-Modal Learned Sparse
Retrieval (MLSR). We conduct experiments using several MLSR model
configurations and evaluate the performance on the image suggestion task. We
find that solving the task solely based on the image content is challenging.
Enriching the image content with its caption improves the model performance
significantly, implying the importance of image captions to provide
fine-grained concepts and context information of images. Our approach presents
a practical and effective solution for training LSR retrieval models in
multi-modal settings.",http://arxiv.org/abs/2402.07736v1,82,14,16,16,18,18
Conditional Generative Models are Sufficient to Sample from Any Causal Effect Estimand,"Causal inference from observational data has recently found many applications
in machine learning. While sound and complete algorithms exist to compute
causal effects, many of these algorithms require explicit access to conditional
likelihoods over the observational distribution, which is difficult to estimate
in the high-dimensional regime, such as with images. To alleviate this issue,
researchers have approached the problem by simulating causal relations with
neural models and obtained impressive results. However, none of these existing
approaches can be applied to generic scenarios such as causal graphs on image
data with latent confounders, or obtain conditional interventional samples. In
this paper, we show that any identifiable causal effect given an arbitrary
causal graph can be computed through push-forward computations of conditional
generative models. Based on this result, we devise a diffusion-based approach
to sample from any (conditional) interventional distribution on image data. To
showcase our algorithm's performance, we conduct experiments on a Colored MNIST
dataset having both the treatment ($X$) and the target variables ($Y$) as
images and obtain interventional samples from $P(y|do(x))$. As an application
of our algorithm, we evaluate two large conditional generative models that are
pre-trained on the CelebA dataset by analyzing the strength of spurious
correlations and the level of disentanglement they achieve.",http://arxiv.org/abs/2402.07419v1,82,15,18,16,18,15
MIML library: a Modular and Flexible Library for Multi-instance Multi-label Learning,"MIML library is a Java software tool to develop, test, and compare
classification algorithms for multi-instance multi-label (MIML) learning. The
library includes 43 algorithms and provides a specific format and facilities
for data managing and partitioning, holdout and cross-validation methods,
standard metrics for performance evaluation, and generation of reports. In
addition, algorithms can be executed through $xml$ configuration files without
needing to program. It is platform-independent, extensible, free, open-source,
and available on GitHub under the GNU General Public License.",http://dx.doi.org/10.1016/j.neucom.2022.05.068,82,16,18,16,18,14
Three Pathways to Neurosymbolic Reinforcement Learning with Interpretable Model and Policy Networks,"Neurosymbolic AI combines the interpretability, parsimony, and explicit
reasoning of classical symbolic approaches with the statistical learning of
data-driven neural approaches. Models and policies that are simultaneously
differentiable and interpretable may be key enablers of this marriage. This
paper demonstrates three pathways to implementing such models and policies in a
real-world reinforcement learning setting. Specifically, we study a broad class
of neural networks that build interpretable semantics directly into their
architecture. We reveal and highlight both the potential and the essential
difficulties of combining logic, simulation, and learning. One lesson is that
learning benefits from continuity and differentiability, but classical logic is
discrete and non-differentiable. The relaxation to real-valued, differentiable
representations presents a trade-off; the more learnable, the less
interpretable. Another lesson is that using logic in the context of a numerical
simulation involves a non-trivial mapping from raw (e.g., real-valued time
series) simulation data to logical predicates. Some open questions this note
exposes include: What are the limits of rule-based controllers, and how
learnable are they? Do the differentiable interpretable approaches discussed
here scale to large, complex, uncertain systems? Can we truly achieve
interpretability? We highlight these and other themes across the three
approaches.",http://arxiv.org/abs/2402.05307v1,82,16,18,18,16,14
On the Interaction between Software Engineers and Data Scientists when building Machine Learning-Enabled Systems,"In recent years, Machine Learning (ML) components have been increasingly
integrated into the core systems of organizations. Engineering such systems
presents various challenges from both a theoretical and practical perspective.
One of the key challenges is the effective interaction between actors with
different backgrounds who need to work closely together, such as software
engineers and data scientists. This paper presents an exploratory case study to
understand the current interaction and collaboration dynamics between these
roles in ML projects. We conducted semi-structured interviews with four
practitioners with experience in software engineering and data science of a
large ML-enabled system project and analyzed the data using reflexive thematic
analysis. Our findings reveal several challenges that can hinder collaboration
between software engineers and data scientists, including differences in
technical expertise, unclear definitions of each role's duties, and the lack of
documents that support the specification of the ML-enabled system. We also
indicate potential solutions to address these challenges, such as fostering a
collaborative culture, encouraging team communication, and producing concise
system documentation. This study contributes to understanding the complex
dynamics between software engineers and data scientists in ML projects and
provides insights for improving collaboration and communication in this
context. We encourage future studies investigating this interaction in other
projects.",http://arxiv.org/abs/2402.05334v1,82,14,16,18,18,16
Avoiding Catastrophe in Continuous Spaces by Asking for Help,"Most reinforcement learning algorithms with formal regret guarantees assume
all mistakes are reversible and rely on essentially trying all possible
options. This approach leads to poor outcomes when some mistakes are
irreparable or even catastrophic. We propose a variant of the contextual bandit
problem where the goal is to minimize the chance of catastrophe. Specifically,
we assume that the payoff each round represents the chance of avoiding
catastrophe that round, and try to maximize the product of payoffs (the overall
chance of avoiding catastrophe). To give the agent some chance of success, we
allow a limited number of queries to a mentor and assume a Lipschitz continuous
payoff function. We present an algorithm whose regret and rate of querying the
mentor both approach 0 as the time horizon grows, assuming a continuous 1D
state space and a relatively ""simple"" payoff function. We also provide a
matching lower bound: without the simplicity assumption: any algorithm either
constantly asks for help or is nearly guaranteed to cause catastrophe. Finally,
we identify the key obstacle to generalizing our algorithm to a
multi-dimensional state space.",http://arxiv.org/abs/2402.08062v1,82,18,15,18,18,13
CURE: Simulation-Augmented Auto-Tuning in Robotics,"Robotic systems are typically composed of various subsystems, such as
localization and navigation, each encompassing numerous configurable components
(e.g., selecting different planning algorithms). Once an algorithm has been
selected for a component, its associated configuration options must be set to
the appropriate values. Configuration options across the system stack interact
non-trivially. Finding optimal configurations for highly configurable robots to
achieve desired performance poses a significant challenge due to the
interactions between configuration options across software and hardware that
result in an exponentially large and complex configuration space. These
challenges are further compounded by the need for transferability between
different environments and robotic platforms. Data efficient optimization
algorithms (e.g., Bayesian optimization) have been increasingly employed to
automate the tuning of configurable parameters in cyber-physical systems.
However, such optimization algorithms converge at later stages, often after
exhausting the allocated budget (e.g., optimization steps, allotted time) and
lacking transferability. This paper proposes CURE -- a method that identifies
causally relevant configuration options, enabling the optimization process to
operate in a reduced search space, thereby enabling faster optimization of
robot performance. CURE abstracts the causal relationships between various
configuration options and robot performance objectives by learning a causal
model in the source (a low-cost environment such as the Gazebo simulator) and
applying the learned knowledge to perform optimization in the target (e.g.,
Turtlebot 3 physical robot). We demonstrate the effectiveness and
transferability of CURE by conducting experiments that involve varying degrees
of deployment changes in both physical robots and simulation.",http://arxiv.org/abs/2402.05399v1,82,16,18,18,16,14
Principled Penalty-based Methods for Bilevel Reinforcement Learning and RLHF,"Bilevel optimization has been recently applied to many machine learning
tasks. However, their applications have been restricted to the supervised
learning setting, where static objective functions with benign structures are
considered. But bilevel problems such as incentive design, inverse
reinforcement learning (RL), and RL from human feedback (RLHF) are often
modeled as dynamic objective functions that go beyond the simple static
objective structures, which pose significant challenges of using existing
bilevel solutions. To tackle this new class of bilevel problems, we introduce
the first principled algorithmic framework for solving bilevel RL problems
through the lens of penalty formulation. We provide theoretical studies of the
problem landscape and its penalty-based (policy) gradient algorithms. We
demonstrate the effectiveness of our algorithms via simulations in the
Stackelberg Markov game, RL from human feedback and incentive design.",http://arxiv.org/abs/2402.06886v1,82,18,17,17,15,15
Which Frequencies do CNNs Need? Emergent Bottleneck Structure in Feature Learning,"We describe the emergence of a Convolution Bottleneck (CBN) structure in
CNNs, where the network uses its first few layers to transform the input
representation into a representation that is supported only along a few
frequencies and channels, before using the last few layers to map back to the
outputs. We define the CBN rank, which describes the number and type of
frequencies that are kept inside the bottleneck, and partially prove that the
parameter norm required to represent a function $f$ scales as depth times the
CBN rank $f$. We also show that the parameter norm depends at next order on the
regularity of $f$. We show that any network with almost optimal parameter norm
will exhibit a CBN structure in both the weights and - under the assumption
that the network is stable under large learning rate - the activations, which
motivates the common practice of down-sampling; and we verify that the CBN
results still hold with down-sampling. Finally we use the CBN structure to
interpret the functions learned by CNNs on a number of tasks.",http://arxiv.org/abs/2402.08010v1,82,16,17,17,16,16
Convergence of Gradient Descent with Small Initialization for Unregularized Matrix Completion,"We study the problem of symmetric matrix completion, where the goal is to
reconstruct a positive semidefinite matrix $\rm{X}^\star \in
\mathbb{R}^{d\times d}$ of rank-$r$, parameterized by $\rm{U}\rm{U}^{\top}$,
from only a subset of its observed entries. We show that the vanilla gradient
descent (GD) with small initialization provably converges to the ground truth
$\rm{X}^\star$ without requiring any explicit regularization. This convergence
result holds true even in the over-parameterized scenario, where the true rank
$r$ is unknown and conservatively over-estimated by a search rank $r'\gg r$.
The existing results for this problem either require explicit regularization, a
sufficiently accurate initial point, or exact knowledge of the true rank $r$.
  In the over-parameterized regime where $r'\geq r$, we show that, with
$\widetilde\Omega(dr^9)$ observations, GD with an initial point $\|\rm{U}_0\|
\leq \epsilon$ converges near-linearly to an $\epsilon$-neighborhood of
$\rm{X}^\star$. Consequently, smaller initial points result in increasingly
accurate solutions. Surprisingly, neither the convergence rate nor the final
accuracy depends on the over-parameterized search rank $r'$, and they are only
governed by the true rank $r$. In the exactly-parameterized regime where
$r'=r$, we further enhance this result by proving that GD converges at a faster
rate to achieve an arbitrarily small accuracy $\epsilon>0$, provided the
initial point satisfies $\|\rm{U}_0\| = O(1/d)$. At the crux of our method lies
a novel weakly-coupled leave-one-out analysis, which allows us to establish the
global convergence of GD, extending beyond what was previously possible using
the classical leave-one-out analysis.",http://arxiv.org/abs/2402.06756v1,82,18,16,18,16,14
In-Context Learning Can Re-learn Forbidden Tasks,"Despite significant investment into safety training, large language models
(LLMs) deployed in the real world still suffer from numerous vulnerabilities.
One perspective on LLM safety training is that it algorithmically forbids the
model from answering toxic or harmful queries. To assess the effectiveness of
safety training, in this work, we study forbidden tasks, i.e., tasks the model
is designed to refuse to answer. Specifically, we investigate whether
in-context learning (ICL) can be used to re-learn forbidden tasks despite the
explicit fine-tuning of the model to refuse them. We first examine a toy
example of refusing sentiment classification to demonstrate the problem. Then,
we use ICL on a model fine-tuned to refuse to summarise made-up news articles.
Finally, we investigate whether ICL can undo safety training, which could
represent a major security risk. For the safety task, we look at Vicuna-7B,
Starling-7B, and Llama2-7B. We show that the attack works out-of-the-box on
Starling-7B and Vicuna-7B but fails on Llama2-7B. Finally, we propose an ICL
attack that uses the chat template tokens like a prompt injection attack to
achieve a better attack success rate on Vicuna-7B and Starling-7B.
  Trigger Warning: the appendix contains LLM-generated text with violence,
suicide, and misinformation.",http://arxiv.org/abs/2402.05723v1,82,18,16,18,16,14
MAIDCRL: Semi-centralized Multi-Agent Influence Dense-CNN Reinforcement Learning,"Distributed decision-making in multi-agent systems presents difficult
challenges for interactive behavior learning in both cooperative and
competitive systems. To mitigate this complexity, MAIDRL presents a
semi-centralized Dense Reinforcement Learning algorithm enhanced by agent
influence maps (AIMs), for learning effective multi-agent control on StarCraft
Multi-Agent Challenge (SMAC) scenarios. In this paper, we extend the DenseNet
in MAIDRL and introduce semi-centralized Multi-Agent Dense-CNN Reinforcement
Learning, MAIDCRL, by incorporating convolutional layers into the deep model
architecture, and evaluate the performance on both homogeneous and
heterogeneous scenarios. The results show that the CNN-enabled MAIDCRL
significantly improved the learning performance and achieved a faster learning
rate compared to the existing MAIDRL, especially on more complicated
heterogeneous SMAC scenarios. We further investigate the stability and
robustness of our model. The statistics reflect that our model not only
achieves higher winning rate in all the given scenarios but also boosts the
agent's learning process in fine-grained decision-making.",http://dx.doi.org/10.1109/CoG51982.2022.9893711,82,16,18,18,16,14
Integrating MLSecOps in the Biotechnology Industry 5.0,"Biotechnology Industry 5.0 is advancing with the integration of cutting-edge
technologies like Machine Learning (ML), the Internet Of Things (IoT), and
cloud computing. It is no surprise that an industry that utilizes data from
customers and can alter their lives is a target of a variety of attacks. This
chapter provides a perspective of how Machine Learning Security Operations
(MLSecOps) can help secure the biotechnology Industry 5.0. The chapter provides
an analysis of the threats in the biotechnology Industry 5.0 and how ML
algorithms can help secure with industry best practices. This chapter explores
the scope of MLSecOps in the biotechnology Industry 5.0, highlighting how
crucial it is to comply with current regulatory frameworks. With biotechnology
Industry 5.0 developing innovative solutions in healthcare, supply chain
management, biomanufacturing, pharmaceuticals sectors, and more, the chapter
also discusses the MLSecOps best practices that industry and enterprises should
follow while also considering ethical responsibilities. Overall, the chapter
provides a discussion of how to integrate MLSecOps into the design, deployment,
and regulation of the processes in biotechnology Industry 5.0.",http://arxiv.org/abs/2402.07967v1,82,16,16,18,18,14
Using Graph Theory for Improving Machine Learning-based Detection of Cyber Attacks,"Early detection of network intrusions and cyber threats is one of the main
pillars of cybersecurity. One of the most effective approaches for this purpose
is to analyze network traffic with the help of artificial intelligence
algorithms, with the aim of detecting the possible presence of an attacker by
distinguishing it from a legitimate user. This is commonly done by collecting
the traffic exchanged between terminals in a network and analyzing it on a
per-packet or per-connection basis. In this paper, we propose instead to
perform pre-processing of network traffic under analysis with the aim of
extracting some new metrics on which we can perform more efficient detection
and overcome some limitations of classical approaches. These new metrics are
based on graph theory, and consider the network as a whole, rather than
focusing on individual packets or connections. Our approach is validated
through experiments performed on publicly available data sets, from which it
results that it can not only overcome some of the limitations of classical
approaches, but also achieve a better detection capability of cyber threats.",http://arxiv.org/abs/2402.07878v1,82,16,18,16,18,14
Learning using privileged information for segmenting tumors on digital mammograms,"Limited amount of data and data sharing restrictions, due to GDPR compliance,
constitute two common factors leading to reduced availability and accessibility
when referring to medical data. To tackle these issues, we introduce the
technique of Learning Using Privileged Information. Aiming to substantiate the
idea, we attempt to build a robust model that improves the segmentation quality
of tumors on digital mammograms, by gaining privileged information knowledge
during the training procedure. Towards this direction, a baseline model, called
student, is trained on patches extracted from the original mammograms, while an
auxiliary model with the same architecture, called teacher, is trained on the
corresponding enhanced patches accessing, in this way, privileged information.
We repeat the student training procedure by providing the assistance of the
teacher model this time. According to the experimental results, it seems that
the proposed methodology performs better in the most of the cases and it can
achieve 10% higher F1 score in comparison with the baseline.",http://arxiv.org/abs/2402.06379v1,82,18,16,16,18,14
Learn to Teach: Improve Sample Efficiency in Teacher-student Learning for Sim-to-Real Transfer,"Simulation-to-reality (sim-to-real) transfer is a fundamental problem for
robot learning. Domain Randomization, which adds randomization during training,
is a powerful technique that effectively addresses the sim-to-real gap.
However, the noise in observations makes learning significantly harder.
Recently, studies have shown that employing a teacher-student learning paradigm
can accelerate training in randomized environments. Learned with privileged
information, a teacher agent can instruct the student agent to operate in noisy
environments. However, this approach is often not sample efficient as the
experience collected by the teacher is discarded completely when training the
student, wasting information revealed by the environment. In this work, we
extend the teacher-student learning paradigm by proposing a sample efficient
learning framework termed Learn to Teach (L2T) that recycles experience
collected by the teacher agent. We observe that the dynamics of the
environments for both agents remain unchanged, and the state space of the
teacher is coupled with the observation space of the student. We show that a
single-loop algorithm can train both the teacher and student agents under both
Reinforcement Learning and Inverse Reinforcement Learning contexts. We
implement variants of our methods, conduct experiments on the MuJoCo benchmark,
and apply our methods to the Cassie robot locomotion problem. Extensive
experiments show that our method achieves competitive performance while only
requiring environmental interaction with the teacher.",http://arxiv.org/abs/2402.06783v1,82,16,18,18,16,14
Injecting Wiktionary to improve token-level contextual representations using contrastive learning,"While static word embeddings are blind to context, for lexical semantics
tasks context is rather too present in contextual word embeddings, vectors of
same-meaning occurrences being too different (Ethayarajh, 2019). Fine-tuning
pre-trained language models (PLMs) using contrastive learning was proposed,
leveraging automatically self-augmented examples (Liu et al., 2021b). In this
paper, we investigate how to inject a lexicon as an alternative source of
supervision, using the English Wiktionary. We also test how dimensionality
reduction impacts the resulting contextual word embeddings. We evaluate our
approach on the Word-In-Context (WiC) task, in the unsupervised setting (not
using the training set). We achieve new SoTA result on the original WiC test
set. We also propose two new WiC test sets for which we show that our
fine-tuning method achieves substantial improvements. We also observe
improvements, although modest, for the semantic frame induction task. Although
we experimented on English to allow comparison with related work, our method is
adaptable to the many languages for which large Wiktionaries exist.",http://arxiv.org/abs/2402.07817v1,82,16,18,16,18,14
Multiple Instance Learning for Cheating Detection and Localization in Online Examinations,"The spread of the Coronavirus disease-2019 epidemic has caused many courses
and exams to be conducted online. The cheating behavior detection model in
examination invigilation systems plays a pivotal role in guaranteeing the
equality of long-distance examinations. However, cheating behavior is rare, and
most researchers do not comprehensively take into account features such as head
posture, gaze angle, body posture, and background information in the task of
cheating behavior detection. In this paper, we develop and present CHEESE, a
CHEating detection framework via multiplE inStancE learning. The framework
consists of a label generator that implements weak supervision and a feature
encoder to learn discriminative features. In addition, the framework combines
body posture and background features extracted by 3D convolution with eye gaze,
head posture and facial features captured by OpenFace 2.0. These features are
fed into the spatio-temporal graph module by stitching to analyze the
spatio-temporal changes in video clips to detect the cheating behaviors. Our
experiments on three datasets, UCF-Crime, ShanghaiTech and Online Exam
Proctoring (OEP), prove the effectiveness of our method as compared to the
state-of-the-art approaches, and obtain the frame-level AUC score of 87.58% on
the OEP dataset.",http://dx.doi.org/10.1109/TCDS.2024.3349705,82,18,16,16,16,16
The Duet of Representations and How Explanations Exacerbate It,"An algorithm effects a causal representation of relations between features
and labels in the human's perception. Such a representation might conflict with
the human's prior belief. Explanations can direct the human's attention to the
conflicting feature and away from other relevant features. This leads to causal
overattribution and may adversely affect the human's information processing. In
a field experiment we implemented an XGBoost-trained model as a decision-making
aid for counselors at a public employment service to predict candidates' risk
of long-term unemployment. The treatment group of counselors was also provided
with SHAP. The results show that the quality of the human's decision-making is
worse when a feature on which the human holds a conflicting prior belief is
displayed as part of the explanation.",http://dx.doi.org/10.1007/978-3-031-44067-0_10,82,18,18,16,16,14
Time-Series Classification for Dynamic Strategies in Multi-Step Forecasting,"Multi-step forecasting (MSF) in time-series, the ability to make predictions
multiple time steps into the future, is fundamental to almost all temporal
domains. To make such forecasts, one must assume the recursive complexity of
the temporal dynamics. Such assumptions are referred to as the forecasting
strategy used to train a predictive model. Previous work shows that it is not
clear which forecasting strategy is optimal a priori to evaluating on unseen
data. Furthermore, current approaches to MSF use a single (fixed) forecasting
strategy.
  In this paper, we characterise the instance-level variance of optimal
forecasting strategies and propose Dynamic Strategies (DyStrat) for MSF. We
experiment using 10 datasets from different scales, domains, and lengths of
multi-step horizons. When using a random-forest-based classifier, DyStrat
outperforms the best fixed strategy, which is not knowable a priori, 94% of the
time, with an average reduction in mean-squared error of 11%. Our approach
typically triples the top-1 accuracy compared to current approaches. Notably,
we show DyStrat generalises well for any MSF task.",http://arxiv.org/abs/2402.08373v1,82,16,18,18,16,14
Function Aligned Regression: A Method Explicitly Learns Functional Derivatives from Data,"Regression is a fundamental task in machine learning that has garnered
extensive attention over the past decades. The conventional approach for
regression involves employing loss functions that primarily concentrate on
aligning model prediction with the ground truth for each individual data
sample, which, as we show, can result in sub-optimal prediction of the
relationships between the different samples. Recent research endeavors have
introduced novel perspectives by incorporating label similarity information to
regression. However, a notable gap persists in these approaches when it comes
to fully capturing the intricacies of the underlying ground truth function. In
this work, we propose FAR (Function Aligned Regression) as a arguably better
and more efficient solution to fit the underlying function of ground truth by
capturing functional derivatives. We demonstrate the effectiveness of the
proposed method practically on 2 synthetic datasets and on 8 extensive
real-world tasks from 6 benchmark datasets with other 8 competitive baselines.
The code is open-sourced at \url{https://github.com/DixianZhu/FAR}.",http://arxiv.org/abs/2402.06104v1,82,18,16,16,18,14
Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models,"The aim of this study is to investigate Machine Unlearning (MU), a burgeoning
field focused on addressing concerns related to neural models inadvertently
retaining personal or sensitive data. Here, a novel approach is introduced to
achieve precise and selective forgetting within language models. Unlike
previous methodologies that adopt completely opposing training objectives, this
approach aims to mitigate adverse effects on language model performance,
particularly in generation tasks. Furthermore, two innovative evaluation
metrics are proposed: Sensitive Information Extraction Likelihood (S-EL) and
Sensitive Information Memory Accuracy (S-MA), designed to gauge the
effectiveness of sensitive information elimination. To reinforce the forgetting
framework, an effective method for annotating sensitive scopes is presented,
involving both online and offline strategies. The online selection mechanism
leverages language probability scores to ensure computational efficiency, while
the offline annotation entails a robust two-stage process based on Large
Language Models (LLMs).",http://arxiv.org/abs/2402.05813v1,82,18,16,16,18,14
Limitations of Agents Simulated by Predictive Models,"There is increasing focus on adapting predictive models into agent-like
systems, most notably AI assistants based on language models. We outline two
structural reasons for why these models can fail when turned into agents.
First, we discuss auto-suggestive delusions. Prior work has shown theoretically
that models fail to imitate agents that generated the training data if the
agents relied on hidden observations: the hidden observations act as
confounding variables, and the models treat actions they generate as evidence
for nonexistent observations. Second, we introduce and formally study a
related, novel limitation: predictor-policy incoherence. When a model generates
a sequence of actions, the model's implicit prediction of the policy that
generated those actions can serve as a confounding variable. The result is that
models choose actions as if they expect future actions to be suboptimal,
causing them to be overly conservative. We show that both of those failures are
fixed by including a feedback loop from the environment, that is, re-training
the models on their own actions. We give simple demonstrations of both
limitations using Decision Transformers and confirm that empirical results
agree with our conceptual and formal analysis. Our treatment provides a
unifying view of those failure modes, and informs the question of why
fine-tuning offline learned policies with online learning makes them more
effective.",http://arxiv.org/abs/2402.05829v1,82,17,16,16,17,16
Rethinking Data Selection for Supervised Fine-Tuning,"Although supervised finetuning (SFT) has emerged as an essential technique to
align large language models with humans, it is considered superficial, with
style learning being its nature. At the same time, recent works indicate the
importance of data selection for SFT, showing that finetuning with high-quality
and diverse subsets of the original dataset leads to superior downstream
performance. In this work, we rethink the intuition behind data selection for
SFT. Considering SFT is superficial, we propose that essential demonstrations
for SFT should focus on reflecting human-like interactions instead of data
quality or diversity. However, it is not straightforward to directly assess to
what extent a demonstration reflects human styles. Towards an initial attempt
in this direction, we find selecting instances with long responses is
surprisingly more effective for SFT than utilizing full datasets or instances
selected based on quality and diversity. We hypothesize that such a simple
heuristic implicitly mimics a crucial aspect of human-style conversation:
detailed responses are usually more helpful.",http://arxiv.org/abs/2402.06094v1,82,17,16,16,17,16
Rethinking the Capacity of Graph Neural Networks for Branching Strategy,"Graph neural networks (GNNs) have been widely used to predict properties and
heuristics of mixed-integer linear programs (MILPs) and hence accelerate MILP
solvers. This paper investigates the capacity of GNNs to represent strong
branching (SB) scores that provide an efficient strategy in the
branch-and-bound algorithm.
  Although message-passing GNN (MP-GNN), as the simplest GNN structure, is
frequently employed in the existing literature to learn SB scores, we prove a
fundamental limitation in its expressive power -- there exist two MILP
instances with different SB scores that cannot be distinguished by any MP-GNN,
regardless of the number of parameters. In addition, we establish a universal
approximation theorem for another GNN structure called the second-order
folklore GNN (2-FGNN). We show that for any data distribution over MILPs, there
always exists a 2-FGNN that can approximate the SB score with arbitrarily high
accuracy and arbitrarily high probability. A small-scale numerical experiment
is conducted to directly validate our theoretical findings.",http://arxiv.org/abs/2402.07099v1,82,16,17,17,16,16
Exploration by Optimization with Hybrid Regularizers: Logarithmic Regret with Adversarial Robustness in Partial Monitoring,"Partial monitoring is a generic framework of online decision-making problems
with limited observations. To make decisions from such limited observations, it
is necessary to find an appropriate distribution for exploration. Recently, a
powerful approach for this purpose, exploration by optimization (ExO), was
proposed, which achieves the optimal bounds in adversarial environments with
follow-the-regularized-leader for a wide range of online decision-making
problems. However, a naive application of ExO in stochastic environments
significantly degrades regret bounds. To resolve this problem in locally
observable games, we first establish a novel framework and analysis for ExO
with a hybrid regularizer. This development allows us to significantly improve
the existing regret bounds of best-of-both-worlds (BOBW) algorithms, which
achieves nearly optimal bounds both in stochastic and adversarial environments.
In particular, we derive a stochastic regret bound of $O(\sum_{a \neq a^*} k^2
m^2 \log T / \Delta_a)$, where $k$, $m$, and $T$ are the numbers of actions,
observations and rounds, $a^*$ is an optimal action, and $\Delta_a$ is the
suboptimality gap for action $a$. This bound is roughly $\Theta(k^2 \log T)$
times smaller than existing BOBW bounds. In addition, for globally observable
games, we provide a new BOBW algorithm with the first $O(\log T)$ stochastic
bound.",http://arxiv.org/abs/2402.08321v1,82,18,17,16,18,13
Towards actionability for open medical imaging datasets: lessons from community-contributed platforms for data management and stewardship,"Medical imaging datasets are fundamental to artificial intelligence (AI) in
healthcare. The accuracy, robustness and fairness of diagnostic algorithms
depend on the data (and its quality) on which the models are trained and
evaluated. Medical imaging datasets have become increasingly available to the
public, and are often hosted on Community-Contributed Platforms (CCP),
including private companies like Kaggle or HuggingFace. While open data is
important to enhance the redistribution of data's public value, we find that
the current CCP governance model fails to uphold the quality needed and
recommended practices for sharing, documenting, and evaluating datasets. In
this paper we investigate medical imaging datasets on CCPs and how they are
documented, shared, and maintained. We first highlight some differences between
medical imaging and computer vision, particularly in the potentially harmful
downstream effects due to poor adoption of recommended dataset management
practices. We then analyze 20 (10 medical and 10 computer vision) popular
datasets on CCPs and find vague licenses, lack of persistent identifiers and
storage, duplicates and missing metadata, with differences between the
platforms. We present ""actionability"" as a conceptual metric to reveal the data
quality gap between characteristics of data on CCPs and the desired
characteristics of data for AI in healthcare. Finally, we propose a
commons-based stewardship model for documenting, sharing and maintaining
datasets on CCPs and end with a discussion of limitations and open questions.",http://arxiv.org/abs/2402.06353v1,82,16,18,18,16,14
A Competition Winning Deep Reinforcement Learning Agent in microRTS,"Scripted agents have predominantly won the five previous iterations of the
IEEE microRTS ($\mu$RTS) competitions hosted at CIG and CoG. Despite Deep
Reinforcement Learning (DRL) algorithms making significant strides in real-time
strategy (RTS) games, their adoption in this primarily academic competition has
been limited due to the considerable training resources required and the
complexity inherent in creating and debugging such agents. RAISocketAI is the
first DRL agent to win the IEEE microRTS competition. In a benchmark without
performance constraints, RAISocketAI regularly defeated the two prior
competition winners. This first competition-winning DRL submission can be a
benchmark for future microRTS competitions and a starting point for future DRL
research. Iteratively fine-tuning the base policy and transfer learning to
specific maps were critical to RAISocketAI's winning performance. These
strategies can be used to economically train future DRL agents. Further work in
Imitation Learning using Behavior Cloning and fine-tuning these models with DRL
has proven promising as an efficient way to bootstrap models with demonstrated,
competitive behaviors.",http://arxiv.org/abs/2402.08112v1,82,16,18,18,16,14
Coordinated Disclosure for AI: Beyond Security Vulnerabilities,"Harm reporting in the field of Artificial Intelligence (AI) currently
operates on an ad hoc basis, lacking a structured process for disclosing or
addressing algorithmic flaws. In contrast, the Coordinated Vulnerability
Disclosure (CVD) ethos and ecosystem play a pivotal role in software security
and transparency. Within the U.S. context, there has been a protracted legal
and policy struggle to establish a safe harbor from the Computer Fraud and
Abuse Act, aiming to foster institutional support for security researchers
acting in good faith. Notably, algorithmic flaws in Machine Learning (ML)
models present distinct challenges compared to traditional software
vulnerabilities, warranting a specialized approach. To address this gap, we
propose the implementation of a dedicated Coordinated Flaw Disclosure (CFD)
framework tailored to the intricacies of machine learning and artificial
intelligence issues. This paper delves into the historical landscape of
disclosures in ML, encompassing the ad hoc reporting of harms and the emergence
of participatory auditing. By juxtaposing these practices with the
well-established disclosure norms in cybersecurity, we argue that the broader
adoption of CFD has the potential to enhance public trust through transparent
processes that carefully balance the interests of both organizations and the
community.",http://arxiv.org/abs/2402.07039v1,82,16,17,18,16,15
Stitching Sub-Trajectories with Conditional Diffusion Model for Goal-Conditioned Offline RL,"Offline Goal-Conditioned Reinforcement Learning (Offline GCRL) is an
important problem in RL that focuses on acquiring diverse goal-oriented skills
solely from pre-collected behavior datasets. In this setting, the reward
feedback is typically absent except when the goal is achieved, which makes it
difficult to learn policies especially from a finite dataset of suboptimal
behaviors. In addition, realistic scenarios involve long-horizon planning,
which necessitates the extraction of useful skills within sub-trajectories.
Recently, the conditional diffusion model has been shown to be a promising
approach to generate high-quality long-horizon plans for RL. However, their
practicality for the goal-conditioned setting is still limited due to a number
of technical assumptions made by the methods. In this paper, we propose SSD
(Sub-trajectory Stitching with Diffusion), a model-based offline GCRL method
that leverages the conditional diffusion model to address these limitations. In
summary, we use the diffusion model that generates future plans conditioned on
the target goal and value, with the target value estimated from the
goal-relabeled offline dataset. We report state-of-the-art performance in the
standard benchmark set of GCRL tasks, and demonstrate the capability to
successfully stitch the segments of suboptimal trajectories in the offline data
to generate high-quality plans.",http://arxiv.org/abs/2402.07226v1,82,16,16,16,18,16
Scaling Artificial Intelligence for Digital Wargaming in Support of Decision-Making,"In this unprecedented era of technology-driven transformation, it becomes
more critical than ever that we aggressively invest in developing robust
artificial intelligence (AI) for wargaming in support of decision-making. By
advancing AI-enabled systems and pairing these with human judgment, we will be
able to enhance all-domain awareness, improve the speed and quality of our
decision cycles, offer recommendations for novel courses of action, and more
rapidly counter our adversary's actions. It therefore becomes imperative that
we accelerate the development of AI to help us better address the complexity of
modern challenges and dilemmas that currently requires human intelligence and,
if possible, attempt to surpass human intelligence--not to replace humans, but
to augment and better inform human decision-making at machine speed. Although
deep reinforcement learning continues to show promising results in intelligent
agent behavior development for the long-horizon, complex tasks typically found
in combat modeling and simulation, further research is needed to enable the
scaling of AI to deal with these intricate and expansive state-spaces
characteristic of wargaming for either concept development, education, or
analysis. To help address this challenge, in our research, we are developing
and implementing a hierarchical reinforcement learning framework that includes
a multi-model approach and dimension-invariant observation abstractions.",http://dx.doi.org/10.14339/STO-MP-MSG-207-23-PDF,82,16,18,18,16,14
The Effect of Haptic Guidance during Robotic-assisted Motor Training is Modulated by Personality Traits,"The provision of robotic assistance during motor training has proven to be
effective in enhancing motor learning in some healthy trainee groups as well as
patients. Personalizing such robotic assistance can help further improve motor
(re)learning outcomes and cater better to the trainee's needs and desires.
However, the development of personalized haptic assistance is hindered by the
lack of understanding of the link between the trainee's personality and the
effects of haptic guidance during human-robot interaction. To address this gap,
we ran an experiment with 42 healthy participants who trained with a robotic
device to control a virtual pendulum to hit incoming targets either with or
without haptic guidance. We found that certain personal traits affected how
users adapt and interact with the guidance during training. In particular,
those participants with an 'Achiever gaming style' performed better and applied
lower interaction forces to the robotic device than the average participant as
the training progressed. Conversely, participants with the 'Free spirit game
style' increased the interaction force in the course of training. We also found
an interaction between some personal characteristics and haptic guidance.
Specifically, participants with a higher 'Transformation of challenge' trait
exhibited poorer performance during training while receiving haptic guidance
compared to an average participant receiving haptic guidance. Furthermore,
individuals with an external Locus of Control tended to increase their
interaction force with the device, deviating from the pattern observed in an
average participant under the same guidance. These findings suggest that
individual characteristics may play a crucial role in the effectiveness of
haptic guidance training strategies.",http://arxiv.org/abs/2402.06325v1,82,16,18,18,16,14
A plastic correction algorithm for full-field elasto-plastic finite element simulations : critical assessment of predictive capabilities and improvement by machine learning,"This paper introduces a new local plastic correction algorithm developed to
accelerate finite element simulations for structures with elasto-plastic
constitutive laws. The proposed method belongs to the category of generalized
multiaxial Neuber-type methods enabled by pointwise proportional evolution
rules. The algorithm numerically integrates J2 plasticity laws as a function of
the finite element elastic response of the structure, to obtain full-field 3D
elasto-plastic quantities for any proportionally applied loading. Examples of
the numerical capabilities of this algorithm are shown on a structure
containing a distribution of pores, for monotonic and fatigue loading. The
approximation errors due to the proposed local plastic correction are also
investigated. As a second point of innovation, we show that the proposed local
plastic correction can be accelerated when dealing with large-scale structures
by employing a simple meta-model, with virtually no added errors. Finally, we
develop and investigate the merits of an additional deep-learning-based
corrective layer to reduce approximations errors on a subset of structures for
which full elasto-plastic FE simulations are performed, the solutions of which
are subsequently used as training set for a Convolutional Neural Network
algorithm designed to learn the error between full FE and plastic correction
approximations.",http://arxiv.org/abs/2402.06313v1,82,18,16,18,16,14
Anatomy of a Robotaxi Crash: Lessons from the Cruise Pedestrian Dragging Mishap,"An October 2023 crash between a GM Cruise robotaxi and a pedestrian in San
Francisco resulted not only in a severe injury, but also dramatic upheaval at
that company that will likely have lasting effects throughout the industry. The
issues stem not just from the crash facts themselves, but also how Cruise
mishandled dealing with their robotaxi dragging a pedestrian under the vehicle
after the initial post-crash stop. A pair of external investigation reports
provide raw material describing the incident and critique the company response
from a regulatory interaction point of view, but did not include potential
safety recommendations in scope. We use that report material to highlight
specific facts and relationships between events by tying together different
pieces of the report material. We then explore safety lessons that might be
learned with regard to technology, operational safety practices, and
organizational reaction to incidents.",http://arxiv.org/abs/2402.06046v1,82,14,16,16,18,18
Educational data mining and learning analytics: An updated survey,"This survey is an updated and improved version of the previous one published
in 2013 in this journal with the title data mining in education. It reviews in
a comprehensible and very general way how Educational Data Mining and Learning
Analytics have been applied over educational data. In the last decade, this
research area has evolved enormously and a wide range of related terms are now
used in the bibliography such as Academic Analytics, Institutional Analytics,
Teaching Analytics, Data-Driven Education, Data-Driven Decision-Making in
Education, Big Data in Education, and Educational Data Science. This paper
provides the current state of the art by reviewing the main publications, the
key milestones, the knowledge discovery cycle, the main educational
environments, the specific tools, the free available datasets, the most used
methods, the main objectives, and the future trends in this research area.",http://dx.doi.org/10.1002/widm.1355,82,16,20,14,18,14
Asynchronous Diffusion Learning with Agent Subsampling and Local Updates,"In this work, we examine a network of agents operating asynchronously, aiming
to discover an ideal global model that suits individual local datasets. Our
assumption is that each agent independently chooses when to participate
throughout the algorithm and the specific subset of its neighbourhood with
which it will cooperate at any given moment. When an agent chooses to take
part, it undergoes multiple local updates before conveying its outcomes to the
sub-sampled neighbourhood. Under this setup, we prove that the resulting
asynchronous diffusion strategy is stable in the mean-square error sense and
provide performance guarantees specifically for the federated learning setting.
We illustrate the findings with numerical simulations.",http://arxiv.org/abs/2402.05529v1,82,16,18,16,18,14
MusicTraces: A collaborative music and paint activity for autistic people,"Painting and music therapy approaches can help to foster social interaction
for autistic people. However, the tools sometimes lack of flexibility and fail
to keep people's attention. Unknowns also remain about the effect of combining
these approaches. Though, very few studies have investigated how Multisensory
Environments (MSEs) could help to address these issues. This paper presents the
design of a full-body music and painting activity called ""MusicTraces"" which
aims to foster collaboration between people with moderate to severe learning
disabilities and complex needs, and in particular autism, within an MSE. The
co-design process with caregivers and people neurodevelopmental conditions is
detailed, including a workshop, the initial design, remote iterations, and a
design critique.",http://arxiv.org/abs/2402.06043v1,82,16,18,16,18,14
"Through the Lens of Split Vote: Exploring Disagreement, Difficulty and Calibration in Legal Case Outcome Classification","In legal decisions, split votes (SV) occur when judges cannot reach a
unanimous decision, posing a difficulty for lawyers who must navigate diverse
legal arguments and opinions. In high-stakes domains, understanding the
alignment of perceived difficulty between humans and AI systems is crucial to
build trust. However, existing NLP calibration methods focus on a classifier's
awareness of predictive performance, measured against the human majority class,
overlooking inherent human label variation (HLV). This paper explores split
votes as naturally observable human disagreement and value pluralism. We
collect judges' vote distributions from the European Court of Human Rights
(ECHR), and present SV-ECHR, a case outcome classification (COC) dataset with
SV information. We build a taxonomy of disagreement with SV-specific
subcategories. We further assess the alignment of perceived difficulty between
models and humans, as well as confidence- and human-calibration of COC models.
We observe limited alignment with the judge vote distribution. To our
knowledge, this is the first systematic exploration of calibration to human
judgements in legal NLP. Our study underscores the necessity for further
research on measuring and enhancing model calibration considering HLV in legal
decision tasks.",http://arxiv.org/abs/2402.07214v1,82,16,18,18,16,14
Checking the Sufficiently Scattered Condition using a Global Non-Convex Optimization Software,"The sufficiently scattered condition (SSC) is a key condition in the study of
identifiability of various matrix factorization problems, including
nonnegative, minimum-volume, symmetric, simplex-structured, and polytopic
matrix factorizations. The SSC allows one to guarantee that the computed matrix
factorization is unique/identifiable, up to trivial ambiguities. However, this
condition is NP-hard to check in general. In this paper, we show that it can
however be checked in a reasonable amount of time in realistic scenarios, when
the factorization rank is not too large. This is achieved by formulating the
problem as a non-convex quadratic optimization problem over a bounded set. We
use the global non-convex optimization software Gurobi, and showcase the
usefulness of this code on synthetic data sets and on real-world hyperspectral
images.",http://arxiv.org/abs/2402.06019v1,82,15,17,17,19,14
Hybrid Active Teaching Methodology for Learning Development: A Self-assessment Case Study Report in Computer Engineering,"The primary objective is to emphasize the merits of active methodologies and
cross-disciplinary curricula in Requirement Engineering. This direction
promises a holistic and applied trajectory for Computer Engineering education,
supported by the outcomes of our case study, where artifact-centric learning
proved effective, with 73% of students achieving the highest grade.
Self-assessments further corroborated academic excellence, emphasizing
students' engagement in skill enhancement and knowledge acquisition.",http://dx.doi.org/10.1145/3605098.3636132,82,16,16,18,18,14
LISR: Learning Linear 3D Implicit Surface Representation Using Compactly Supported Radial Basis Functions,"Implicit 3D surface reconstruction of an object from its partial and noisy 3D
point cloud scan is the classical geometry processing and 3D computer vision
problem. In the literature, various 3D shape representations have been
developed, differing in memory efficiency and shape retrieval effectiveness,
such as volumetric, parametric, and implicit surfaces. Radial basis functions
provide memory-efficient parameterization of the implicit surface. However, we
show that training a neural network using the mean squared error between the
ground-truth implicit surface and the linear basis-based implicit surfaces does
not converge to the global solution. In this work, we propose locally supported
compact radial basis functions for a linear representation of the implicit
surface. This representation enables us to generate 3D shapes with arbitrary
topologies at any resolution due to their continuous nature. We then propose a
neural network architecture for learning the linear implicit shape
representation of the 3D surface of an object. We learn linear implicit shapes
within a supervised learning framework using ground truth Signed-Distance Field
(SDF) data for guidance. The classical strategies face difficulties in finding
linear implicit shapes from a given 3D point cloud due to numerical issues
(requires solving inverse of a large matrix) in basis and query point
selection. The proposed approach achieves better Chamfer distance and
comparable F-score than the state-of-the-art approach on the benchmark dataset.
We also show the effectiveness of the proposed approach by using it for the 3D
shape completion task.",http://arxiv.org/abs/2402.07301v1,82,16,17,16,18,15
Modeling Balanced Explicit and Implicit Relations with Contrastive Learning for Knowledge Concept Recommendation in MOOCs,"The knowledge concept recommendation in Massive Open Online Courses (MOOCs)
is a significant issue that has garnered widespread attention. Existing methods
primarily rely on the explicit relations between users and knowledge concepts
on the MOOC platforms for recommendation. However, there are numerous implicit
relations (e.g., shared interests or same knowledge levels between users)
generated within the users' learning activities on the MOOC platforms. Existing
methods fail to consider these implicit relations, and these relations
themselves are difficult to learn and represent, causing poor performance in
knowledge concept recommendation and an inability to meet users' personalized
needs. To address this issue, we propose a novel framework based on contrastive
learning, which can represent and balance the explicit and implicit relations
for knowledge concept recommendation in MOOCs (CL-KCRec). Specifically, we
first construct a MOOCs heterogeneous information network (HIN) by modeling the
data from the MOOC platforms. Then, we utilize a relation-updated graph
convolutional network and stacked multi-channel graph neural network to
represent the explicit and implicit relations in the HIN, respectively.
Considering that the quantity of explicit relations is relatively fewer
compared to implicit relations in MOOCs, we propose a contrastive learning with
prototypical graph to enhance the representations of both relations to capture
their fruitful inherent relational knowledge, which can guide the propagation
of students' preferences within the HIN. Based on these enhanced
representations, to ensure the balanced contribution of both towards the final
recommendation, we propose a dual-head attention mechanism for balanced fusion.
Experimental results demonstrate that CL-KCRec outperforms several
state-of-the-art baselines on real-world datasets in terms of HR, NDCG and MRR.",http://arxiv.org/abs/2402.08256v1,82,18,16,16,16,16
Evaluating Co-Creativity using Total Information Flow,"Co-creativity in music refers to two or more musicians or musical agents
interacting with one another by composing or improvising music. However, this
is a very subjective process and each musician has their own preference as to
which improvisation is better for some context. In this paper, we aim to create
a measure based on total information flow to quantitatively evaluate the
co-creativity process in music. In other words, our measure is an indication of
how ""good"" a creative musical process is. Our main hypothesis is that a good
musical creation would maximize information flow between the participants
captured by music voices recorded in separate tracks. We propose a method to
compute the information flow using pre-trained generative models as entropy
estimators. We demonstrate how our method matches with human perception using a
qualitative study.",http://arxiv.org/abs/2402.06810v1,82,18,16,16,16,16
Towards Equitable Agile Research and Development of AI and Robotics,"Machine Learning (ML) and 'Artificial Intelligence' ('AI') methods tend to
replicate and amplify existing biases and prejudices, as do Robots with AI. For
example, robots with facial recognition have failed to identify Black Women as
human, while others have categorized people, such as Black Men, as criminals
based on appearance alone. A 'culture of modularity' means harms are perceived
as 'out of scope', or someone else's responsibility, throughout employment
positions in the 'AI supply chain'. Incidents are routine enough
(incidentdatabase.ai lists over 2000 examples) to indicate that few
organizations are capable of completely respecting peoples' rights; meeting
claimed equity, diversity, and inclusion (EDI or DEI) goals; or recognizing and
then addressing such failures in their organizations and artifacts. We propose
a framework for adapting widely practiced Research and Development (R&D)
project management methodologies to build organizational equity capabilities
and better integrate known evidence-based best practices. We describe how
project teams can organize and operationalize the most promising practices,
skill sets, organizational cultures, and methods to detect and address
rights-based fairness, equity, accountability, and ethical problems as early as
possible when they are often less harmful and easier to mitigate; then monitor
for unforeseen incidents to adaptively and constructively address them. Our
primary example adapts an Agile development process based on Scrum, one of the
most widely adopted approaches to organizing R&D teams. We also discuss
limitations of our proposed framework and future research directions.",http://arxiv.org/abs/2402.08242v1,82,16,16,18,18,14
Differentially Private Range Queries with Correlated Input Perturbation,"This work proposes a class of locally differentially private mechanisms for
linear queries, in particular range queries, that leverages correlated input
perturbation to simultaneously achieve unbiasedness, consistency, statistical
transparency, and control over utility requirements in terms of accuracy
targets expressed either in certain query margins or as implied by the
hierarchical database structure. The proposed Cascade Sampling algorithm
instantiates the mechanism exactly and efficiently. Our bounds show that we
obtain near-optimal utility while being empirically competitive against output
perturbation methods.",http://arxiv.org/abs/2402.07066v1,82,18,16,18,16,14
Deep Learning for Medical Image Segmentation with Imprecise Annotation,"Medical image segmentation (MIS) plays an instrumental role in medical image
analysis, where considerable efforts have been devoted to automating the
process. Currently, mainstream MIS approaches are based on deep neural networks
(DNNs) which are typically trained on a dataset that contains annotation masks
produced by doctors. However, in the medical domain, the annotation masks
generated by different doctors can inherently vary because a doctor may
unnecessarily produce precise and unique annotations to meet the goal of
diagnosis. Therefore, the DNN model trained on the data annotated by certain
doctors, often just a single doctor, could undesirably favour those doctors who
annotate the training data, leading to the unsatisfaction of a new doctor who
will use the trained model. To address this issue, this work investigates the
utilization of multi-expert annotation to enhance the adaptability of the model
to a new doctor and we conduct a pilot study on the MRI brain segmentation
task. Experimental results demonstrate that the model trained on a dataset with
multi-expert annotation can efficiently cater for a new doctor, after
lightweight fine-tuning on just a few annotations from the new doctor.",http://arxiv.org/abs/2402.07330v1,82,16,18,18,16,14
The boundary of neural network trainability is fractal,"Some fractals -- for instance those associated with the Mandelbrot and
quadratic Julia sets -- are computed by iterating a function, and identifying
the boundary between hyperparameters for which the resulting series diverges or
remains bounded. Neural network training similarly involves iterating an update
function (e.g. repeated steps of gradient descent), can result in convergent or
divergent behavior, and can be extremely sensitive to small changes in
hyperparameters. Motivated by these similarities, we experimentally examine the
boundary between neural network hyperparameters that lead to stable and
divergent training. We find that this boundary is fractal over more than ten
decades of scale in all tested configurations.",http://arxiv.org/abs/2402.06184v1,82,18,16,18,16,14
Reconfigurable Stochastic Neurons Based on Strain Engineered Low Barrier Nanomagnets,"Stochastic neurons are efficient hardware accelerators for solving a large
variety of combinatorial optimization problems. ""Binary"" stochastic neurons
(BSN) are those whose states fluctuate randomly between two levels +1 and -1,
with the probability of being in either level determined by an external bias.
""Analog"" stochastic neurons (ASNs), in contrast, can assume any state between
the two levels randomly (hence ""analog"") and can perform analog signal
processing. They may be leveraged for such tasks as temporal sequence learning,
processing and prediction. Both BSNs and ASNs can be used to build efficient
and scalable neural networks. Both can be implemented with low (potential
energy) barrier nanomagnets (LBMs) whose random magnetization orientations
encode the binary or analog state variables. The difference between them is
that the potential energy barrier in a BSN LBM, albeit low, is much higher than
that in an ASN LBM. As a result, a BSN LBM has a clear double well potential
profile, which makes its magnetization orientation assume one of two
orientations at any time, resulting in the binary behavior. ASN nanomagnets, on
the other hand, hardly have any energy barrier at all and hence lack the double
well feature. That makes their magnetizations fluctuate in an analog fashion.
Hence, one can reconfigure an ASN to a BSN, and vice-versa, by simply raising
and lowering the energy barrier. If the LBM is magnetostrictive, then this can
be done with local (electrically generated) strain. Such a reconfiguration
capability heralds a powerful field programmable architecture for a p-computer,
and the energy cost for this type of reconfiguration is miniscule.",http://arxiv.org/abs/2402.06168v1,82,19,17,18,16,12
Persian Speech Emotion Recognition by Fine-Tuning Transformers,"Given the significance of speech emotion recognition, numerous methods have
been developed in recent years to create effective and efficient systems in
this domain. One of these methods involves the use of pretrained transformers,
fine-tuned to address this specific problem, resulting in high accuracy.
Despite extensive discussions and global-scale efforts to enhance these
systems, the application of this innovative and effective approach has received
less attention in the context of Persian speech emotion recognition. In this
article, we review the field of speech emotion recognition and its background,
with an emphasis on the importance of employing transformers in this context.
We present two models, one based on spectrograms and the other on the audio
itself, fine-tuned using the shEMO dataset. These models significantly enhance
the accuracy of previous systems, increasing it from approximately 65% to 80%
on the mentioned dataset. Subsequently, to investigate the effect of
multilinguality on the fine-tuning process, these same models are fine-tuned
twice. First, they are fine-tuned using the English IEMOCAP dataset, and then
they are fine-tuned with the Persian shEMO dataset. This results in an improved
accuracy of 82% for the Persian emotion recognition system. Keywords: Persian
Speech Emotion Recognition, shEMO, Self-Supervised Learning",http://arxiv.org/abs/2402.07326v1,82,16,18,16,16,16
Digital Computers Break the Curse of Dimensionality: Adaptive Bounds via Finite Geometry,"Many of the foundations of machine learning rely on the idealized premise
that all input and output spaces are infinite, e.g.~$\mathbb{R}^d$. This core
assumption is systematically violated in practice due to digital computing
limitations from finite machine precision, rounding, and limited RAM. In short,
digital computers operate on finite grids in $\mathbb{R}^d$. By exploiting
these discrete structures, we show the curse of dimensionality in statistical
learning is systematically broken when models are implemented on real
computers. Consequentially, we obtain new generalization bounds with
dimension-free rates for kernel and deep ReLU MLP regressors, which are
implemented on real-world machines.
  Our results are derived using a new non-asymptotic concentration of measure
result between a probability measure over any finite metric space and its
empirical version associated with $N$ i.i.d. samples when measured in the
$1$-Wasserstein distance. Unlike standard concentration of measure results, the
concentration rates in our bounds do not hold uniformly for all sample sizes
$N$; instead, our rates can adapt to any given $N$. This yields significantly
tighter bounds for realistic sample sizes while achieving the optimal
worst-case rate of $\mathcal{O}(1/N^{1/2})$ for massive. Our results are built
on new techniques combining metric embedding theory with optimal transport",http://arxiv.org/abs/2402.05576v1,82,18,17,16,17,14
Learning Contrastive Feature Representations for Facial Action Unit Detection,"The predominant approach to facial action unit (AU) detection revolves around
a supervised multi-label binary classification problem. Existing methodologies
often encode pixel-level information of AUs, thereby imposing substantial
demands on model complexity and expressiveness. Moreover, this practice
elevates the susceptibility to overfitting due to the presence of noisy AU
labels. In the present study, we introduce a contrastive learning framework
enhanced by both supervised and self-supervised signals. The objective is to
acquire discriminative features, deviating from the conventional pixel-level
learning paradigm within the domain of AU detection. To address the challenge
posed by noisy AU labels, we augment the supervised signal through the
introduction of a self-supervised signal. This augmentation is achieved through
positive sample sampling, encompassing three distinct types of positive sample
pairs. Furthermore, to mitigate the imbalanced distribution of each AU type, we
employ an importance re-weighting strategy tailored for minority AUs. The
resulting loss, denoted as AUNCE, is proposed to encapsulate this strategy. Our
experimental assessments, conducted on two widely-utilized benchmark datasets
(BP4D and DISFA), underscore the superior performance of our approach compared
to state-of-the-art methods in the realm of AU detection.",http://arxiv.org/abs/2402.06165v1,82,16,18,16,16,16
Effort and Size Estimation in Software Projects with Large Language Model-based Intelligent Interfaces,"The advancement of Large Language Models (LLM) has also resulted in an
equivalent proliferation in its applications. Software design, being one, has
gained tremendous benefits in using LLMs as an interface component that extends
fixed user stories. However, inclusion of LLM-based AI agents in software
design often poses unexpected challenges, especially in the estimation of
development efforts. Through the example of UI-based user stories, we provide a
comparison against traditional methods and propose a new way to enhance
specifications of natural language-based questions that allows for the
estimation of development effort by taking into account data sources,
interfaces and algorithms.",http://arxiv.org/abs/2402.07158v1,82,18,16,16,16,16
Sequential Flow Matching for Generative Modeling,"Straightening the probability flow of the continuous-time generative models,
such as diffusion models or flow-based models, is the key to fast sampling
through the numerical solvers, existing methods learn a linear path by directly
generating the probability path the joint distribution between the noise and
data distribution. One key reason for the slow sampling speed of the ODE-based
solvers that simulate these generative models is the global truncation error of
the ODE solver, caused by the high curvature of the ODE trajectory, which
explodes the truncation error of the numerical solvers in the low-NFE regime.
To address this challenge, We propose a novel method called SeqRF, a learning
technique that straightens the probability flow to reduce the global truncation
error and hence enable acceleration of sampling and improve the synthesis
quality. In both theoretical and empirical studies, we first observe the
straightening property of our SeqRF. Through empirical evaluations via SeqRF
over flow-based generative models, We achieve surpassing results on CIFAR-10,
CelebA-$64 \times 64$, and LSUN-Church datasets.",http://arxiv.org/abs/2402.06461v1,82,18,16,16,18,14
Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs,"How do transformer-based large language models (LLMs) store and retrieve
knowledge? We focus on the most basic form of this task -- factual recall,
where the model is tasked with explicitly surfacing stored facts in prompts of
form `Fact: The Colosseum is in the country of'. We find that the mechanistic
story behind factual recall is more complex than previously thought. It
comprises several distinct, independent, and qualitatively different mechanisms
that additively combine, constructively interfering on the correct attribute.
We term this generic phenomena the additive motif: models compute through
summing up multiple independent contributions. Each mechanism's contribution
may be insufficient alone, but summing results in constructive interfere on the
correct answer. In addition, we extend the method of direct logit attribution
to attribute an attention head's output to individual source tokens. We use
this technique to unpack what we call `mixed heads' -- which are themselves a
pair of two separate additive updates from different source tokens.",http://arxiv.org/abs/2402.07321v1,82,17,19,17,15,14
Pretrained Generative Language Models as General Learning Frameworks for Sequence-Based Tasks,"We propose that small pretrained foundational generative language models with
millions of parameters can be utilized as a general learning framework for
sequence-based tasks. Our proposal overcomes the computational resource, skill
set, and timeline challenges associated with training neural networks and
language models from scratch. Further, our approach focuses on creating small
and highly specialized models that can accurately execute a challenging task of
which the base model is incapable of performing. We demonstrate that 125M,
350M, and 1.3B parameter pretrained foundational language models can be
instruction fine-tuned with 10,000-to-1,000,000 instruction examples to achieve
near state-of-the-art results on challenging cheminformatics tasks. We also
demonstrate the role of successive language model fine-tuning epochs on
improved outcomes, as well as the importance of both data formatting and
pretrained foundational language model selection for instruction fine-tuning
success.",http://arxiv.org/abs/2402.05616v1,82,16,18,18,14,16
Auditing Work: Exploring the New York City algorithmic bias audit regime,"In July 2023, New York City (NYC) initiated the first algorithm auditing
system for commercial machine-learning systems. Local Law 144 (LL 144) mandates
NYC-based employers using automated employment decision-making tools (AEDTs) in
hiring to undergo annual bias audits conducted by an independent auditor. This
paper examines lessons from LL 144 for other national algorithm auditing
attempts. Through qualitative interviews with 16 experts and practitioners
within the regime, we find that LL 144 has not effectively established an
auditing regime. The law fails to clearly define key aspects, such as AEDTs and
independent auditors, leading auditors, AEDT vendors, and companies using AEDTs
to define the law's practical implementation in ways that failed to protect job
applicants. Contributing factors include the law's flawed transparency-driven
theory of change, industry lobbying narrowing the definition of AEDTs,
practical and cultural challenges faced by auditors in accessing data, and wide
disagreement over what constitutes a legitimate auditor, resulting in four
distinct 'auditor roles.' We conclude with four recommendations for
policymakers seeking to create similar bias auditing regimes, emphasizing
clearer definitions, metrics, and increased accountability. By exploring LL 144
through the lens of auditors, our paper advances the evidence base around audit
as an accountability mechanism, providing guidance for policymakers seeking to
create similar regimes.",http://arxiv.org/abs/2402.08101v1,82,18,16,18,16,14
Jointly Learning Representations for Map Entities via Heterogeneous Graph Contrastive Learning,"The electronic map plays a crucial role in geographic information systems,
serving various urban managerial scenarios and daily life services. Developing
effective Map Entity Representation Learning (MERL) methods is crucial to
extracting embedding information from electronic maps and converting map
entities into representation vectors for downstream applications. However,
existing MERL methods typically focus on one specific category of map entities,
such as POIs, road segments, or land parcels, which is insufficient for
real-world diverse map-based applications and might lose latent structural and
semantic information interacting between entities of different types. Moreover,
using representations generated by separate models for different map entities
can introduce inconsistencies. Motivated by this, we propose a novel method
named HOME-GCL for learning representations of multiple categories of map
entities. Our approach utilizes a heterogeneous map entity graph (HOME graph)
that integrates both road segments and land parcels into a unified framework. A
HOME encoder with parcel-segment joint feature encoding and heterogeneous graph
transformer is then deliberately designed to convert segments and parcels into
representation vectors. Moreover, we introduce two types of contrastive
learning tasks, namely intra-entity and inter-entity tasks, to train the
encoder in a self-supervised manner. Extensive experiments on three large-scale
datasets covering road segment-based, land parcel-based, and trajectory-based
tasks demonstrate the superiority of our approach. To the best of our
knowledge, HOME-GCL is the first attempt to jointly learn representations for
road segments and land parcels using a unified model.",http://arxiv.org/abs/2402.06135v1,82,16,18,18,16,14
"Efficient Models for the Detection of Hate, Abuse and Profanity","Large Language Models (LLMs) are the cornerstone for many Natural Language
Processing (NLP) tasks like sentiment analysis, document classification, named
entity recognition, question answering, summarization, etc. LLMs are often
trained on data which originates from the web. This data is prone to having
content with Hate, Abuse and Profanity (HAP). For a detailed definition of HAP,
please refer to the Appendix. Due to the LLMs being exposed to HAP content
during training, the models learn it and may then generate hateful or profane
content. For example, when the open-source RoBERTa model (specifically, the
RoBERTA base model) from the HuggingFace (HF) Transformers library is prompted
to replace the mask token in `I do not know that Persian people are that MASK`
it returns the word `stupid` with the highest score. This is unacceptable in
civil discourse.The detection of Hate, Abuse and Profanity in text is a vital
component of creating civil and unbiased LLMs, which is needed not only for
English, but for all languages. In this article, we briefly describe the
creation of HAP detectors and various ways of using them to make models civil
and acceptable in the output they generate.",http://arxiv.org/abs/2402.05624v1,82,18,18,16,14,16
Learning semantic image quality for fetal ultrasound from noisy ranking annotation,"We introduce the notion of semantic image quality for applications where
image quality relies on semantic requirements. Working in fetal ultrasound,
where ranking is challenging and annotations are noisy, we design a robust
coarse-to-fine model that ranks images based on their semantic image quality
and endow our predicted rankings with an uncertainty estimate. To annotate
rankings on training data, we design an efficient ranking annotation scheme
based on the merge sort algorithm. Finally, we compare our ranking algorithm to
a number of state-of-the-art ranking algorithms on a challenging fetal
ultrasound quality assessment task, showing the superior performance of our
method on the majority of rank correlation metrics.",http://arxiv.org/abs/2402.08294v1,82,16,18,16,16,16
Learning pseudo-contractive denoisers for inverse problems,"Deep denoisers have shown excellent performance in solving inverse problems
in signal and image processing. In order to guarantee the convergence, the
denoiser needs to satisfy some Lipschitz conditions like non-expansiveness.
However, enforcing such constraints inevitably compromises recovery
performance. This paper introduces a novel training strategy that enforces a
weaker constraint on the deep denoiser called pseudo-contractiveness. By
studying the spectrum of the Jacobian matrix, relationships between different
denoiser assumptions are revealed. Effective algorithms based on gradient
descent and Ishikawa process are derived, and further assumptions of strict
pseudo-contractiveness yield efficient algorithms using half-quadratic
splitting and forward-backward splitting. The proposed algorithms theoretically
converge strongly to a fixed point. A training strategy based on holomorphic
transformation and functional calculi is proposed to enforce the
pseudo-contractive denoiser assumption. Extensive experiments demonstrate
superior performance of the pseudo-contractive denoiser compared to related
denoisers. The proposed methods are competitive in terms of visual effects and
quantitative values.",http://arxiv.org/abs/2402.05637v1,82,18,18,16,16,14
Which Pretrain Samples to Rehearse when Finetuning Pretrained Models?,"Fine-tuning pretrained foundational models on specific tasks is now the de
facto approach for text and vision tasks. A known pitfall of this approach is
the forgetting of pretraining knowledge that happens during finetuning.
Rehearsing samples randomly from the pretrain dataset is a common approach to
alleviate such forgetting. However, we find that random mixing unintentionally
includes samples which are not (yet) forgotten or unlearnable by the model. We
propose a novel sampling scheme, mix-cd, that identifies and prioritizes
samples that actually face forgetting, which we call collateral damage. Since
directly identifying collateral damage samples is computationally expensive, we
propose a procedure to estimate the distribution of such samples by tracking
the statistics of finetuned samples. Our approach is lightweight, easy to
implement, and can be seamlessly integrated into existing models, offering an
effective means to retain pretrain performance without additional computational
costs.",http://arxiv.org/abs/2402.08096v1,82,18,17,17,16,14
Modeling of Key Quality Indicators for End-to-End Network Management: Preparing for 5G,"Thanks to evolving cellular telecommunication networks, providers can deploy
a wide range of services. Soon, 5G mobile networks will be available to handle
all types of services and applications for vast numbers of users through their
mobile equipment. To effectively manage new 5G systems, end-to-end (E2E)
performance analysis and optimization will be key features. However, estimating
the end-user experience is not an easy task for network operators. The amount
of end-user performance information operators can measure from the network is
limited, complicating this approach. Here we explore the calculation of service
metrics [known as key quality indicators (KQIs)] from classic low-layer
measurements and parameters. We propose a complete machine-learning (ML)
modeling framework. This system's low-layer metrics can be applied to measure
service-layer performance. To assess the approach, we implemented and evaluated
the proposed system on a real cellular network testbed.",http://dx.doi.org/10.1109/MVT.2019.2938448,82,16,18,18,16,14
"Rocks Coding, Not Development--A Human-Centric, Experimental Evaluation of LLM-Supported SE Tasks","Recently, large language models (LLM) based generative AI has been gaining
momentum for their impressive high-quality performances in multiple domains,
particularly after the release of the ChatGPT. Many believe that they have the
potential to perform general-purpose problem-solving in software development
and replace human software developers. Nevertheless, there are in a lack of
serious investigation into the capability of these LLM techniques in fulfilling
software development tasks. In a controlled 2 $\times$ 2 between-subject
experiment with 109 participants, we examined whether and to what degree
working with ChatGPT was helpful in the coding task and typical software
development task and how people work with ChatGPT. We found that while ChatGPT
performed well in solving simple coding problems, its performance in supporting
typical software development tasks was not that good. We also observed the
interactions between participants and ChatGPT and found the relations between
the interactions and the outcomes. Our study thus provides first-hand insights
into using ChatGPT to fulfill software engineering tasks with real-world
developers and motivates the need for novel interaction mechanisms that help
developers effectively work with large language models to achieve desired
outcomes.",http://arxiv.org/abs/2402.05650v2,82,16,18,16,18,14
Delving into Parameter-Efficient Fine-Tuning in Code Change Learning: An Empirical Study,"Compared to Full-Model Fine-Tuning (FMFT), Parameter Efficient Fine-Tuning
(PEFT) has demonstrated superior performance and lower computational overhead
in several code understanding tasks, such as code summarization and code
search. This advantage can be attributed to PEFT's ability to alleviate the
catastrophic forgetting issue of Pre-trained Language Models (PLMs) by updating
only a small number of parameters. As a result, PEFT effectively harnesses the
pre-trained general-purpose knowledge for downstream tasks. However, existing
studies primarily involve static code comprehension, aligning with the
pre-training paradigm of recent PLMs and facilitating knowledge transfer, but
they do not account for dynamic code changes. Thus, it remains unclear whether
PEFT outperforms FMFT in task-specific adaptation for code-change-related
tasks. To address this question, we examine two prevalent PEFT methods, namely
Adapter Tuning (AT) and Low-Rank Adaptation (LoRA), and compare their
performance with FMFT on five popular PLMs. Specifically, we evaluate their
performance on two widely-studied code-change-related tasks: Just-In-Time
Defect Prediction (JIT-DP) and Commit Message Generation (CMG). The results
demonstrate that both AT and LoRA achieve state-of-the-art (SOTA) results in
JIT-DP and exhibit comparable performances in CMG when compared to FMFT and
other SOTA approaches. Furthermore, AT and LoRA exhibit superiority in
cross-lingual and low-resource scenarios. We also conduct three probing tasks
to explain the efficacy of PEFT techniques on JIT-DP and CMG tasks from both
static and dynamic perspectives. The study indicates that PEFT, particularly
through the use of AT and LoRA, offers promising advantages in
code-change-related tasks, surpassing FMFT in certain aspects.",http://arxiv.org/abs/2402.06247v1,82,16,18,16,16,16
A Neural-network Enhanced Video Coding Framework beyond ECM,"In this paper, a hybrid video compression framework is proposed that serves
as a demonstrative showcase of deep learning-based approaches extending beyond
the confines of traditional coding methodologies. The proposed hybrid framework
is founded upon the Enhanced Compression Model (ECM), which is a further
enhancement of the Versatile Video Coding (VVC) standard. We have augmented the
latest ECM reference software with well-designed coding techniques, including
block partitioning, deep learning-based loop filter, and the activation of
block importance mapping (BIM) which was integrated but previously inactive
within ECM, further enhancing coding performance. Compared with ECM-10.0, our
method achieves 6.26, 13.33, and 12.33 BD-rate savings for the Y, U, and V
components under random access (RA) configuration, respectively.",http://arxiv.org/abs/2402.08397v1,82,16,18,18,16,14
Interpretable classifiers for tabular data via discretization and feature selection,"We introduce a method for computing immediately human interpretable yet
accurate classifiers from tabular data. The classifiers obtained are short
DNF-formulas, computed via first discretizing the original data to Boolean form
and then using feature selection coupled with a very fast algorithm for
producing the best possible Boolean classifier for the setting. We demonstrate
the approach via 14 experiments, obtaining results with accuracies mainly
similar to ones obtained via random forests, XGBoost, and existing results for
the same datasets in the literature. In several cases, our approach in fact
outperforms the reference results in relation to accuracy, even though the main
objective of our study is the immediate interpretability of our classifiers. We
also prove a new result on the probability that the classifier we obtain from
real-life data corresponds to the ideally best classifier with respect to the
background distribution the data comes from.",http://arxiv.org/abs/2402.05680v1,82,16,18,16,18,14
Learning Best-in-Class Policies for the Predict-then-Optimize Framework,"We propose a novel family of decision-aware surrogate losses, called
Perturbation Gradient (PG) losses, for the predict-then-optimize framework.
These losses directly approximate the downstream decision loss and can be
optimized using off-the-shelf gradient-based methods. Importantly, unlike
existing surrogate losses, the approximation error of our PG losses vanishes as
the number of samples grows. This implies that optimizing our surrogate loss
yields a best-in-class policy asymptotically, even in misspecified settings.
This is the first such result in misspecified settings and we provide numerical
evidence confirming our PG losses substantively outperform existing proposals
when the underlying model is misspecified and the noise is not centrally
symmetric. Insofar as misspecification is commonplace in practice -- especially
when we might prefer a simpler, more interpretable model -- PG losses offer a
novel, theoretically justified, method for computationally tractable
decision-aware learning.",http://arxiv.org/abs/2402.03256v2,80,18,18,16,16,12
The Virtues of Pessimism in Inverse Reinforcement Learning,"Inverse Reinforcement Learning (IRL) is a powerful framework for learning
complex behaviors from expert demonstrations. However, it traditionally
requires repeatedly solving a computationally expensive reinforcement learning
(RL) problem in its inner loop. It is desirable to reduce the exploration
burden by leveraging expert demonstrations in the inner-loop RL. As an example,
recent work resets the learner to expert states in order to inform the learner
of high-reward expert states. However, such an approach is infeasible in the
real world. In this work, we consider an alternative approach to speeding up
the RL subroutine in IRL: \emph{pessimism}, i.e., staying close to the expert's
data distribution, instantiated via the use of offline RL algorithms. We
formalize a connection between offline RL and IRL, enabling us to use an
arbitrary offline RL algorithm to improve the sample efficiency of IRL. We
validate our theory experimentally by demonstrating a strong correlation
between the efficacy of an offline RL algorithm and how well it works as part
of an IRL procedure. By using a strong offline RL algorithm as part of an IRL
procedure, we are able to find policies that match expert performance
significantly more efficiently than the prior art.",http://arxiv.org/abs/2402.02616v2,80,18,16,16,16,14
Beyond Lengthscales: No-regret Bayesian Optimisation With Unknown Hyperparameters Of Any Type,"Bayesian optimisation requires fitting a Gaussian process model, which in
turn requires specifying hyperparameters - most of the theoretical literature
assumes those hyperparameters are known. The commonly used maximum likelihood
estimator for hyperparameters of the Gaussian process is consistent only if the
data fills the space uniformly, which does not have to be the case in Bayesian
optimisation. Since no guarantees exist regarding the correctness of
hyperparameter estimation, and those hyperparameters can significantly affect
the Gaussian process fit, theoretical analysis of Bayesian optimisation with
unknown hyperparameters is very challenging. Previously proposed algorithms
with the no-regret property were only able to handle the special case of
unknown lengthscales, reproducing kernel Hilbert space norm and applied only to
the frequentist case. We propose a novel algorithm, HE-GP-UCB, which is the
first algorithm enjoying the no-regret property in the case of unknown
hyperparameters of arbitrary form, and which supports both Bayesian and
frequentist settings. Our proof idea is novel and can easily be extended to
other variants of Bayesian optimisation. We show this by extending our
algorithm to the adversarially robust optimisation setting under unknown
hyperparameters. Finally, we empirically evaluate our algorithm on a set of toy
problems and show that it can outperform the maximum likelihood estimator.",http://arxiv.org/abs/2402.01632v2,80,18,16,18,14,14
Denoising Diffusion Probabilistic Models in Six Simple Steps,"Denoising Diffusion Probabilistic Models (DDPMs) are a very popular class of
deep generative model that have been successfully applied to a diverse range of
problems including image and video generation, protein and material synthesis,
weather forecasting, and neural surrogates of partial differential equations.
Despite their ubiquity it is hard to find an introduction to DDPMs which is
simple, comprehensive, clean and clear. The compact explanations necessary in
research papers are not able to elucidate all of the different design steps
taken to formulate the DDPM and the rationale of the steps that are presented
is often omitted to save space. Moreover, the expositions are typically
presented from the variational lower bound perspective which is unnecessary and
arguably harmful as it obfuscates why the method is working and suggests
generalisations that do not perform well in practice. On the other hand,
perspectives that take the continuous time-limit are beautiful and general, but
they have a high barrier-to-entry as they require background knowledge of
stochastic differential equations and probability flow. In this note, we
distill down the formulation of the DDPM into six simple steps each of which
comes with a clear rationale. We assume that the reader is familiar with
fundamental topics in machine learning including basic probabilistic modelling,
Gaussian distributions, maximum likelihood estimation, and deep learning.",http://arxiv.org/abs/2402.04384v2,80,16,18,16,16,14
The Complexity of Sequential Prediction in Dynamical Systems,"We study the problem of learning to predict the next state of a dynamical
system when the underlying evolution function is unknown. Unlike previous work,
we place no parametric assumptions on the dynamical system, and study the
problem from a learning theory perspective. We define new combinatorial
measures and dimensions and show that they quantify the optimal mistake and
regret bounds in the realizable and agnostic setting respectively.",http://arxiv.org/abs/2402.06614v1,80,18,18,16,16,12
Optimizing Predictive AI in Physical Design Flows with Mini Pixel Batch Gradient Descent,"Exploding predictive AI has enabled fast yet effective evaluation and
decision-making in modern chip physical design flows. State-of-the-art
frameworks typically include the objective of minimizing the mean square error
(MSE) between the prediction and the ground truth. We argue the averaging
effect of MSE induces limitations in both model training and deployment, and
good MSE behavior does not guarantee the capability of these models to assist
physical design flows which are likely sabotaged due to a small portion of
prediction error. To address this, we propose mini-pixel batch gradient descent
(MPGD), a plug-and-play optimization algorithm that takes the most informative
entries into consideration, offering probably faster and better convergence.
Experiments on representative benchmark suits show the significant benefits of
MPGD on various physical design prediction tasks using CNN or Graph-based
models.",http://arxiv.org/abs/2402.06034v1,80,18,16,16,16,14
A Logical Approach to Criminal Case Investigation,"XAI (eXplanable AI) techniques that have the property of explaining the
reasons for their conclusions, i.e. explainability or interpretability, are
attracting attention. XAI is expected to be used in the development of forensic
science and the justice system. In today's forensic and criminal investigation
environment, experts face many challenges due to large amounts of data, small
pieces of evidence in a chaotic and complex environment, traditional laboratory
structures and sometimes inadequate knowledge. All these can lead to failed
investigations and miscarriages of justice. In this paper, we describe the
application of one logical approach to crime scene investigation. The subject
of the application is ``The Adventure of the Speckled Band'' from the Sherlock
Holmes short stories. The applied data is the knowledge graph created for the
Knowledge Graph Reasoning Challenge. We tried to find the murderer by inferring
each person with the motive, opportunity, and method. We created an ontology of
motives and methods of murder from dictionaries and dictionaries, added it to
the knowledge graph of ``The Adventure of the Speckled Band'', and applied
scripts to determine motives, opportunities, and methods.",http://arxiv.org/abs/2402.08284v1,80,15,16,17,16,16
YAMLE: Yet Another Machine Learning Environment,"YAMLE: Yet Another Machine Learning Environment is an open-source framework
that facilitates rapid prototyping and experimentation with machine learning
(ML) models and methods. The key motivation is to reduce repetitive work when
implementing new approaches and improve reproducibility in ML research. YAMLE
includes a command-line interface and integrations with popular and
well-maintained PyTorch-based libraries to streamline training, hyperparameter
optimisation, and logging. The ambition for YAMLE is to grow into a shared
ecosystem where researchers and practitioners can quickly build on and compare
existing implementations. Find it at: https://github.com/martinferianc/yamle.",http://arxiv.org/abs/2402.06268v1,80,16,16,16,16,16
Value function interference and greedy action selection in value-based multi-objective reinforcement learning,"Multi-objective reinforcement learning (MORL) algorithms extend conventional
reinforcement learning (RL) to the more general case of problems with multiple,
conflicting objectives, represented by vector-valued rewards. Widely-used
scalar RL methods such as Q-learning can be modified to handle multiple
objectives by (1) learning vector-valued value functions, and (2) performing
action selection using a scalarisation or ordering operator which reflects the
user's utility with respect to the different objectives. However, as we
demonstrate here, if the user's utility function maps widely varying
vector-values to similar levels of utility, this can lead to interference in
the value-function learned by the agent, leading to convergence to sub-optimal
policies. This will be most prevalent in stochastic environments when
optimising for the Expected Scalarised Return criterion, but we present a
simple example showing that interference can also arise in deterministic
environments. We demonstrate empirically that avoiding the use of random
tie-breaking when identifying greedy actions can ameliorate, but not fully
overcome, the problems caused by value function interference.",http://arxiv.org/abs/2402.06266v1,80,16,16,16,16,16
Investigating Consistency in Query-Based Meeting Summarization: A Comparative Study of Different Embedding Methods,"With more and more advanced data analysis techniques emerging, people will
expect these techniques to be applied in more complex tasks and solve problems
in our daily lives. Text Summarization is one of famous applications in Natural
Language Processing (NLP) field. It aims to automatically generate summary with
important information based on a given context, which is important when you
have to deal with piles of documents. Summarization techniques can help capture
key points in a short time and bring convenience in works. One of applicable
situation is meeting summarization, especially for important meeting that tend
to be long, complicated, multi-topic and multi-person. Therefore, when people
want to review specific content from a meeting, it will be hard and
time-consuming to find the related spans in the meeting transcript. However,
most of previous works focus on doing summarization for newsletters, scientific
articles...etc, which have a clear document structure and an official format.
For the documents with complex structure like transcripts, we think those works
are not quite suitable for meeting summarization. Besides, the consistency of
summary is another issue common to be discussed in NLP field. To conquer
challenges of meeting summarization, we are inspired by ""QMSum: A New Benchmark
for Query-based Multi-domain Meeting Summarization"" proposed by Microsoft and
we also propose our Locater model designed to extract relevant spans based on
given transcript and query, which are then summarized by Summarizer model.
Furthermore, we perform a comparative study by applying different word
embedding techniques to improve summary consistency.",http://arxiv.org/abs/2402.06907v1,80,16,18,16,16,14
Impact on Public Health Decision Making by Utilizing Big Data Without Domain Knowledge,"New data sources, and artificial intelligence (AI) methods to extract
information from them are becoming plentiful, and relevant to decision making
in many societal applications. An important example is street view imagery,
available in over 100 countries, and considered for applications such as
assessing built environment aspects in relation to community health outcomes.
Relevant to such uses, important examples of bias in the use of AI are evident
when decision-making based on data fails to account for the robustness of the
data, or predictions are based on spurious correlations. To study this risk, we
utilize 2.02 million GSV images along with health, demographic, and
socioeconomic data from New York City. Initially, we demonstrate that built
environment characteristics inferred from GSV labels at the intra-city level
may exhibit inadequate alignment with the ground truth. We also find that the
average individual-level behavior of physical inactivity significantly mediates
the impact of built environment features by census tract, as measured through
GSV. Finally, using a causal framework which accounts for these mediators of
environmental impacts on health, we find that altering 10% of samples in the
two lowest tertiles would result in a 4.17 (95% CI 3.84 to 4.55) or 17.2 (95%
CI 14.4 to 21.3) times bigger decrease on the prevalence of obesity or
diabetes, than the same proportional intervention on the number of crosswalks
by census tract. This work illustrates important issues of robustness and model
specification for informing effective allocation of interventions using new
data sources.",http://arxiv.org/abs/2402.06059v1,80,17,16,19,14,14
What the Fix? A Study of ASATs Rule Documentation,"Automatic Static Analysis Tools (ASATs) are widely used by software
developers to diffuse and enforce coding practices. Yet, we know little about
the documentation of ASATs, despite it being critical to learn about the coding
practices in the first place. We shed light on this through several
contributions. First, we analyze the documentation of more than 100 rules of 16
ASATs for multiple programming languages, and distill a taxonomy of the
purposes of the documentation-What triggers a rule; Why it is important; and
how to Fix an issue-and its types of contents. Then, we conduct a survey to
assess the effectiveness of the documentation in terms of its goals and types
of content. We highlight opportunities for improvement in ASAT documentation.
In particular, we find that the Why purpose is missing in half of the rules we
survey; moreover, when the Why is present, it is more likely to have quality
issues than the What and the Fix.",http://dx.doi.org/10.1145/3643916.3644404,80,16,16,16,16,16
Teleoscope: Exploring Themes in Large Document Sets By Example,"Qualitative thematic exploration of data by hand does not scale and
researchers create and update a personalized point of view as they explore
data. As a result, machine learning (ML) approaches that might help with
exploration are challenging to apply. We developed Teleoscope, a web-based
system that supports interactive exploration of large corpora (100K-1M) of
short documents (1-3 paragraphs). Teleoscope provides visual programming
workflows that have semantic and computational meaning; helping researchers to
retrace, share, and recompute their sense-making process. Attempting to create
qualitative ""themes"" rather than ""topics,"" our NLP approach tunes an ML model
to ""think like you"" without significant retraining. Here, we present our
two-year design process and validation of Teleoscope, including a multi-week
study with qualitative researchers (N = 5), a six-month field deployment with a
qualitative research group, and an on-going public release.",http://arxiv.org/abs/2402.06124v1,80,16,18,16,16,14
Link-aware link prediction over temporal graph by pattern recognition,"A temporal graph can be considered as a stream of links, each of which
represents an interaction between two nodes at a certain time. On temporal
graphs, link prediction is a common task, which aims to answer whether the
query link is true or not. To do this task, previous methods usually focus on
the learning of representations of the two nodes in the query link. We point
out that the learned representation by their models may encode too much
information with side effects for link prediction because they have not
utilized the information of the query link, i.e., they are link-unaware. Based
on this observation, we propose a link-aware model: historical links and the
query link are input together into the following model layers to distinguish
whether this input implies a reasonable pattern that ends with the query link.
During this process, we focus on the modeling of link evolution patterns rather
than node representations. Experiments on six datasets show that our model
achieves strong performances compared with state-of-the-art baselines, and the
results of link prediction are interpretable. The code and datasets are
available on the project website: https://github.com/lbq8942/TGACN.",http://arxiv.org/abs/2402.07199v1,80,18,17,17,15,13
A survey of recent methods for addressing AI fairness and bias in biomedicine,"Artificial intelligence (AI) systems have the potential to revolutionize
clinical practices, including improving diagnostic accuracy and surgical
decision-making, while also reducing costs and manpower. However, it is
important to recognize that these systems may perpetuate social inequities or
demonstrate biases, such as those based on race or gender. Such biases can
occur before, during, or after the development of AI models, making it critical
to understand and address potential biases to enable the accurate and reliable
application of AI models in clinical settings. To mitigate bias concerns during
model development, we surveyed recent publications on different debiasing
methods in the fields of biomedical natural language processing (NLP) or
computer vision (CV). Then we discussed the methods that have been applied in
the biomedical domain to address bias. We performed our literature search on
PubMed, ACM digital library, and IEEE Xplore of relevant articles published
between January 2018 and December 2023 using multiple combinations of keywords.
We then filtered the result of 10,041 articles automatically with loose
constraints, and manually inspected the abstracts of the remaining 890 articles
to identify the 55 articles included in this review. Additional articles in the
references are also included in this review. We discuss each method and compare
its strengths and weaknesses. Finally, we review other potential methods from
the general domain that could be applied to biomedicine to address bias and
improve fairness.The bias of AIs in biomedicine can originate from multiple
sources. Existing debiasing methods that focus on algorithms can be categorized
into distributional or algorithmic.",http://arxiv.org/abs/2402.08250v1,80,16,17,18,15,14
Weakly Supervised Learners for Correction of AI Errors with Provable Performance Guarantees,"We present a new methodology for handling AI errors by introducing weakly
supervised AI error correctors with a priori performance guarantees. These AI
correctors are auxiliary maps whose role is to moderate the decisions of some
previously constructed underlying classifier by either approving or rejecting
its decisions. The rejection of a decision can be used as a signal to suggest
abstaining from making a decision. A key technical focus of the work is in
providing performance guarantees for these new AI correctors through bounds on
the probabilities of incorrect decisions. These bounds are distribution
agnostic and do not rely on assumptions on the data dimension. Our empirical
example illustrates how the framework can be applied to improve the performance
of an image classifier in a challenging real-world task where training data are
scarce.",http://arxiv.org/abs/2402.00899v3,80,16,18,18,16,12
Geometry-induced Implicit Regularization in Deep ReLU Neural Networks,"It is well known that neural networks with many more parameters than training
examples do not overfit. Implicit regularization phenomena, which are still not
well understood, occur during optimization and 'good' networks are favored.
Thus the number of parameters is not an adequate measure of complexity if we do
not consider all possible networks but only the 'good' ones. To better
understand which networks are favored during optimization, we study the
geometry of the output set as parameters vary. When the inputs are fixed, we
prove that the dimension of this set changes and that the local dimension,
called batch functional dimension, is almost surely determined by the
activation patterns in the hidden layers. We prove that the batch functional
dimension is invariant to the symmetries of the network parameterization:
neuron permutations and positive rescalings. Empirically, we establish that the
batch functional dimension decreases during optimization. As a consequence,
optimization leads to parameters with low batch functional dimensions. We call
this phenomenon geometry-induced implicit regularization.The batch functional
dimension depends on both the network parameters and inputs. To understand the
impact of the inputs, we study, for fixed parameters, the largest attainable
batch functional dimension when the inputs vary. We prove that this quantity,
called computable full functional dimension, is also invariant to the
symmetries of the network's parameterization, and is determined by the
achievable activation patterns. We also provide a sampling theorem, showing a
fast convergence of the estimation of the computable full functional dimension
for a random input of increasing size. Empirically we find that the computable
full functional dimension remains close to the number of parameters, which is
related to the notion of local identifiability. This differs from the observed
values for the batch functional dimension computed on training inputs and test
inputs. The latter are influenced by geometry-induced implicit regularization.",http://arxiv.org/abs/2402.08269v1,80,15,18,17,15,15
Learned Image Compression with Text Quality Enhancement,"Learned image compression has gained widespread popularity for their
efficiency in achieving ultra-low bit-rates. Yet, images containing substantial
textual content, particularly screen-content images (SCI), often suffers from
text distortion at such compressed levels. To address this, we propose to
minimize a novel text logit loss designed to quantify the disparity in text
between the original and reconstructed images, thereby improving the perceptual
quality of the reconstructed text. Through rigorous experimentation across
diverse datasets and employing state-of-the-art algorithms, our findings reveal
significant enhancements in the quality of reconstructed text upon integration
of the proposed loss function with appropriate weighting. Notably, we achieve a
Bjontegaard delta (BD) rate of -32.64% for Character Error Rate (CER) and
-28.03% for Word Error Rate (WER) on average by applying the text logit loss
for two screenshot datasets. Additionally, we present quantitative metrics
tailored for evaluating text quality in image compression tasks. Our findings
underscore the efficacy and potential applicability of our proposed text logit
loss function across various text-aware image compression contexts.",http://arxiv.org/abs/2402.08643v1,80,16,20,16,18,10
Buffer Overflow in Mixture of Experts,"Mixture of Experts (MoE) has become a key ingredient for scaling large
foundation models while keeping inference costs steady. We show that expert
routing strategies that have cross-batch dependencies are vulnerable to
attacks. Malicious queries can be sent to a model and can affect a model's
output on other benign queries if they are grouped in the same batch. We
demonstrate this via a proof-of-concept attack in a toy experimental setting.",http://arxiv.org/abs/2402.05526v1,80,16,18,16,16,14
Generative Nowcasting of Marine Fog Visibility in the Grand Banks area and Sable Island in Canada,"This study presents the application of generative deep learning techniques to
evaluate marine fog visibility nowcasting using the FATIMA (Fog and turbulence
interactions in the marine atmosphere) campaign observations collected during
July 2022 in the North Atlantic in the Grand Banks area and vicinity of Sable
Island (SI), northeast of Canada. The measurements were collected using the
Vaisala Forward Scatter Sensor model FD70 and Weather Transmitter model WXT50,
and Gill R3A ultrasonic anemometer mounted on the Research Vessel Atlantic
Condor. To perform nowcasting, the time series of fog visibility (Vis), wind
speed, dew point depression, and relative humidity with respect to water were
preprocessed to have lagged time step features. Generative nowcasting of Vis
time series for lead times of 30 and 60 minutes were performed using
conditional generative adversarial networks (cGAN) regression at visibility
thresholds of Vis < 1 km and < 10 km. Extreme gradient boosting (XGBoost) was
used as a baseline method for comparison against cGAN. At the 30 min lead time,
Vis was best predicted with cGAN at Vis < 1 km (RMSE = 0.151 km) and with
XGBoost at Vis < 10 km (RMSE = 2.821 km). At the 60 min lead time, Vis was best
predicted with XGBoost at Vis < 1 km (RMSE = 0.167 km) and Vis < 10 km (RMSE =
3.508 km), but the cGAN RMSE was similar to XGBoost. Despite nowcasting Vis at
30 min being quite difficult, the ability of the cGAN model to track the
variation in Vis at 1 km suggests that there is potential for generative
analysis of marine fog visibility using observational meteorological
parameters.",http://arxiv.org/abs/2402.06800v1,80,14,16,16,18,16
LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation,"Low-Rank Adaptation (LoRA) introduces auxiliary parameters for each layer to
fine-tune the pre-trained model under limited computing resources. But it still
faces challenges of resource consumption when scaling up to larger models.
Previous studies employ pruning techniques by evaluating the importance of LoRA
parameters for different layers to address the problem. However, these efforts
only analyzed parameter features to evaluate their importance. Indeed, the
output of LoRA related to the parameters and data is the factor that directly
impacts the frozen model. To this end, we propose LoRA-drop which evaluates the
importance of the parameters by analyzing the LoRA output. We retain LoRA for
important layers and the LoRA of the other layers share the same parameters.
Abundant experiments on NLU and NLG tasks demonstrate the effectiveness of
LoRA-drop.",http://arxiv.org/abs/2402.07721v1,80,16,16,18,16,14
Findings of the First Workshop on Simulating Conversational Intelligence in Chat,"The aim of this workshop is to bring together experts working on open-domain
dialogue research. In this speedily advancing research area many challenges
still exist, such as learning information from conversations, engaging in
realistic and convincing simulation of human intelligence and reasoning.
SCI-CHAT follows previous workshops on open domain dialogue but with a focus on
the simulation of intelligent conversation as judged in a live human
evaluation. Models aim to include the ability to follow a challenging topic
over a multi-turn conversation, while positing, refuting and reasoning over
arguments. The workshop included both a research track and shared task. The
main goal of this paper is to provide an overview of the shared task and a link
to an additional paper that will include an in depth analysis of the shared
task results following presentation at the workshop.",http://arxiv.org/abs/2402.06420v1,80,15,17,15,18,15
Is Adversarial Training with Compressed Datasets Effective?,"Dataset Condensation (DC) refers to the recent class of dataset compression
methods that generate a smaller, synthetic, dataset from a larger dataset. This
synthetic dataset retains the essential information of the original dataset,
enabling models trained on it to achieve performance levels comparable to those
trained on the full dataset. Most current DC methods have mainly concerned with
achieving high test performance with limited data budget, and have not directly
addressed the question of adversarial robustness. In this work, we investigate
the impact of adversarial robustness on models trained with compressed
datasets. We show that the compressed datasets obtained from DC methods are not
effective in transferring adversarial robustness to models. As a solution to
improve dataset compression efficiency and adversarial robustness
simultaneously, we propose a novel robustness-aware dataset compression method
based on finding the Minimal Finite Covering (MFC) of the dataset. The proposed
method is (1) obtained by one-time computation and is applicable for any model,
(2) more effective than DC methods when applying adversarial training over MFC,
(3) provably robust by minimizing the generalized adversarial loss.
Additionally, empirical evaluation on three datasets shows that the proposed
method is able to achieve better robustness and performance trade-off compared
to DC methods such as distribution matching.",http://arxiv.org/abs/2402.05675v1,80,16,18,16,16,14
Refined Sample Complexity for Markov Games with Independent Linear Function Approximation,"Markov Games (MG) is an important model for Multi-Agent Reinforcement
Learning (MARL). It was long believed that the ""curse of multi-agents"" (i.e.,
the algorithmic performance drops exponentially with the number of agents) is
unavoidable until several recent works (Daskalakis et al., 2023; Cui et al.,
2023; Wang et al., 2023. While these works did resolve the curse of
multi-agents, when the state spaces are prohibitively large and (linear)
function approximations are deployed, they either had a slower convergence rate
of $O(T^{-1/4})$ or brought a polynomial dependency on the number of actions
$A_{\max}$ -- which is avoidable in single-agent cases even when the loss
functions can arbitrarily vary with time (Dai et al., 2023). This paper first
refines the `AVLPR` framework by Wang et al. (2023), with an insight of
*data-dependent* (i.e., stochastic) pessimistic estimation of the
sub-optimality gap, allowing a broader choice of plug-in algorithms. When
specialized to MGs with independent linear function approximations, we propose
novel *action-dependent bonuses* to cover occasionally extreme estimation
errors. With the help of state-of-the-art techniques from the single-agent RL
literature, we give the first algorithm that tackles the curse of multi-agents,
attains the optimal $O(T^{-1/2})$ convergence rate, and avoids
$\text{poly}(A_{\max})$ dependency simultaneously.",http://arxiv.org/abs/2402.07082v1,80,18,16,18,16,12
Fast UCB-type algorithms for stochastic bandits with heavy and super heavy symmetric noise,"In this study, we propose a new method for constructing UCB-type algorithms
for stochastic multi-armed bandits based on general convex optimization methods
with an inexact oracle. We derive the regret bounds corresponding to the
convergence rates of the optimization methods. We propose a new algorithm
Clipped-SGD-UCB and show, both theoretically and empirically, that in the case
of symmetric noise in the reward, we can achieve an $O(\log T\sqrt{KT\log T})$
regret bound instead of $O\left (T^{\frac{1}{1+\alpha}}
K^{\frac{\alpha}{1+\alpha}} \right)$ for the case when the reward distribution
satisfies $\mathbb{E}_{X \in D}[|X|^{1+\alpha}] \leq \sigma^{1+\alpha}$
($\alpha \in (0, 1])$, i.e. perform better than it is assumed by the general
lower bound for bandits with heavy-tails. Moreover, the same bound holds even
when the reward distribution does not have the expectation, that is, when
$\alpha<0$.",http://arxiv.org/abs/2402.07062v1,80,16,18,18,16,12
Prompt Design and Engineering: Introduction and Advanced Methods,"Prompt design and engineering has rapidly become essential for maximizing the
potential of large language models. In this paper, we introduce core concepts,
advanced techniques like Chain-of-Thought and Reflection, and the principles
behind building LLM-based agents. Finally, we provide a survey of tools for
prompt engineers.",http://arxiv.org/abs/2401.14423v3,80,16,18,16,16,14
An Algorithmic Framework for Constructing Multiple Decision Trees by Evaluating Their Combination Performance Throughout the Construction Process,"Predictions using a combination of decision trees are known to be effective
in machine learning. Typical ideas for constructing a combination of decision
trees for prediction are bagging and boosting. Bagging independently constructs
decision trees without evaluating their combination performance and averages
them afterward. Boosting constructs decision trees sequentially, only
evaluating a combination performance of a new decision tree and the fixed past
decision trees at each step. Therefore, neither method directly constructs nor
evaluates a combination of decision trees for the final prediction. When the
final prediction is based on a combination of decision trees, it is natural to
evaluate the appropriateness of the combination when constructing them. In this
study, we propose a new algorithmic framework that constructs decision trees
simultaneously and evaluates their combination performance throughout the
construction process. Our framework repeats two procedures. In the first
procedure, we construct new candidates of combinations of decision trees to
find a proper combination of decision trees. In the second procedure, we
evaluate each combination performance of decision trees under some criteria and
select a better combination. To confirm the performance of the proposed
framework, we perform experiments on synthetic and benchmark data.",http://arxiv.org/abs/2402.06452v1,80,16,18,16,16,14
ShiftDTW: adapting the DTW metric for cyclic time series clustering,"The elasticity of the DTW metric provides a more flexible comparison between
time series and is used in numerous machine learning domains such as
classification or clustering. However, it does not align the measurements at
the beginning and end of time series if they have a shift occurring right at
the start of one series, with the omitted part appearing at the end of that
series. Due to the cyclicity of such series - which lack a definite beginning
or end - we rely on the Cyclic DTW approach to propose a less computationally
expensive approximation of this calculation method. This approximation will
then be employed in conjunction with the K-Means clustering method.",http://arxiv.org/abs/2402.05631v2,80,17,16,17,15,15
Transition Constrained Bayesian Optimization via Markov Decision Processes,"Bayesian optimization is a methodology to optimize black-box functions.
Traditionally, it focuses on the setting where you can arbitrarily query the
search space. However, many real-life problems do not offer this flexibility;
in particular, the search space of the next query may depend on previous ones.
Example challenges arise in the physical sciences in the form of local movement
constraints, required monotonicity in certain variables, and transitions
influencing the accuracy of measurements. Altogether, such transition
constraints necessitate a form of planning. This work extends Bayesian
optimization via the framework of Markov Decision Processes, iteratively
solving a tractable linearization of our objective using reinforcement learning
to obtain a policy that plans ahead over long horizons. The resulting policy is
potentially history-dependent and non-Markovian. We showcase applications in
chemical reactor optimization, informative path planning, machine calibration,
and other synthetic examples.",http://arxiv.org/abs/2402.08406v1,80,16,17,16,17,14
Scalable Wasserstein Gradient Flow for Generative Modeling through Unbalanced Optimal Transport,"Wasserstein Gradient Flow (WGF) describes the gradient dynamics of
probability density within the Wasserstein space. WGF provides a promising
approach for conducting optimization over the probability distributions.
Numerically approximating the continuous WGF requires the time discretization
method. The most well-known method for this is the JKO scheme. In this regard,
previous WGF models employ the JKO scheme and parametrize transport map for
each JKO step. However, this approach results in quadratic training complexity
$O(K^2)$ with the number of JKO step $K$. This severely limits the scalability
of WGF models. In this paper, we introduce a scalable WGF-based generative
model, called Semi-dual JKO (S-JKO). Our model is based on the semi-dual form
of the JKO step, derived from the equivalence between the JKO step and the
Unbalanced Optimal Transport. Our approach reduces the training complexity to
$O(K)$. We demonstrate that our model significantly outperforms existing
WGF-based generative models, achieving FID scores of 2.62 on CIFAR-10 and 6.19
on CelebA-HQ-256, which are comparable to state-of-the-art image generative
models.",http://arxiv.org/abs/2402.05443v1,80,16,16,16,16,16
Strategically-Robust Learning Algorithms for Bidding in First-Price Auctions,"Learning to bid in repeated first-price auctions is a fundamental problem at
the interface of game theory and machine learning, which has seen a recent
surge in interest due to the transition of display advertising to first-price
auctions. In this work, we propose a novel concave formulation for
pure-strategy bidding in first-price auctions, and use it to analyze natural
Gradient-Ascent-based algorithms for this problem. Importantly, our analysis
goes beyond regret, which was the typical focus of past work, and also accounts
for the strategic backdrop of online-advertising markets where bidding
algorithms are deployed -- we prove that our algorithms cannot be exploited by
a strategic seller and that they incentivize truth-telling for the buyer.
  Concretely, we show that our algorithms achieve $O(\sqrt{T})$ regret when the
highest competing bids are generated adversarially, and show that no online
algorithm can do better. We further prove that the regret improves to $O(\log
T)$ when the competition is stationary and stochastic. Moving beyond regret, we
show that a strategic seller cannot exploit our algorithms to extract more
revenue on average than is possible under the optimal mechanism, i.e., the
seller cannot do much better than posting the monopoly reserve price in each
auction. Finally, we prove that our algorithm is also incentive compatible --
it is a (nearly) dominant strategy for the buyer to report her values
truthfully to the algorithm as a whole.",http://arxiv.org/abs/2402.07363v1,80,18,16,16,16,14
On Differentially Private Subspace Estimation Without Distributional Assumptions,"Private data analysis faces a significant challenge known as the curse of
dimensionality, leading to increased costs. However, many datasets possess an
inherent low-dimensional structure. For instance, during optimization via
gradient descent, the gradients frequently reside near a low-dimensional
subspace. If the low-dimensional structure could be privately identified using
a small amount of points, we could avoid paying (in terms of privacy and
accuracy) for the high ambient dimension.
  On the negative side, Dwork, Talwar, Thakurta, and Zhang (STOC 2014) proved
that privately estimating subspaces, in general, requires an amount of points
that depends on the dimension. But Singhal and Steinke (NeurIPS 2021) bypassed
this limitation by considering points that are i.i.d. samples from a Gaussian
distribution whose covariance matrix has a certain eigenvalue gap. Yet, it was
still left unclear whether we could provide similar upper bounds without
distributional assumptions and whether we could prove lower bounds that depend
on similar eigenvalue gaps.
  In this work, we make progress in both directions. We formulate the problem
of private subspace estimation under two different types of singular value gaps
of the input data and prove new upper and lower bounds for both types. In
particular, our results determine what type of gap is sufficient and necessary
for estimating a subspace with an amount of points that is independent of the
dimension.",http://arxiv.org/abs/2402.06465v1,80,16,18,16,16,14
Lessons Learned from Mining the Hugging Face Repository,"The rapidly evolving fields of Machine Learning (ML) and Artificial
Intelligence have witnessed the emergence of platforms like Hugging Face (HF)
as central hubs for model development and sharing. This experience report
synthesizes insights from two comprehensive studies conducted on HF, focusing
on carbon emissions and the evolutionary and maintenance aspects of ML models.
Our objective is to provide a practical guide for future researchers embarking
on mining software repository studies within the HF ecosystem to enhance the
quality of these studies. We delve into the intricacies of the replication
package used in our studies, highlighting the pivotal tools and methodologies
that facilitated our analysis. Furthermore, we propose a nuanced stratified
sampling strategy tailored for the diverse HF Hub dataset, ensuring a
representative and comprehensive analytical approach. The report also
introduces preliminary guidelines, transitioning from repository mining to
cohort studies, to establish causality in repository mining studies,
particularly within the ML model of HF context. This transition is inspired by
existing frameworks and is adapted to suit the unique characteristics of the HF
model ecosystem. Our report serves as a guiding framework for researchers,
contributing to the responsible and sustainable advancement of ML, and
fostering a deeper understanding of the broader implications of ML models.",http://arxiv.org/abs/2402.07323v1,80,16,20,16,16,12
Exploring the Impact of In-Browser Deep Learning Inference on Quality of User Experience and Performance,"Deep Learning (DL) is increasingly being integrated into Web applications
through a method known as ""in-browser inference"", where the DL processes occur
directly within Web browsers. However, the actual performance of this method
and its effect on user experience quality (QoE) is not well-understood. This
gap in knowledge necessitates new forms of QoE measurement, going beyond
traditional metrics such as page load time. To address this, we conducted the
first extensive performance evaluation of in-browser inference. We introduced
new metrics for this purpose: responsiveness, smoothness, and inference
accuracy.
  Our thorough study included 9 widely-used DL models and tested them across 50
popular PC Web browsers. The findings show a significant latency issue with
in-browser inference: it's on average 16.9 times slower on CPU and 4.9 times
slower on GPU than native inference methods. Several factors contribute to this
latency, including underused hardware instruction sets, inherent delays in the
runtime environment, resource competition within the browser, and
inefficiencies in software libraries and GPU abstractions.
  Moreover, in-browser inference demands a lot of memory, sometimes up to 334.6
times more than the size of the DL models themselves. This excessive memory
usage is partly due to suboptimal memory management. Additionally, we noticed
that in-browser inference increases the time it takes for graphical user
interface (GUI) components to load in web browsers by a significant 67.2\%,
which severely impacts the overall QoE for users of web applications that
depend on this technology.",http://arxiv.org/abs/2402.05981v1,80,16,18,16,16,14
Classifying point clouds at the facade-level using geometric features and deep learning networks,"3D building models with facade details are playing an important role in many
applications now. Classifying point clouds at facade-level is key to create
such digital replicas of the real world. However, few studies have focused on
such detailed classification with deep neural networks. We propose a method
fusing geometric features with deep learning networks for point cloud
classification at facade-level. Our experiments conclude that such early-fused
features improve deep learning methods' performance. This method can be applied
for compensating deep learning networks' ability in capturing local geometric
information and promoting the advancement of semantic segmentation.",http://arxiv.org/abs/2402.06506v1,80,18,16,16,16,14
Random Geometric Graph Alignment with Graph Neural Networks,"We characterize the performance of graph neural networks for graph alignment
problems in the presence of vertex feature information. More specifically,
given two graphs that are independent perturbations of a single random
geometric graph with noisy sparse features, the task is to recover an unknown
one-to-one mapping between the vertices of the two graphs. We show under
certain conditions on the sparsity and noise level of the feature vectors, a
carefully designed one-layer graph neural network can with high probability
recover the correct alignment between the vertices with the help of the graph
structure. We also prove that our conditions on the noise level are tight up to
logarithmic factors. Finally we compare the performance of the graph neural
network to directly solving an assignment problem on the noisy vertex features.
We demonstrate that when the noise level is at least constant this direct
matching fails to have perfect recovery while the graph neural network can
tolerate noise level growing as fast as a power of the size of the graph.",http://arxiv.org/abs/2402.07340v1,80,16,18,16,14,16
SelfSwapper: Self-Supervised Face Swapping via Shape Agnostic Masked AutoEncoder,"Face swapping has gained significant attention for its varied applications.
The majority of previous face swapping approaches have relied on the seesaw
game training scheme, which often leads to the instability of the model
training and results in undesired samples with blended identities due to the
target identity leakage problem. This paper introduces the Shape Agnostic
Masked AutoEncoder (SAMAE) training scheme, a novel self-supervised approach
designed to enhance face swapping model training. Our training scheme addresses
the limitations of traditional training methods by circumventing the
conventional seesaw game and introducing clear ground truth through its
self-reconstruction training regime. It effectively mitigates identity leakage
by masking facial regions of the input images and utilizing learned
disentangled identity and non-identity features. Additionally, we tackle the
shape misalignment problem with new techniques including perforation confusion
and random mesh scaling, and establishes a new state-of-the-art, surpassing
other baseline methods, preserving both identity and non-identity attributes,
without sacrificing on either aspect.",http://arxiv.org/abs/2402.07370v1,80,16,18,16,16,14
BdSLW60: A Word-Level Bangla Sign Language Dataset,"Sign language discourse is an essential mode of daily communication for the
deaf and hard-of-hearing people. However, research on Bangla Sign Language
(BdSL) faces notable limitations, primarily due to the lack of datasets.
Recognizing wordlevel signs in BdSL (WL-BdSL) presents a multitude of
challenges, including the need for well-annotated datasets, capturing the
dynamic nature of sign gestures from facial or hand landmarks, developing
suitable machine learning or deep learning-based models with substantial video
samples, and so on. In this paper, we address these challenges by creating a
comprehensive BdSL word-level dataset named BdSLW60 in an unconstrained and
natural setting, allowing positional and temporal variations and allowing sign
users to change hand dominance freely. The dataset encompasses 60 Bangla sign
words, with a significant scale of 9307 video trials provided by 18 signers
under the supervision of a sign language professional. The dataset was
rigorously annotated and cross-checked by 60 annotators. We also introduced a
unique approach of a relative quantization-based key frame encoding technique
for landmark based sign gesture recognition. We report the benchmarking of our
BdSLW60 dataset using the Support Vector Machine (SVM) with testing accuracy up
to 67.6% and an attention-based bi-LSTM with testing accuracy up to 75.1%. The
dataset is available at https://www.kaggle.com/datasets/hasaniut/bdslw60 and
the code base is accessible from https://github.com/hasanssl/BdSLW60_Code.",http://arxiv.org/abs/2402.08635v1,80,18,16,18,16,12
Learning to Control Emulated Muscles in Real Robots: Towards Exploiting Bio-Inspired Actuator Morphology,"Recent studies have demonstrated the immense potential of exploiting muscle
actuator morphology for natural and robust movement -- in simulation. A
validation on real robotic hardware is yet missing. In this study, we emulate
muscle actuator properties on hardware in real-time, taking advantage of modern
and affordable electric motors. We demonstrate that our setup can emulate a
simplified muscle model on a real robot while being controlled by a learned
policy. We improve upon an existing muscle model by deriving a damping rule
that ensures that the model is not only performant and stable but also tuneable
for the real hardware. Our policies are trained by reinforcement learning
entirely in simulation, where we show that previously reported benefits of
muscles extend to the case of quadruped locomotion and hopping: the learned
policies are more robust and exhibit more regular gaits. Finally, we confirm
that the learned policies can be executed on real hardware and show that
sim-to-real transfer with real-time emulated muscles on a quadruped robot is
possible. These results show that artificial muscles can be highly beneficial
actuators for future generations of robust legged robots.",http://arxiv.org/abs/2402.05371v1,80,18,16,18,16,12
Analysing the Sample Complexity of Opponent Shaping,"Learning in general-sum games often yields collectively sub-optimal results.
Addressing this, opponent shaping (OS) methods actively guide the learning
processes of other agents, empirically leading to improved individual and group
performances in many settings. Early OS methods use higher-order derivatives to
shape the learning of co-players, making them unsuitable for shaping multiple
learning steps. Follow-up work, Model-free Opponent Shaping (M-FOS), addresses
these by reframing the OS problem as a meta-game. In contrast to early OS
methods, there is little theoretical understanding of the M-FOS framework.
Providing theoretical guarantees for M-FOS is hard because A) there is little
literature on theoretical sample complexity bounds for meta-reinforcement
learning B) M-FOS operates in continuous state and action spaces, so
theoretical analysis is challenging. In this work, we present R-FOS, a tabular
version of M-FOS that is more suitable for theoretical analysis. R-FOS
discretises the continuous meta-game MDP into a tabular MDP. Within this
discretised MDP, we adapt the $R_{max}$ algorithm, most prominently used to
derive PAC-bounds for MDPs, as the meta-learner in the R-FOS algorithm. We
derive a sample complexity bound that is exponential in the cardinality of the
inner state and action space and the number of agents. Our bound guarantees
that, with high probability, the final policy learned by an R-FOS agent is
close to the optimal policy, apart from a constant factor. Finally, we
investigate how R-FOS's sample complexity scales in the size of state-action
space. Our theoretical results on scaling are supported empirically in the
Matching Pennies environment.",http://arxiv.org/abs/2402.05782v1,80,16,16,16,16,16
Learning Optimal Tax Design in Nonatomic Congestion Games,"We study how to learn the optimal tax design to maximize the efficiency in
nonatomic congestion games. It is known that self-interested behavior among the
players can damage the system's efficiency. Tax mechanisms is a common method
to alleviate this issue and induce socially optimal behavior. In this work, we
take the initial step for learning the optimal tax that can minimize the social
cost with \emph{equilibrium feedback}, i.e., the tax designer can only observe
the equilibrium state under the enforced tax. Existing algorithms are not
applicable due to the exponentially large tax function space, nonexistence of
the gradient, and nonconvexity of the objective. To tackle these challenges,
our algorithm leverages several novel components: (1) piece-wise linear tax to
approximate the optimal tax; (2) an extra linear term to guarantee a strongly
convex potential function; (3) efficient subroutine to find the ``boundary''
tax. The algorithm can find an $\epsilon$-optimal tax with $O(\beta
F^2/\epsilon)$ sample complexity, where $\beta$ is the smoothness of the cost
function and $F$ is the number of facilities.",http://arxiv.org/abs/2402.07437v1,80,18,18,16,14,14
Learning Attributed Graphlets: Predictive Graph Mining by Graphlets with Trainable Attribute,"The graph classification problem has been widely studied; however, achieving
an interpretable model with high predictive performance remains a challenging
issue. This paper proposes an interpretable classification algorithm for
attributed graph data, called LAGRA (Learning Attributed GRAphlets). LAGRA
learns importance weights for small attributed subgraphs, called attributed
graphlets (AGs), while simultaneously optimizing their attribute vectors. This
enables us to obtain a combination of subgraph structures and their attribute
vectors that strongly contribute to discriminating different classes. A
significant characteristics of LAGRA is that all the subgraph structures in the
training dataset can be considered as a candidate structures of AGs. This
approach can explore all the potentially important subgraphs exhaustively, but
obviously, a naive implementation can require a large amount of computations.
To mitigate this issue, we propose an efficient pruning strategy by combining
the proximal gradient descent and a graph mining tree search. Our pruning
strategy can ensure that the quality of the solution is maintained compared to
the result without pruning. We empirically demonstrate that LAGRA has superior
or comparable prediction performance to the standard existing algorithms
including graph neural networks, while using only a small number of AGs in an
interpretable manner.",http://arxiv.org/abs/2402.06932v1,80,16,17,16,16,15
PICL: Physics Informed Contrastive Learning for Partial Differential Equations,"Neural operators have recently grown in popularity as Partial Differential
Equation (PDEs) surrogate models. Learning solution functionals, rather than
functions, has proven to be a powerful approach to calculate fast, accurate
solutions to complex PDEs. While much work has been done evaluating neural
operator performance on a wide variety of surrogate modeling tasks, these works
normally evaluate performance on a single equation at a time. In this work, we
develop a novel contrastive pretraining framework utilizing Generalized
Contrastive Loss that improves neural operator generalization across multiple
governing equations simultaneously. Governing equation coefficients are used to
measure ground-truth similarity between systems. A combination of
physics-informed system evolution and latent-space model output are anchored to
input data and used in our distance function. We find that physics-informed
contrastive pretraining improves both accuracy and generalization for the
Fourier Neural Operator in fixed-future task, with comparable performance on
the autoregressive rollout, and superresolution tasks for the 1D Heat,
Burgers', and linear advection equations.",http://arxiv.org/abs/2401.16327v2,80,16,18,18,16,12
SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models,"We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM)
series developed upon SPHINX. To improve the architecture and training
efficiency, we modify the SPHINX framework by removing redundant visual
encoders, bypassing fully-padded sub-images with skip tokens, and simplifying
multi-stage training into a one-stage all-in-one paradigm. To fully unleash the
potential of MLLMs, we assemble a comprehensive multi-domain and multimodal
dataset covering publicly available resources in language, vision, and
vision-language tasks. We further enrich this collection with our curated OCR
intensive and Set-of-Mark datasets, extending the diversity and generality. By
training over different base LLMs including TinyLlama1.1B, InternLM2-7B,
LLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in
parameter size and multilingual capabilities. Comprehensive benchmarking
reveals a strong correlation between the multi-modal performance with the data
and parameter scales. Code and models are released at
https://github.com/Alpha-VLLM/LLaMA2-Accessory",http://arxiv.org/abs/2402.05935v1,80,18,16,16,18,12
A holographic mobile-based application for practicing pronunciation of basic English vocabulary for Spanish speaking children,"This paper describes a holographic mobile-based application designed to help
Spanish-speaking children to practice the pronunciation of basic English
vocabulary words. The mastery of vocabulary is a fundamental step when learning
a language but is often perceived as boring. Producing the correct
pronunciation is frequently regarded as the most difficult and complex skill
for new learners of English. In order to address these problems this research
takes advantage of the power of multi-channel stimuli (sound, image and
interaction) in a mobilebased hologram application in order to motivate
students and improve their experience of practicing. We adapted the
prize-winning HolograFX game and developed a new mobile application to help
practice English pronunciation. A 3D holographic robot that acts as a virtual
teacher interacts via voice with the children. To test the tool we carried out
an experiment with 70 Spanish pre-school children divided into three classes,
the control group using traditional methods such as images in books and on the
blackboard, and two experimental groups using our drills and practice software.
One experimental group used the mobile application without the holographic game
and the other experimental group used the application with the holographic
game. We performed pre-test and post-test performance assessments, a
satisfaction survey and emotion analysis. The results are very promising. They
show that the use of the holographic mobile-based application had a significant
impact on the children's motivation. It also improved their performance
compared to traditional methods used in the classroom.",http://dx.doi.org/10.1016/j.ijhcs.2018.11.009,80,16,16,16,16,16
Sharp Rates in Dependent Learning Theory: Avoiding Sample Size Deflation for the Square Loss,"In this work, we study statistical learning with dependent ($\beta$-mixing)
data and square loss in a hypothesis class $\mathscr{F}\subset L_{\Psi_p}$
where $\Psi_p$ is the norm $\|f\|_{\Psi_p} \triangleq \sup_{m\geq 1} m^{-1/p}
\|f\|_{L^m} $ for some $p\in [2,\infty]$. Our inquiry is motivated by the
search for a sharp noise interaction term, or variance proxy, in learning with
dependent data. Absent any realizability assumption, typical non-asymptotic
results exhibit variance proxies that are deflated \emph{multiplicatively} by
the mixing time of the underlying covariates process. We show that whenever the
topologies of $L^2$ and $\Psi_p$ are comparable on our hypothesis class
$\mathscr{F}$ -- that is, $\mathscr{F}$ is a weakly sub-Gaussian class:
$\|f\|_{\Psi_p} \lesssim \|f\|_{L^2}^\eta$ for some $\eta\in (0,1]$ -- the
empirical risk minimizer achieves a rate that only depends on the complexity of
the class and second order statistics in its leading term. Our result holds
whether the problem is realizable or not and we refer to this as a \emph{near
mixing-free rate}, since direct dependence on mixing is relegated to an
additive higher order term. We arrive at our result by combining the above
notion of a weakly sub-Gaussian class with mixed tail generic chaining. This
combination allows us to compute sharp, instance-optimal rates for a wide range
of problems. %Our approach, reliant on mixed tail generic chaining, allows us
to obtain sharp, instance-optimal rates. Examples that satisfy our framework
include sub-Gaussian linear regression, more general smoothly parameterized
function classes, finite hypothesis classes, and bounded smoothness classes.",http://arxiv.org/abs/2402.05928v1,80,17,16,18,14,15
Next-Generation Teleophthalmology: AI-enabled Quality Assessment Aiding Remote Smartphone-based Consultation,"Blindness and other eye diseases are a global health concern, particularly in
low- and middle-income countries like India. In this regard, during the
COVID-19 pandemic, teleophthalmology became a lifeline, and the Grabi
attachment for smartphone-based eye imaging gained in use. However, quality of
user-captured image often remained inadequate, requiring clinician vetting and
delays. In this backdrop, we propose an AI-based quality assessment system with
instant feedback mimicking clinicians' judgments and tested on patient-captured
images. Dividing the complex problem hierarchically, here we tackle a
nontrivial part, and demonstrate a proof of the concept.",http://arxiv.org/abs/2402.07118v1,80,16,18,16,18,12
Nonlinear electro-elastic finite element analysis with neural network constitutive models,"In the present work, the applicability of physics-augmented neural network
(PANN) constitutive models for complex electro-elastic finite element analysis
is demonstrated. For the investigations, PANN models for electro-elastic
material behavior at finite deformations are calibrated to different
synthetically generated datasets, including an analytical isotropic potential,
a homogenised rank-one laminate, and a homogenised metamaterial with a
spherical inclusion. Subsequently, boundary value problems inspired by
engineering applications of composite electro-elastic materials are considered.
Scenarios with large electrically induced deformations and instabilities are
particularly challenging and thus necessitate extensive investigations of the
PANN constitutive models in the context of finite element analyses. First of
all, an excellent prediction quality of the model is required for very general
load cases occurring in the simulation. Furthermore, simulation of large
deformations and instabilities poses challenges on the stability of the
numerical solver, which is closely related to the constitutive model. In all
cases studied, the PANN models yield excellent prediction qualities and a
stable numerical behavior even in highly nonlinear scenarios. This can be
traced back to the PANN models excellent performance in learning both the first
and second derivatives of the ground truth electro-elastic potentials, even
though it is only calibrated on the first derivatives. Overall, this work
demonstrates the applicability of PANN constitutive models for the efficient
and robust simulation of engineering applications of composite electro-elastic
materials.",http://arxiv.org/abs/2402.07007v1,80,16,18,16,16,14
Investigating Generalization Behaviours of Generative Flow Networks,"Generative Flow Networks (GFlowNets, GFNs) are a generative framework for
learning unnormalized probability mass functions over discrete spaces. Since
their inception, GFlowNets have proven to be useful for learning generative
models in applications where the majority of the discrete space is unvisited
during training. This has inspired some to hypothesize that GFlowNets, when
paired with deep neural networks (DNNs), have favourable generalization
properties. In this work, we empirically verify some of the hypothesized
mechanisms of generalization of GFlowNets. In particular, we find that the
functions that GFlowNets learn to approximate have an implicit underlying
structure which facilitate generalization. We also find that GFlowNets are
sensitive to being trained offline and off-policy; however, the reward
implicitly learned by GFlowNets is robust to changes in the training
distribution.",http://arxiv.org/abs/2402.05309v1,80,16,18,16,18,12
Federated Offline Reinforcement Learning: Collaborative Single-Policy Coverage Suffices,"Offline reinforcement learning (RL), which seeks to learn an optimal policy
using offline data, has garnered significant interest due to its potential in
critical applications where online data collection is infeasible or expensive.
This work explores the benefit of federated learning for offline RL, aiming at
collaboratively leveraging offline datasets at multiple agents. Focusing on
finite-horizon episodic tabular Markov decision processes (MDPs), we design
FedLCB-Q, a variant of the popular model-free Q-learning algorithm tailored for
federated offline RL. FedLCB-Q updates local Q-functions at agents with novel
learning rate schedules and aggregates them at a central server using
importance averaging and a carefully designed pessimistic penalty term. Our
sample complexity analysis reveals that, with appropriately chosen parameters
and synchronization schedules, FedLCB-Q achieves linear speedup in terms of the
number of agents without requiring high-quality datasets at individual agents,
as long as the local datasets collectively cover the state-action space visited
by the optimal policy, highlighting the power of collaboration in the federated
setting. In fact, the sample complexity almost matches that of the single-agent
counterpart, as if all the data are stored at a central location, up to
polynomial factors of the horizon length. Furthermore, FedLCB-Q is
communication-efficient, where the number of communication rounds is only
linear with respect to the horizon length up to logarithmic factors.",http://arxiv.org/abs/2402.05876v1,80,18,16,16,16,14
SpeechCLIP+: Self-supervised multi-task representation learning for speech via CLIP and speech-image data,"The recently proposed visually grounded speech model SpeechCLIP is an
innovative framework that bridges speech and text through images via CLIP
without relying on text transcription. On this basis, this paper introduces two
extensions to SpeechCLIP. First, we apply the Continuous Integrate-and-Fire
(CIF) module to replace a fixed number of CLS tokens in the cascaded
architecture. Second, we propose a new hybrid architecture that merges the
cascaded and parallel architectures of SpeechCLIP into a multi-task learning
framework. Our experimental evaluation is performed on the Flickr8k and
SpokenCOCO datasets. The results show that in the speech keyword extraction
task, the CIF-based cascaded SpeechCLIP model outperforms the previous cascaded
SpeechCLIP model using a fixed number of CLS tokens. Furthermore, through our
hybrid architecture, cascaded task learning boosts the performance of the
parallel branch in image-speech retrieval tasks.",http://arxiv.org/abs/2402.06959v1,80,18,17,16,15,14
Comparing skill of historical rainfall data based monsoon rainfall prediction in India with NCEP-NWP forecasts,"In this draft we consider the problem of forecasting rainfall across India
during the four monsoon months, one day as well as three days in advance. We
train neural networks using historical daily gridded precipitation data for
India obtained from IMD for the time period $1901- 2022$, at a spatial
resolution of $1^{\circ} \times 1^{\circ}$. This is compared with the numerical
weather prediction (NWP) forecasts obtained from NCEP (National Centre for
Environmental Prediction) available for the period 2011-2022. We conduct a
detailed country wide analysis and separately analyze some of the most
populated cities in India. Our conclusion is that forecasts obtained by
applying deep learning to historical rainfall data are more accurate compared
to NWP forecasts as well as predictions based on persistence. On average,
compared to our predictions, forecasts from NCEP-NWP model have about 34%
higher error for a single day prediction, and over 68% higher error for a three
day prediction. Similarly, persistence estimates report a 29% higher error in a
single day forecast, and over 54% error in a three day forecast. We further
observe that data up to 20 days in the past is useful in reducing errors of one
and three day forecasts, when a transformer based learning architecture, and to
a lesser extent when an LSTM is used. A key conclusion suggested by our
preliminary analysis is that NWP forecasts can be substantially improved upon
through more and diverse data relevant to monsoon prediction combined with
carefully selected neural network architecture.",http://arxiv.org/abs/2402.07851v1,80,18,16,18,14,14
"Epistemic Power, Objectivity and Gender in AI Ethics Labor: Legitimizing Located Complaints","What counts as legitimate AI ethics labor, and consequently, what are the
epistemic terms on which AI ethics claims are rendered legitimate? Based on 75
interviews with technologists including researchers, developers, open source
contributors, artists, and activists, this paper explores various epistemic
bases from which AI ethics is practiced. In the context of outside attacks on
AI ethics as an impediment to ""progress,"" I show how some AI ethics practices
have reached toward scholarly authority, automation and quantification and
achieved some legitimacy, while those based on richly embodied and situated
lived experience have not. This paper draws the works of feminist Anthropology
and Science and Technology Studies (STS) scholars Diana Forsythe and Lucy
Suchman together with the works of postcolonial feminist theorist Sara Ahmed
and Black feminist theorist Kristie Dotson to examine the implications of
dominant AI ethics practices. I argue that by entrenching the epistemic power
of quantification, dominant AI ethics practices risk legitimizing AI ethics as
a project in equal and opposite measure to the extent that they delegitimize
and marginalize embodied and lived experiences as legitimate parts of the same
project. In response, I propose and sketch the idea of humble technical
practices: quantified or technical practices which specifically seek to make
their epistemic limits clear, with a view to flattening hierarchies of
epistemic power.",http://arxiv.org/abs/2402.08171v1,80,16,20,18,16,10
Previously on the Stories: Recap Snippet Identification for Story Reading,"Similar to the ""previously-on"" scenes in TV shows, recaps can help book
reading by recalling the readers' memory about the important elements in
previous texts to better understand the ongoing plot. Despite its usefulness,
this application has not been well studied in the NLP community. We propose the
first benchmark on this useful task called Recap Snippet Identification with a
hand-crafted evaluation dataset. Our experiments show that the proposed task is
challenging to PLMs, LLMs, and proposed methods as the task requires a deep
understanding of the plot correlation between snippets.",http://arxiv.org/abs/2402.07271v1,80,16,18,16,16,14
Greedy Matchings in Bipartite Graphs with Ordered Vertex Sets,"We define and study greedy matchings in vertex-ordered bipartite graphs. It
is shown that each vertex-ordered bipartite graph has a unique greedy matching.
The proof uses (a weak form of) Newman's lemma. The vertex ordering is called a
preference relation. Given a vertex-ordered bipartite graph, the goal is to
match every vertex of one vertex class but to leave unmatched as many as
possible vertices of low preference in the other concept class. We investigate
how well greedy algorithms perform in this setting. It is shown that they have
optimal performance provided that the vertex-ordering is cleverly chosen. The
study of greedy matchings is motivated by problems in learning theory like
illustrating or teaching concepts by means of labeled examples.",http://arxiv.org/abs/2402.06729v1,79,16,18,15,15,15
Exploring Interaction Patterns for Debugging: Enhancing Conversational Capabilities of AI-assistants,"The widespread availability of Large Language Models (LLMs) within Integrated
Development Environments (IDEs) has led to their speedy adoption.
Conversational interactions with LLMs enable programmers to obtain natural
language explanations for various software development tasks. However, LLMs
often leap to action without sufficient context, giving rise to implicit
assumptions and inaccurate responses. Conversations between developers and LLMs
are primarily structured as question-answer pairs, where the developer is
responsible for asking the the right questions and sustaining conversations
across multiple turns. In this paper, we draw inspiration from interaction
patterns and conversation analysis -- to design Robin, an enhanced
conversational AI-assistant for debugging. Through a within-subjects user study
with 12 industry professionals, we find that equipping the LLM to -- (1)
leverage the insert expansion interaction pattern, (2) facilitate turn-taking,
and (3) utilize debugging workflows -- leads to lowered conversation barriers,
effective fault localization, and 5x improvement in bug resolution rates.",http://arxiv.org/abs/2402.06229v1,78,16,18,16,16,12
SAT-based Learning of Computation Tree Logic,"The CTL learning problem consists in finding for a given sample of positive
and negative Kripke structures a distinguishing CTL formula that is verified by
the former but not by the latter. Further constraints may bound the size and
shape of the desired formula or even ask for its minimality in terms of
syntactic size. This synthesis problem is motivated by explanation generation
for dissimilar models, e.g. comparing a faulty implementation with the original
protocol. We devise a SAT-based encoding for a fixed size CTL formula, then
provide an incremental approach that guarantees minimality. We further report
on a prototype implementation whose contribution is twofold: first, it allows
us to assess the efficiency of various output fragments and optimizations.
Secondly, we can experimentally evaluate this tool by randomly mutating Kripke
structures or syntactically introducing errors in higher-level models, then
learning CTL distinguishing formulas.",http://arxiv.org/abs/2402.06366v2,78,15,16,18,16,13
An Ordinal Regression Framework for a Deep Learning Based Severity Assessment for Chest Radiographs,"This study investigates the application of ordinal regression methods for
categorizing disease severity in chest radiographs. We propose a framework that
divides the ordinal regression problem into three parts: a model, a target
function, and a classification function. Different encoding methods, including
one-hot, Gaussian, progress-bar, and our soft-progress-bar, are applied using
ResNet50 and ViT-B-16 deep learning models. We show that the choice of encoding
has a strong impact on performance and that the best encoding depends on the
chosen weighting of Cohen's kappa and also on the model architecture used. We
make our code publicly available on GitHub.",http://arxiv.org/abs/2402.05685v1,78,16,18,16,16,12
Clustering Techniques Selection for a Hybrid Regression Model: A Case Study Based on a Solar Thermal System,"This work addresses the performance comparison between four clustering
techniques with the objective of achieving strong hybrid models in supervised
learning tasks. A real dataset from a bio-climatic house named Sotavento placed
on experimental wind farm and located in Xermade (Lugo) in Galicia (Spain) has
been collected. Authors have chosen the thermal solar generation system in
order to study how works applying several cluster methods followed by a
regression technique to predict the output temperature of the system. With the
objective of defining the quality of each clustering method two possible
solutions have been implemented. The first one is based on three unsupervised
learning metrics (Silhouette, Calinski-Harabasz and Davies-Bouldin) while the
second one, employs the most common error measurements for a regression
algorithm such as Multi Layer Perceptron.",http://dx.doi.org/10.1080/01969722.2022.2030006,78,16,15,17,15,15
Understanding Test-Time Augmentation,"Test-Time Augmentation (TTA) is a very powerful heuristic that takes
advantage of data augmentation during testing to produce averaged output.
Despite the experimental effectiveness of TTA, there is insufficient discussion
of its theoretical aspects. In this paper, we aim to give theoretical
guarantees for TTA and clarify its behavior.",http://arxiv.org/abs/2402.06892v1,78,15,16,18,15,14
Jacquard V2: Refining Datasets using the Human In the Loop Data Correction Method,"In the context of rapid advancements in industrial automation, vision-based
robotic grasping plays an increasingly crucial role. In order to enhance visual
recognition accuracy, the utilization of large-scale datasets is imperative for
training models to acquire implicit knowledge related to the handling of
various objects. Creating datasets from scratch is a time and labor-intensive
process. Moreover, existing datasets often contain errors due to automated
annotations aimed at expediency, making the improvement of these datasets a
substantial research challenge. Consequently, several issues have been
identified in the annotation of grasp bounding boxes within the popular
Jacquard Grasp. We propose utilizing a Human-In-The-Loop(HIL) method to enhance
dataset quality. This approach relies on backbone deep learning networks to
predict object positions and orientations for robotic grasping. Predictions
with Intersection over Union (IOU) values below 0.2 undergo an assessment by
human operators. After their evaluation, the data is categorized into False
Negatives(FN) and True Negatives(TN). FN are then subcategorized into either
missing annotations or catastrophic labeling errors. Images lacking labels are
augmented with valid grasp bounding box information, whereas images afflicted
by catastrophic labeling errors are completely removed. The open-source tool
Labelbee was employed for 53,026 iterations of HIL dataset enhancement, leading
to the removal of 2,884 images and the incorporation of ground truth
information for 30,292 images. The enhanced dataset, named the Jacquard V2
Grasping Dataset, served as the training data for a range of neural networks.",http://arxiv.org/abs/2402.05747v1,78,16,18,16,14,14
A Change Detection Reality Check,"In recent years, there has been an explosion of proposed change detection
deep learning architectures in the remote sensing literature. These approaches
claim to offer state-of the-art performance on different standard benchmark
datasets. However, has the field truly made significant progress? In this paper
we perform experiments which conclude a simple U-Net segmentation baseline
without training tricks or complicated architectural changes is still a top
performer for the task of change detection.",http://arxiv.org/abs/2402.06994v1,78,16,16,16,16,14
How Much is Unseen Depends Chiefly on Information About the Seen,"It might seem counter-intuitive at first: We find that, in expectation, the
proportion of data points in an unknown population-that belong to classes that
do not appear in the training data-is almost entirely determined by the number
$f_k$ of classes that do appear in the training data the same number of times.
While in theory we show that the difference of the induced estimator decays
exponentially in the size of the sample, in practice the high variance prevents
us from using it directly for an estimator of the sample coverage. However, our
precise characterization of the dependency between $f_k$'s induces a large
search space of different representations of the expected value, which can be
deterministically instantiated as estimators. Hence, we turn to optimization
and develop a genetic algorithm that, given only the sample, searches for an
estimator with minimal mean-squared error (MSE). In our experiments, our
genetic algorithm discovers estimators that have a substantially smaller MSE
than the state-of-the-art Good-Turing estimator. This holds for over 96% of
runs when there are at least as many samples as classes. Our estimators' MSE is
roughly 80% of the Good-Turing estimator's.",http://arxiv.org/abs/2402.05835v1,78,16,14,16,16,16
Embedding Compression for Teacher-to-Student Knowledge Transfer,"Common knowledge distillation methods require the teacher model and the
student model to be trained on the same task. However, the usage of embeddings
as teachers has also been proposed for different source tasks and target tasks.
Prior work that uses embeddings as teachers ignores the fact that the teacher
embeddings are likely to contain irrelevant knowledge for the target task. To
address this problem, we propose to use an embedding compression module with a
trainable teacher transformation to obtain a compact teacher embedding. Results
show that adding the embedding compression module improves the classification
performance, especially for unsupervised teacher embeddings. Moreover, student
models trained with the guidance of embeddings show stronger generalizability.",http://arxiv.org/abs/2402.06761v1,78,15,16,17,15,15
GenEFT: Understanding Statics and Dynamics of Model Generalization via Effective Theory,"We present GenEFT: an effective theory framework for shedding light on the
statics and dynamics of neural network generalization, and illustrate it with
graph learning examples. We first investigate the generalization phase
transition as data size increases, comparing experimental results with
information-theory-based approximations. We find generalization in a Goldilocks
zone where the decoder is neither too weak nor too powerful. We then introduce
an effective theory for the dynamics of representation learning, where
latent-space representations are modeled as interacting particles (repons), and
find that it explains our experimentally observed phase transition between
generalization and overfitting as encoder and decoder learning rates are
scanned. This highlights the power of physics-inspired effective theories for
bridging the gap between theoretical predictions and practice in machine
learning.",http://arxiv.org/abs/2402.05916v1,78,18,16,16,14,14
Towards a Systematic Approach to Design New Ensemble Learning Algorithms,"Ensemble learning has been a focal point of machine learning research due to
its potential to improve predictive performance. This study revisits the
foundational work on ensemble error decomposition, historically confined to
bias-variance-covariance analysis for regression problems since the 1990s.
Recent advancements introduced a ""unified theory of diversity,"" which proposes
an innovative bias-variance-diversity decomposition framework. Leveraging this
contemporary understanding, our research systematically explores the
application of this decomposition to guide the creation of new ensemble
learning algorithms. Focusing on regression tasks, we employ neural networks as
base learners to investigate the practical implications of this theoretical
framework. This approach used 7 simple ensemble methods, we name them
strategies, for neural networks that were used to generate 21 new ensemble
algorithms. Among these, most of the methods aggregated with the snapshot
strategy, one of the 7 strategies used, showcase superior predictive
performance across diverse datasets w.r.t. the Friedman rank test with the
Conover post-hoc test. Our systematic design approach contributes a suite of
effective new algorithms and establishes a structured pathway for future
ensemble learning algorithm development.",http://arxiv.org/abs/2402.06818v1,78,18,16,16,16,12
Artificial Intelligence for Literature Reviews: Opportunities and Challenges,"This manuscript presents a comprehensive review of the use of Artificial
Intelligence (AI) in Systematic Literature Reviews (SLRs). A SLR is a rigorous
and organised methodology that assesses and integrates previous research on a
given topic. Numerous tools have been developed to assist and partially
automate the SLR process. The increasing role of AI in this field shows great
potential in providing more effective support for researchers, moving towards
the semi-automatic creation of literature reviews. Our study focuses on how AI
techniques are applied in the semi-automation of SLRs, specifically in the
screening and extraction phases. We examine 21 leading SLR tools using a
framework that combines 23 traditional features with 11 AI features. We also
analyse 11 recent tools that leverage large language models for searching the
literature and assisting academic writing. Finally, the paper discusses current
trends in the field, outlines key research challenges, and suggests directions
for future research.",http://arxiv.org/abs/2402.08565v1,78,15,16,16,16,15
A Scalable Algorithm for Individually Fair K-means Clustering,"We present a scalable algorithm for the individually fair ($p$,
$k$)-clustering problem introduced by Jung et al. and Mahabadi et al. Given $n$
points $P$ in a metric space, let $\delta(x)$ for $x\in P$ be the radius of the
smallest ball around $x$ containing at least $n / k$ points. A clustering is
then called individually fair if it has centers within distance $\delta(x)$ of
$x$ for each $x\in P$. While good approximation algorithms are known for this
problem no efficient practical algorithms with good theoretical guarantees have
been presented. We design the first fast local-search algorithm that runs in
~$O(nk^2)$ time and obtains a bicriteria $(O(1), 6)$ approximation. Then we
show empirically that not only is our algorithm much faster than prior work,
but it also produces lower-cost solutions.",http://arxiv.org/abs/2402.06730v1,78,16,16,16,16,14
Understanding the Effect of Noise in LLM Training Data with Algorithmic Chains of Thought,"During both pretraining and fine-tuning, Large Language Models
(\textbf{LLMs}) are trained on trillions of tokens of text of widely varying
quality. Both phases of training typically involve heuristically filtering out
``low-quality'' or \textit{noisy} training samples, yet little is known
quantitatively about how the type or intensity of noise affects downstream
performance. In this work, we study how noise in chain of thought
(\textbf{CoT}) impacts task performance in the highly-controlled setting of
algorithmically solvable tasks. First, we develop the Traced Integer
(\textbf{TInt}) framework to generate highly customizable noised execution
traces for any arithmetic function on lists of integers. We then define two
types of noise: \textit{static} noise, a local form of noise which is applied
after the CoT trace is computed, and \textit{dynamic} noise, a global form of
noise which propagates errors in the trace as it is computed. We then evaluate
the test performance of pretrained models both prompted and fine-tuned on
noised datasets with varying levels of dataset contamination and intensity. We
find fine-tuned models are extremely robust to high levels of static noise but
struggle significantly more with lower levels of dynamic noise. In contrast,
few-shot prompted models appear more sensitive to even static noise. We
conclude with a discussion of how our findings impact noise filtering
best-practices, in particular emphasizing the importance of removing samples
containing destructive dynamic noise with global errors.",http://arxiv.org/abs/2402.04004v2,78,18,16,16,14,14
SoftEDA: Rethinking Rule-Based Data Augmentation with Soft Labels,"Rule-based text data augmentation is widely used for NLP tasks due to its
simplicity. However, this method can potentially damage the original meaning of
the text, ultimately hurting the performance of the model. To overcome this
limitation, we propose a straightforward technique for applying soft labels to
augmented data. We conducted experiments across seven different classification
tasks and empirically demonstrated the effectiveness of our proposed approach.
We have publicly opened our source code for reproducibility.",http://arxiv.org/abs/2402.05591v1,78,16,14,16,18,14
Detection of Spider Mites on Labrador Beans through Machine Learning Approaches Using Custom Datasets,"Amidst growing food production demands, early plant disease detection is
essential to safeguard crops; this study proposes a visual machine learning
approach for plant disease detection, harnessing RGB and NIR data collected in
real-world conditions through a JAI FS-1600D-10GE camera to build an RGBN
dataset. A two-stage early plant disease detection model with YOLOv8 and a
sequential CNN was used to train on a dataset with partial labels, which showed
a 3.6% increase in mAP compared to a single-stage end-to-end segmentation
model. The sequential CNN model achieved 90.62% validation accuracy utilising
RGBN data. An average of 6.25% validation accuracy increase is found using RGBN
in classification compared to RGB using ResNet15 and the sequential CNN models.
Further research and dataset improvements are needed to meet food production
demands.",http://arxiv.org/abs/2402.07895v1,78,16,16,16,16,14
Model Editing with Canonical Examples,"We introduce model editing with canonical examples, a setting in which (1) a
single learning example is provided per desired behavior, (2) evaluation is
performed exclusively out-of-distribution, and (3) deviation from an initial
model is strictly limited. A canonical example is a simple instance of good
behavior, e.g., The capital of Mauritius is Port Louis) or bad behavior, e.g.,
An aspect of researchers is coldhearted). The evaluation set contains more
complex examples of each behavior (like a paragraph in which the capital of
Mauritius is called for.) We create three datasets and modify three more for
model editing with canonical examples, covering knowledge-intensive
improvements, social bias mitigation, and syntactic edge cases. In our
experiments on Pythia language models, we find that LoRA outperforms full
finetuning and MEMIT. We then turn to the Backpack language model architecture
because it is intended to enable targeted improvement. The Backpack defines a
large bank of sense vectors--a decomposition of the different uses of each
word--which are weighted and summed to form the output logits of the model. We
propose sense finetuning, which selects and finetunes a few ($\approx$ 10)
sense vectors for each canonical example, and find that it outperforms other
finetuning methods, e.g., 4.8% improvement vs 0.3%. Finally, we improve
GPT-J-6B by an inference-time ensemble with just the changes from sense
finetuning of a 35x smaller Backpack, in one setting outperforming editing
GPT-J itself (4.1% vs 1.0%).",http://arxiv.org/abs/2402.06155v1,78,16,17,15,15,15
The Complexity of Algebraic Algorithms for LWE,"Arora & Ge introduced a noise-free polynomial system to compute the secret of
a Learning With Errors (LWE) instance via linearization. Albrecht et al. later
utilized the Arora-Ge polynomial model to study the complexity of Gr\""obner
basis computations on LWE polynomial systems under the assumption of
semi-regularity. In this paper we revisit the Arora-Ge polynomial and prove
that it satisfies a genericity condition recently introduced by Caminata &
Gorla, called being in generic coordinates. For polynomial systems in generic
coordinates one can always estimate the complexity of DRL Gr\""obner basis
computations in terms of the Castelnuovo-Mumford regularity and henceforth also
via the Macaulay bound.
  Moreover, we generalize the Gr\""obner basis algorithm of Semaev & Tenti to
arbitrary polynomial systems with a finite degree of regularity. In particular,
existence of this algorithm yields another approach to estimate the complexity
of DRL Gr\""obner basis computations in terms of the degree of regularity. In
practice, the degree of regularity of LWE polynomial systems is not known,
though one can always estimate the lowest achievable degree of regularity.
Consequently, from a designer's worst case perspective this approach yields
sub-exponential complexity estimates for general, binary secret, and binary
error LWE.
  In recent works by Dachman-Soled et al. the hardness of LWE in the presence
of side information was analyzed. Utilizing their framework we discuss how
hints can be incorporated into LWE polynomial systems and how they affect the
complexity of Gr\""obner basis computations.",http://arxiv.org/abs/2402.07852v1,76,15,18,18,15,10
Fairness of Exposure in Online Restless Multi-armed Bandits,"Restless multi-armed bandits (RMABs) generalize the multi-armed bandits where
each arm exhibits Markovian behavior and transitions according to their
transition dynamics. Solutions to RMAB exist for both offline and online cases.
However, they do not consider the distribution of pulls among the arms. Studies
have shown that optimal policies lead to unfairness, where some arms are not
exposed enough. Existing works in fairness in RMABs focus heavily on the
offline case, which diminishes their application in real-world scenarios where
the environment is largely unknown. In the online scenario, we propose the
first fair RMAB framework, where each arm receives pulls in proportion to its
merit. We define the merit of an arm as a function of its stationary reward
distribution. We prove that our algorithm achieves sublinear fairness regret in
the single pull case $O(\sqrt{T\ln T})$, with $T$ being the total number of
episodes. Empirically, we show that our algorithm performs well in the
multi-pull scenario as well.",http://arxiv.org/abs/2402.06348v1,76,15,16,17,14,14
Mesoscale Traffic Forecasting for Real-Time Bottleneck and Shockwave Prediction,"Accurate real-time traffic state forecasting plays a pivotal role in traffic
control research. In particular, the CIRCLES consortium project necessitates
predictive techniques to mitigate the impact of data source delays. After the
success of the MegaVanderTest experiment, this paper aims at overcoming the
current system limitations and develop a more suited approach to improve the
real-time traffic state estimation for the next iterations of the experiment.
In this paper, we introduce the SA-LSTM, a deep forecasting method integrating
Self-Attention (SA) on the spatial dimension with Long Short-Term Memory (LSTM)
yielding state-of-the-art results in real-time mesoscale traffic forecasting.
We extend this approach to multi-step forecasting with the n-step SA-LSTM,
which outperforms traditional multi-step forecasting methods in the trade-off
between short-term and long-term predictions, all while operating in real-time.",http://arxiv.org/abs/2402.05663v1,76,16,16,16,18,10
Comparing the willingness to share for human-generated vs. AI-generated fake news,"Generative artificial intelligence (AI) presents large risks for society when
it is used to create fake news. A crucial factor for fake news to go viral on
social media is that users share such content. Here, we aim to shed light on
the sharing behavior of users across human-generated vs. AI-generated fake
news. Specifically, we study: (1) What is the perceived veracity of
human-generated fake news vs. AI-generated fake news? (2) What is the user's
willingness to share human-generated fake news vs. AI-generated fake news on
social media? (3) What socio-economic characteristics let users fall for
AI-generated fake news? To this end, we conducted a pre-registered, online
experiment with $N=$ 988 subjects and 20 fake news from the COVID-19 pandemic
generated by GPT-4 vs. humans. Our findings show that AI-generated fake news is
perceived as less accurate than human-generated fake news, but both tend to be
shared equally. Further, several socio-economic factors explain who falls for
AI-generated fake news.",http://arxiv.org/abs/2402.07395v1,76,16,16,16,14,14
Designing NLP-based solutions for requirements variability management: experiences from a design science study at Visma,"Context and motivation: In this industry-academia collaborative project, a
team of researchers, supported by a software architect, business analyst, and
test engineer explored the challenges of requirement variability in a large
business software development company. Question/problem: Following the design
science paradigm, we studied the problem of requirements analysis and tracing
in the context of contractual documents, with a specific focus on managing
requirements variability. This paper reports on the lessons learned from that
experience, highlighting the strategies and insights gained in the realm of
requirements variability management. Principal ideas/results: This experience
report outlines the insights gained from applying design science in
requirements engineering research in industry. We show and evaluate various
strategies to tackle the issue of requirement variability. Contribution: We
report on the iterations and how the solution development evolved in parallel
with problem understanding. From this process, we derive five key lessons
learned to highlight the effectiveness of design science in exploring solutions
for requirement variability in contract-based environments.",http://arxiv.org/abs/2402.07145v1,76,16,14,16,18,12
What is Hiding in Medicine's Dark Matter? Learning with Missing Data in Medical Practices,"Electronic patient records (EPRs) produce a wealth of data but contain
significant missing information. Understanding and handling this missing data
is an important part of clinical data analysis and if left unaddressed could
result in bias in analysis and distortion in critical conclusions. Missing data
may be linked to health care professional practice patterns and imputation of
missing data can increase the validity of clinical decisions. This study
focuses on statistical approaches for understanding and interpreting the
missing data and machine learning based clinical data imputation using a single
centre's paediatric emergency data and the data from UK's largest clinical
audit for traumatic injury database (TARN). In the study of 56,961 data points
related to initial vital signs and observations taken on children presenting to
an Emergency Department, we have shown that missing data are likely to be
non-random and how these are linked to health care professional practice
patterns. We have then examined 79 TARN fields with missing values for 5,791
trauma cases. Singular Value Decomposition (SVD) and k-Nearest Neighbour (kNN)
based missing data imputation methods are used and imputation results against
the original dataset are compared and statistically tested. We have concluded
that the 1NN imputer is the best imputation which indicates a usual pattern of
clinical decision making: find the most similar patients and take their
attributes as imputation.",http://dx.doi.org/10.1109/BigData59044.2023.10386194,76,16,18,16,14,12
Make it more specific: A novel uncertainty based airway segmentation application on 3D U-Net and its variants,"Each medical segmentation task should be considered with a specific AI
algorithm based on its scenario so that the most accurate prediction model can
be obtained. The most popular algorithms in medical segmentation, 3D U-Net and
its variants, can directly implement the task of lung trachea segmentation, but
its failure to consider the special tree-like structure of the trachea suggests
that there is much room for improvement in its segmentation accuracy.
Therefore, a research gap exists because a great amount of state-of-the-art DL
algorithms are vanilla 3D U-Net structures, which do not introduce the various
performance-enhancing modules that come with special natural image modality in
lung airway segmentation. In this paper, we proposed two different network
structures Branch-Level U-Net (B-UNet) and Branch-Level CE-UNet (B-CE-UNet)
which are based on U-Net structure and compared the prediction results with the
same dataset. Specially, both of the two networks add branch loss and central
line loss to learn the feature of fine branch endings of the airways.
Uncertainty estimation algorithms are also included to attain confident
predictions and thereby, increase the overall trustworthiness of our whole
model. In addition, predictions of the lung trachea based on the maximum
connectivity rate were calculated and extracted during post-processing for
segmentation refinement and pruning.",http://arxiv.org/abs/2402.07403v1,76,16,16,18,14,12
A.I. In All The Wrong Places,"This text describes experiences gained across a two-year test period during
which two generations of Generative Artificial Intelligence (A.I.) systems were
incorporated into an interdisciplinary, university level course on A.I. for art
and design practices. The text uses the results from the courses to reflect on
new opportunities for generative systems in art and design, while considering
traps and limits.",http://arxiv.org/abs/2401.16268v3,76,18,16,16,14,12
Cultural gems linked open data: Mapping culture and intangible heritage in European cities,"The recovery and resilience of the cultural and creative sectors after the
COVID-19 pandemic is a current topic with priority for the European Commission.
Cultural gems is a crowdsourced web platform managed by the Joint Research
Centre of the European Commission aimed at creating community-led maps as well
as a common repository for cultural and creative places across European cities
and towns. More than 130,000 physical locations and online cultural activities
in more than 300 European cities and towns are currently tracked by the
application. The main objective of Cultural gems consists in raising a holistic
vision of European culture, reinforcing a sense of belonging to a common
European cultural space. This data article describes the ontology developed for
Cultural gems, adopted to represent the domain of knowledge of the application
by means of FAIR (Findable, Accessible, Interoperable, Reusable) principles and
following the paradigms of Linked Open Data (LOD). We provide an overview of
this dataset, and describe the ontology model, along with the services used to
access and consume the data.",http://dx.doi.org/10.1016/j.dib.2023.109375,76,14,16,18,16,12
Low-degree phase transitions for detecting a planted clique in sublinear time,"We consider the problem of detecting a planted clique of size $k$ in a random
graph on $n$ vertices. When the size of the clique exceeds $\Theta(\sqrt{n})$,
polynomial-time algorithms for detection proliferate. We study faster --
namely, sublinear time -- algorithms in the high-signal regime when $k =
\Theta(n^{1/2 + \delta})$, for some $\delta > 0$. To this end, we consider
algorithms that non-adaptively query a subset $M$ of entries of the adjacency
matrix and then compute a low-degree polynomial function of the revealed
entries. We prove a computational phase transition for this class of
non-adaptive low-degree algorithms: under the scaling $\lvert M \rvert =
\Theta(n^{\gamma})$, the clique can be detected when $\gamma > 3(1/2 - \delta)$
but not when $\gamma < 3(1/2 - \delta)$. As a result, the best known runtime
for detecting a planted clique, $\widetilde{O}(n^{3(1/2-\delta)})$, cannot be
improved without looking beyond the non-adaptive low-degree class.
  Our proof of the lower bound -- based on bounding the conditional low-degree
likelihood ratio -- reveals further structure in non-adaptive detection of a
planted clique. Using (a bound on) the conditional low-degree likelihood ratio
as a potential function, we show that for every non-adaptive query pattern,
there is a highly structured query pattern of the same size that is at least as
effective.",http://arxiv.org/abs/2402.05451v1,76,16,18,16,16,10
Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking,"Large Language Model (LLM) assistants, such as ChatGPT, have emerged as
potential alternatives to search methods for helping users navigate complex,
feature-rich software. LLMs use vast training data from domain-specific texts,
software manuals, and code repositories to mimic human-like interactions,
offering tailored assistance, including step-by-step instructions. In this
work, we investigated LLM-generated software guidance through a within-subject
experiment with 16 participants and follow-up interviews. We compared a
baseline LLM assistant with an LLM optimized for particular software contexts,
SoftAIBot, which also offered guidelines for constructing appropriate prompts.
We assessed task completion, perceived accuracy, relevance, and trust.
Surprisingly, although SoftAIBot outperformed the baseline LLM, our results
revealed no significant difference in LLM usage and user perceptions with or
without prompt guidelines and the integration of domain context. Most users
struggled to understand how the prompt's text related to the LLM's responses
and often followed the LLM's suggestions verbatim, even if they were incorrect.
This resulted in difficulties when using the LLM's advice for software tasks,
leading to low task completion rates. Our detailed analysis also revealed that
users remained unaware of inaccuracies in the LLM's responses, indicating a gap
between their lack of software expertise and their ability to evaluate the
LLM's assistance. With the growing push for designing domain-specific LLM
assistants, we emphasize the importance of incorporating explainable,
context-aware cues into LLMs to help users understand prompt-based
interactions, identify biases, and maximize the utility of LLM assistants.",http://dx.doi.org/10.1145/3640543.3645200,76,16,18,18,14,10
Safe Active Learning for Time-Series Modeling with Gaussian Processes,"Learning time-series models is useful for many applications, such as
simulation and forecasting. In this study, we consider the problem of actively
learning time-series models while taking given safety constraints into account.
For time-series modeling we employ a Gaussian process with a nonlinear
exogenous input structure. The proposed approach generates data appropriate for
time series model learning, i.e. input and output trajectories, by dynamically
exploring the input space. The approach parametrizes the input trajectory as
consecutive trajectory sections, which are determined stepwise given safety
requirements and past observations. We analyze the proposed algorithm and
evaluate it empirically on a technical application. The results show the
effectiveness of our approach in a realistic technical use case.",http://arxiv.org/abs/2402.06276v1,76,16,14,16,20,10
"Reproducibility, energy efficiency and performance of pseudorandom number generators in machine learning: a comparative study of python, numpy, tensorflow, and pytorch implementations","Pseudo-Random Number Generators (PRNGs) have become ubiquitous in machine
learning technologies because they are interesting for numerous methods. The
field of machine learning holds the potential for substantial advancements
across various domains, as exemplified by recent breakthroughs in Large
Language Models (LLMs). However, despite the growing interest, persistent
concerns include issues related to reproducibility and energy consumption.
Reproducibility is crucial for robust scientific inquiry and explainability,
while energy efficiency underscores the imperative to conserve finite global
resources. This study delves into the investigation of whether the leading
Pseudo-Random Number Generators (PRNGs) employed in machine learning languages,
libraries, and frameworks uphold statistical quality and numerical
reproducibility when compared to the original C implementation of the
respective PRNG algorithms. Additionally, we aim to evaluate the time
efficiency and energy consumption of various implementations. Our experiments
encompass Python, NumPy, TensorFlow, and PyTorch, utilizing the Mersenne
Twister, PCG, and Philox algorithms. Remarkably, we verified that the temporal
performance of machine learning technologies closely aligns with that of
C-based implementations, with instances of achieving even superior
performances. On the other hand, it is noteworthy that ML technologies consumed
only 10% more energy than their C-implementation counterparts. However, while
statistical quality was found to be comparable, achieving numerical
reproducibility across different platforms for identical seeds and algorithms
was not achieved.",http://arxiv.org/abs/2401.17345v2,76,16,14,16,18,12
Social Evolution of Published Text and The Emergence of Artificial Intelligence Through Large Language Models and The Problem of Toxicity and Bias,"We provide a birds eye view of the rapid developments in AI and Deep Learning
that has led to the path-breaking emergence of AI in Large Language Models. The
aim of this study is to place all these developments in a pragmatic broader
historical social perspective without any exaggerations while at the same time
without any pessimism that created the AI winter in the 1970s to 1990s. We also
at the same time point out toxicity, bias, memorization, sycophancy, logical
inconsistencies, hallucinations that exist just as a warning to the overly
optimistic. We note here that just as this emergence of AI seems to occur at a
threshold point in the number of neural connections or weights, it has also
been observed that human brain and especially the cortex region is nothing
special or extraordinary but simply a case of scaled-up version of the primate
brain and that even the human intelligence seems like an emergent phenomena of
scale.",http://arxiv.org/abs/2402.07166v1,75,18,15,17,15,10
Emerging Results on Automated Support for Searching and Selecting Evidence for Systematic Literature Review Updates,"Context: The constant growth of primary evidence and Systematic Literature
Reviews (SLRs) publications in the Software Engineering (SE) field leads to the
need for SLR Updates. However, searching and selecting evidence for SLR updates
demands significant effort from SE researchers. Objective: We present emerging
results on an automated approach to support searching and selecting studies for
SLR updates in SE. Method: We developed an automated tool prototype to perform
the snowballing search technique and support selecting relevant studies for SLR
updates using Machine Learning (ML) algorithms. We evaluated our automation
proposition through a small-scale evaluation with a reliable dataset from an
SLR replication and its update. Results: Effectively automating
snowballing-based search strategies showed feasibility with minor losses,
specifically related to papers without Digital Object Identifier (DOI). The ML
algorithm giving the highest performance to select studies for SLR updates was
Linear Support Vector Machine, with approximately 74% recall and 15% precision.
Using such algorithms with conservative thresholds to minimize the risk of
missing papers can significantly reduce evidence selection efforts. Conclusion:
The preliminary results of our evaluation point in promising directions,
indicating the potential of automating snowballing search efforts and of
reducing the number of papers to be manually analyzed by about 2.5 times when
selecting evidence for updating SLRs in SE.",http://arxiv.org/abs/2402.05317v1,75,16,20,18,16,5
An attempt to generate new bridge types from latent space of denoising diffusion Implicit model,"Use denoising diffusion implicit model for bridge-type innovation. The
process of adding noise and denoising to an image can be likened to the process
of a corpse rotting and a detective restoring the scene of a victim being
killed, to help beginners understand. Through an easy-to-understand algebraic
method, derive the function formulas for adding noise and denoising, making it
easier for beginners to master the mathematical principles of the model. Using
symmetric structured image dataset of three-span beam bridge, arch bridge,
cable-stayed bridge and suspension bridge , based on Python programming
language, TensorFlow and Keras deep learning platform framework , denoising
diffusion implicit model is constructed and trained. From the latent space
sampling, new bridge types with asymmetric structures can be generated.
Denoising diffusion implicit model can organically combine different structural
components on the basis of human original bridge types, and create new bridge
types.",http://arxiv.org/abs/2402.07129v1,75,18,16,15,18,8
Distal Interference: Exploring the Limits of Model-Based Continual Learning,"Continual learning is the sequential learning of different tasks by a machine
learning model. Continual learning is known to be hindered by catastrophic
interference or forgetting, i.e. rapid unlearning of earlier learned tasks when
new tasks are learned. Despite their practical success, artificial neural
networks (ANNs) are prone to catastrophic interference. This study analyses how
gradient descent and overlapping representations between distant input points
lead to distal interference and catastrophic interference. Distal interference
refers to the phenomenon where training a model on a subset of the domain leads
to non-local changes on other subsets of the domain. This study shows that
uniformly trainable models without distal interference must be exponentially
large. A novel antisymmetric bounded exponential layer B-spline ANN
architecture named ABEL-Spline is proposed that can approximate any continuous
function, is uniformly trainable, has polynomial computational complexity, and
provides some guarantees for distal interference. Experiments are presented to
demonstrate the theoretical properties of ABEL-Splines. ABEL-Splines are also
evaluated on benchmark regression problems. It is concluded that the weaker
distal interference guarantees in ABEL-Splines are insufficient for model-only
continual learning. It is conjectured that continual learning with polynomial
complexity models requires augmentation of the training data or algorithm.",http://arxiv.org/abs/2402.08255v1,75,18,16,16,15,10
"Le Nozze di Giustizia. Interactions between Artificial Intelligence, Law, Logic, Language and Computation with some case studies in Traffic Regulations and Health Care","An important aim of this paper is to convey some basics of mathematical logic
to the legal community working with Artificial Intelligence. After analysing
what AI is, we decide to delimit ourselves to rule-based AI leaving Neural
Networks and Machine Learning aside. Rule based AI allows for Formal methods
which are described in a rudimentary form. We will then see how mathematical
logic interacts with legal rule-based AI practice. We shall see how
mathematical logic imposes limitations and complications to AI applications. We
classify the limitations and interactions between mathematical logic and legal
AI in three categories: logical, computational and mathematical. The examples
to showcase the interactions will largely come from European traffic
regulations. The paper closes off with some reflections on how and where AI
could be used and on basic mechanisms that shape society.",http://arxiv.org/abs/2402.06487v1,75,12,16,16,18,13
Deep Reinforcement Learning for Controlled Traversing of the Attractor Landscape of Boolean Models in the Context of Cellular Reprogramming,"Cellular reprogramming can be used for both the prevention and cure of
different diseases. However, the efficiency of discovering reprogramming
strategies with classical wet-lab experiments is hindered by lengthy time
commitments and high costs. In this study, we develop a~novel computational
framework based on deep reinforcement learning that facilitates the
identification of reprogramming strategies. For this aim, we formulate
a~control problem in the context of cellular reprogramming for the frameworks
of BNs and PBNs under the asynchronous update mode. Furthermore, we introduce
the notion of a~pseudo-attractor and a~procedure for identification of
pseudo-attractor state during training. Finally, we devise a~computational
framework for solving the control problem, which we test on a~number of
different models.",http://arxiv.org/abs/2402.08491v1,75,16,18,16,14,11
A precise bare simulation approach to the minimization of some distances. II. Further Foundations,"The constrained minimization (respectively maximization) of directed
distances and of related generalized entropies is a fundamental task in
information theory as well as in the adjacent fields of statistics, machine
learning, artificial intelligence, signal processing and pattern recognition.
In our previous paper ""A precise bare simulation approach to the minimization
of some distances. I. Foundations"", we obtained such kind of constrained optima
by a new dimension-free precise bare (pure) simulation method, provided
basically that (i) the underlying directed distance is of f-divergence type,
and that (ii) this can be connected to a light-tailed probability distribution
in a certain manner. In the present paper, we extend this approach such that
constrained optimization problems of a very huge amount of directed distances
and generalized entropies -- and beyond -- can be tackled by a newly developed
dimension-free extended bare simulation method, for obtaining both optima as
well as optimizers. Almost no assumptions (like convexity) on the set of
constraints are needed, within our discrete setup of arbitrary dimension, and
our method is precise (i.e., converges in the limit). For instance, we cover
constrained optimizations of arbitrary f-divergences, Bregman distances, scaled
Bregman distances and weighted Euclidean distances. The potential for
wide-spread applicability is indicated, too; in particular, we deliver many
recent references for uses of the involved distances/divergences in various
different research fields (which may also serve as an interdisciplinary
interface).",http://arxiv.org/abs/2402.08478v1,75,14,16,18,16,11
Gaussian Mixture Models for Affordance Learning using Bayesian Networks,"Affordances are fundamental descriptors of relationships between actions,
objects and effects. They provide the means whereby a robot can predict
effects, recognize actions, select objects and plan its behavior according to
desired goals. This paper approaches the problem of an embodied agent exploring
the world and learning these affordances autonomously from its sensory
experiences. Models exist for learning the structure and the parameters of a
Bayesian Network encoding this knowledge. Although Bayesian Networks are
capable of dealing with uncertainty and redundancy, previous work considered
complete observability of the discrete sensory data, which may lead to hard
errors in the presence of noise. In this paper we consider a probabilistic
representation of the sensors by Gaussian Mixture Models (GMMs) and explicitly
taking into account the probability distribution contained in each discrete
affordance concept, which can lead to a more correct learning.",http://dx.doi.org/10.1109/IROS.2010.5650297,75,14,16,16,16,13
Understanding Deep Learning defenses Against Adversarial Examples Through Visualizations for Dynamic Risk Assessment,"In recent years, Deep Neural Network models have been developed in different
fields, where they have brought many advances. However, they have also started
to be used in tasks where risk is critical. A misdiagnosis of these models can
lead to serious accidents or even death. This concern has led to an interest
among researchers to study possible attacks on these models, discovering a long
list of vulnerabilities, from which every model should be defended. The
adversarial example attack is a widely known attack among researchers, who have
developed several defenses to avoid such a threat. However, these defenses are
as opaque as a deep neural network model, how they work is still unknown. This
is why visualizing how they change the behavior of the target model is
interesting in order to understand more precisely how the performance of the
defended model is being modified. For this work, some defenses, against
adversarial example attack, have been selected in order to visualize the
behavior modification of each of them in the defended model. Adversarial
training, dimensionality reduction and prediction similarity were the selected
defenses, which have been developed using a model composed by convolution
neural network layers and dense neural network layers. In each defense, the
behavior of the original model has been compared with the behavior of the
defended model, representing the target model by a graph in a visualization.",http://dx.doi.org/10.1007/s00521-021-06812-y,74,14,16,16,18,10
Tempered Calculus for ML: Application to Hyperbolic Model Embedding,"Most mathematical distortions used in ML are fundamentally integral in
nature: $f$-divergences, Bregman divergences, (regularized) optimal transport
distances, integral probability metrics, geodesic distances, etc. In this
paper, we unveil a grounded theory and tools which can help improve these
distortions to better cope with ML requirements. We start with a generalization
of Riemann integration that also encapsulates functions that are not strictly
additive but are, more generally, $t$-additive, as in nonextensive statistical
mechanics. Notably, this recovers Volterra's product integral as a special
case. We then generalize the Fundamental Theorem of calculus using an extension
of the (Euclidean) derivative. This, along with a series of more specific
Theorems, serves as a basis for results showing how one can specifically
design, alter, or change fundamental properties of distortion measures in a
simple way, with a special emphasis on geometric- and ML-related properties
that are the metricity, hyperbolicity, and encoding. We show how to apply it to
a problem that has recently gained traction in ML: hyperbolic embeddings with a
""cheap"" and accurate encoding along the hyperbolic vs Euclidean scale. We
unveil a new application for which the Poincar\'e disk model has very appealing
features, and our theory comes in handy: \textit{model} embeddings for boosted
combinations of decision trees, trained using the log-loss (trees) and logistic
loss (combinations).",http://arxiv.org/abs/2402.04163v2,72,16,16,15,14,11
LLMs and the Human Condition,"This paper presents three established theories of human decision-making and
describes how they can be integrated to provide a model of purposive human
action. Taking seriously the idea of language as action the model is then
applied to the conversational user interfaces. Theory based AI research has had
a hard time recently and the aim here is to revitalise interest in
understanding what LLMs are actually doing other than running poorly understood
machine learning routines over all the data the relevant Big Tech company can
hoover up. When a raspberry pi computer for under 50USD is up to 400 times
faster than the first commercial Cray super computer~\cite{crayVpi}, Big Tech
can get really close to having an infinite number of monkeys typing at random
and producing text, some of which will make sense. By understanding where
ChatGPT's apparent intelligence comes from, perhaps we can perform the magic
with fewer resources and at the same time gain some understanding about our
relationship with our world.",http://arxiv.org/abs/2402.08403v1,72,16,14,14,20,8
Investigating Reproducibility in Deep Learning-Based Software Fault Prediction,"Over the past few years, deep learning methods have been applied for a wide
range of Software Engineering (SE) tasks, including in particular for the
important task of automatically predicting and localizing faults in software.
With the rapid adoption of increasingly complex machine learning models, it
however becomes more and more difficult for scholars to reproduce the results
that are reported in the literature. This is in particular the case when the
applied deep learning models and the evaluation methodology are not properly
documented and when code and data are not shared. Given some recent -- and very
worrying -- findings regarding reproducibility and progress in other areas of
applied machine learning, the goal of this work is to analyze to what extent
the field of software engineering, in particular in the area of software fault
prediction, is plagued by similar problems. We have therefore conducted a
systematic review of the current literature and examined the level of
reproducibility of 56 research articles that were published between 2019 and
2022 in top-tier software engineering conferences. Our analysis revealed that
scholars are apparently largely aware of the reproducibility problem, and about
two thirds of the papers provide code for their proposed deep learning models.
However, it turned out that in the vast majority of cases, crucial elements for
reproducibility are missing, such as the code of the compared baselines, code
for data pre-processing or code for hyperparameter tuning. In these cases, it
therefore remains challenging to exactly reproduce the results in the current
research literature. Overall, our meta-analysis therefore calls for improved
research practices to ensure the reproducibility of machine-learning based
research.",http://arxiv.org/abs/2402.05645v1,72,16,14,14,18,10
Discipline and Label: A WEIRD Genealogy and Social Theory of Data Annotation,"Data annotation remains the sine qua non of machine learning and AI. Recent
empirical work on data annotation has begun to highlight the importance of
rater diversity for fairness, model performance, and new lines of research have
begun to examine the working conditions for data annotation workers, the
impacts and role of annotator subjectivity on labels, and the potential
psychological harms from aspects of annotation work. This paper outlines a
critical genealogy of data annotation; starting with its psychological and
perceptual aspects. We draw on similarities with critiques of the rise of
computerized lab-based psychological experiments in the 1970's which question
whether these experiments permit the generalization of results beyond the
laboratory settings within which these results are typically obtained. Do data
annotations permit the generalization of results beyond the settings, or
locations, in which they were obtained? Psychology is overly reliant on
participants from Western, Educated, Industrialized, Rich, and Democratic
societies (WEIRD). Many of the people who work as data annotation platform
workers, however, are not from WEIRD countries; most data annotation workers
are based in Global South countries. Social categorizations and classifications
from WEIRD countries are imposed on non-WEIRD annotators through instructions
and tasks, and through them, on data, which is then used to train or evaluate
AI models in WEIRD countries. We synthesize evidence from several recent lines
of research and argue that data annotation is a form of automated social
categorization that risks entrenching outdated and static social categories
that are in reality dynamic and changing. We propose a framework for
understanding the interplay of the global social conditions of data annotation
with the subjective phenomenological experience of data annotation work.",http://arxiv.org/abs/2402.06811v1,72,16,18,16,10,12
Retzzles: Do Jigsaw Puzzle Actions on Interactive Display Maps Increase the Retention of Map Information?,"While maps provide upfront content, this might not always be the most
effective way for users to remember information. With the proliferation of
interactive displays for tourists and visitors in public spaces, we can create
a more playful user experience with maps than just exploring them. Adding
interactions with the map could also help users retain more information as they
use them. In this paper, we investigated whether completing a jigsaw puzzle of
a map supports users in retaining more information about a specific map. The
results of a between-subject study with a sample of n=28 indicate that
additional interaction helped improve mean scores of textual and spatial recall
but not visual recall. However, the results are not statistically significant,
and the topic is subject to further investigation. Our findings contribute to
discussions on using interactive touchscreen displays in similar learning
scenarios involving memory retention.",http://arxiv.org/abs/2402.06741v1,72,14,15,15,16,12
You Still See Me: How Data Protection Supports the Architecture of ML Surveillance,"Data forms the backbone of machine learning. Thus, data protection law has
strong bearing on how ML systems are governed. Given that most requirements
accompany the processing of personal data, organizations have an incentive to
keep their data out of legal scope. Privacy-preserving techniques incentivized
by data protection law -- data protection techniques -- constitute an important
strategy for ML development because they are used to distill data until it
potentially falls outside the scope of data protection laws.
  In this paper, we examine the impact of a rhetoric that deems data wrapped in
privacy-preserving techniques as data that is ""good-to-go"". We show how the
application of data protection techniques in the development of ML systems --
from private set intersection as part of dataset curation to homomorphic
encryption and federated learning as part of model computation to the framing
of the privacy-utility trade-off as part of model updating -- can further
support individual monitoring and data consolidation. With data accumulation at
the core of how the ML pipeline is configured, we argue that data protection
techniques are often instrumentalized in ways that support infrastructures of
surveillance, rather than to protect individuals associated with data. Finally,
we propose technology and policy strategies to evaluate data protection
techniques in light of the protections they actually confer. We conclude by
highlighting the role that security technologists might play in devising
policies that combat surveillance ML technologies -- recommending the
adversarial mindset inherent to the profession to more precisely articulate and
prevent the use of ""privacy-preserving"" scaffoldings that support surveillance.",http://arxiv.org/abs/2402.06609v1,72,16,14,16,18,8
Sentinels of the Stream: Unleashing Large Language Models for Dynamic Packet Classification in Software Defined Networks -- Position Paper,"With the release of OpenAI's ChatGPT, the field of large language models
(LLM) saw an increase of academic interest in GPT based chat assistants. In the
next few months multiple accesible large language models were released that
included Meta's LLama models and Mistral AI's Mistral and Mixtral MoE models.
These models are available openly for a wide array of purposes with a wide
spectrum of licenses. These LLMs have found their use in a different number of
fields like code development, SQL generation etc. In this work we propose our
plan to explore the applicability of large language model in the domain of
network security. We plan to create Sentinel, a LLM, to analyse network packet
contents and pass a judgment on it's threat level. This work is a preliminary
report that will lay our plan for our future endeavors.",http://arxiv.org/abs/2402.07950v1,70,15,16,17,12,10
